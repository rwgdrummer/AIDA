diff --git a/.gitignore b/.gitignore
index d265940..a4c1abb 100644
--- a/.gitignore
+++ b/.gitignore
@@ -9,6 +9,7 @@
 data/PCSO/*
 data/lfpw
 data/LFW
+data/MEDS
 build*
 scripts/results
 share/openbr/models
@@ -54,3 +55,5 @@
 *.check_cache
 *.sublime-project
 *.sublime-workspace
+/Debug/
+built
\ No newline at end of file
diff --git a/3rdparty/stasm4.0.0/stasm/MOD_1/facedet.cpp b/3rdparty/stasm4.0.0/stasm/MOD_1/facedet.cpp
index ce92c57..a4c5f34 100755
--- a/3rdparty/stasm4.0.0/stasm/MOD_1/facedet.cpp
+++ b/3rdparty/stasm4.0.0/stasm/MOD_1/facedet.cpp
@@ -4,6 +4,8 @@
 
 #include "../stasm.h"
 
+#include "opencv2/imgproc.hpp" //SAB
+
 namespace stasm
 {
 typedef vector<DetPar> vec_DetPar;
diff --git a/3rdparty/stasm4.0.0/stasm/asm.cpp b/3rdparty/stasm4.0.0/stasm/asm.cpp
index e7a36cb..d4487f4 100755
--- a/3rdparty/stasm4.0.0/stasm/asm.cpp
+++ b/3rdparty/stasm4.0.0/stasm/asm.cpp
@@ -5,6 +5,8 @@
 #include "stasm.h"
 #include "stasmhash.h"
 
+#include "opencv2/imgproc.hpp" //SAB
+
 namespace stasm
 {
 static void TraceShape(  // write an image file showing current shape on the image
diff --git a/3rdparty/stasm4.0.0/stasm/faceroi.cpp b/3rdparty/stasm4.0.0/stasm/faceroi.cpp
index b7f9120..734c371 100755
--- a/3rdparty/stasm4.0.0/stasm/faceroi.cpp
+++ b/3rdparty/stasm4.0.0/stasm/faceroi.cpp
@@ -4,6 +4,8 @@
 
 #include "stasm.h"
 
+#include "opencv2/imgproc.hpp" //SAB
+
 namespace stasm
 {
 // Rotations less than 5 are treated as zero to minimize image preprocessing.
@@ -89,7 +91,8 @@
     if (Valid(detpar.rot) && detpar.rot)
     {
         // rotate eyes and mouth
-        const MAT rotmat = getRotationMatrix2D(cv::Point2f(float(detpar_roi.x),
+//SAB        const MAT rotmat = getRotationMatrix2D(cv::Point2f(float(detpar_roi.x),
+        const MAT rotmat = cv::getRotationMatrix2D(cv::Point2f(float(detpar_roi.x),
                                                float(detpar_roi.y)),
                                                -detpar.rot, 1.);
         AlignShapeInPlace(eyemouth_shape, rotmat);
@@ -127,7 +130,8 @@
 
     if (Valid(detpar.rot) && detpar.rot)
     {
-        const MAT rotmat = getRotationMatrix2D(cv::Point2f(float(detpar_roi.x),
+//SAB        const MAT rotmat = getRotationMatrix2D(cv::Point2f(float(detpar_roi.x),
+        const MAT rotmat = cv::getRotationMatrix2D(cv::Point2f(float(detpar_roi.x),
                                                float(detpar_roi.y)),
                                                -detpar.rot,
                                                1.);
@@ -151,7 +155,8 @@
         outshape = FlipShape(outshape, face_roi.cols);
     if (Valid(detpar.rot) && detpar.rot)
     {
-        const MAT rotmat = getRotationMatrix2D(cv::Point2f(float(detpar_roi.x),
+//SAB        const MAT rotmat = getRotationMatrix2D(cv::Point2f(float(detpar_roi.x),
+        const MAT rotmat = cv::getRotationMatrix2D(cv::Point2f(float(detpar_roi.x),
                                                float(detpar_roi.y)),
                                                detpar.rot, 1.);
         outshape = AlignShape(outshape, rotmat);
@@ -198,7 +203,8 @@
 
     else // rotate image so face is upright, results go into face_roi
         warpAffine(Image(img, rect_roi), face_roi,
-                   getRotationMatrix2D(cv::Point2f(float(detpar_roi.x),
+//SAB                   getRotationMatrix2D(cv::Point2f(float(detpar_roi.x),
+                   cv::getRotationMatrix2D(cv::Point2f(float(detpar_roi.x),
                                                    float(detpar_roi.y)),
                                        -detpar.rot, 1.),
                    cv::Size(face_roi.cols, face_roi.rows),
diff --git a/3rdparty/stasm4.0.0/stasm/misc.cpp b/3rdparty/stasm4.0.0/stasm/misc.cpp
index dcf30de..38b6a32 100755
--- a/3rdparty/stasm4.0.0/stasm/misc.cpp
+++ b/3rdparty/stasm4.0.0/stasm/misc.cpp
@@ -5,6 +5,8 @@
 #include "stasm.h"
 #include <sys/stat.h>
 
+#include "opencv2/imgproc.hpp" //SAB
+
 namespace stasm
 {
 //-----------------------------------------------------------------------------
diff --git a/3rdparty/stasm4.0.0/stasm/misc.h b/3rdparty/stasm4.0.0/stasm/misc.h
index f150683..b7a2b02 100755
--- a/3rdparty/stasm4.0.0/stasm/misc.h
+++ b/3rdparty/stasm4.0.0/stasm/misc.h
@@ -5,6 +5,8 @@
 #ifndef STASM_MISC_H
 #define STASM_MISC_H
 
+#include "opencv2/objdetect.hpp" //SAB
+
 namespace stasm
 {
 using cv::Rect;
diff --git a/3rdparty/stasm4.0.0/stasm/pinstart.cpp b/3rdparty/stasm4.0.0/stasm/pinstart.cpp
index e47be40..2ea55b4 100755
--- a/3rdparty/stasm4.0.0/stasm/pinstart.cpp
+++ b/3rdparty/stasm4.0.0/stasm/pinstart.cpp
@@ -4,6 +4,8 @@
 
 #include "stasm.h"
 
+#include "opencv2/imgproc.hpp" //SAB
+
 namespace stasm
 {
 // The following model was machine generated by running
@@ -34,7 +36,8 @@
     CV_Assert(rot >= -360 && rot <= 360); // sanity check, 360 is arb
 
     const MAT rotmat =
-        getRotationMatrix2D(cv::Point2f(float(x), float(y)), rot, 1.);
+//SAB        getRotationMatrix2D(cv::Point2f(float(x), float(y)), rot, 1.);
+        cv::getRotationMatrix2D(cv::Point2f(float(x), float(y)), rot, 1.);
 
     AlignShapeInPlace(shape, rotmat);
 }
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 4793d23..47dc779 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -87,7 +87,8 @@
 endif()
 
 # Find OpenCV
-find_package(OpenCV 2.4.5 REQUIRED)
+#find_package(OpenCV 2.4.5 REQUIRED)
+find_package(OpenCV REQUIRED)
 if(IOS)
   file(GLOB OPENCV_IOS_LIBS "${OpenCV_DIR}/../../lib/*.a")
   list(APPEND BR_THIRDPARTY_LIBS ${OPENCV_IOS_LIBS})
diff --git a/app/br/CMakeLists.txt b/app/br/CMakeLists.txt
index dfa60ed..30a9030 100644
--- a/app/br/CMakeLists.txt
+++ b/app/br/CMakeLists.txt
@@ -2,6 +2,8 @@
   find_package(Threads REQUIRED)
 endif()
 
+include_directories(/opt/local/include)
+
 add_executable(br br.cpp ${BR_RESOURCES})
 target_link_libraries(br openbr ${CMAKE_THREAD_LIBS_INIT})
 qt5_use_modules(br ${QT_DEPENDENCIES})
diff --git a/openbr/CMakeLists.txt b/openbr/CMakeLists.txt
index f42e834..422e917 100644
--- a/openbr/CMakeLists.txt
+++ b/openbr/CMakeLists.txt
@@ -23,14 +23,16 @@
 # Optional GUI module
 if(NOT BR_EMBEDDED)
   aux_source_directory(gui BR_GUI)
-  qt5_add_resources(BR_ICONS ../share/openbr/icons.qrc)
+#SAB  qt5_add_resources(BR_ICONS ../share/openbr/icons.qrc)
+  QT5_ADD_RESOURCES(BR_ICONS ../share/openbr/icons.qrc)
   file(GLOB HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/gui/*.h)
   install(FILES ${HEADERS} DESTINATION include/openbr/gui)
 endif()
 
 # Compile third party resources
 if(BR_THIRDPARTY_RESOURCES)
-  qt5_add_resources(THIRDPARTY_RESOURCES ${BR_THIRDPARTY_RESOURCES})
+#SAB  qt5_add_resources(THIRDPARTY_RESOURCES ${BR_THIRDPARTY_RESOURCES})
+  QT5_ADD_RESOURCES(THIRDPARTY_RESOURCES ${BR_THIRDPARTY_RESOURCES})
 endif()
 
 add_library(openbr SHARED ${SRC} ${BR_CORE} ${BR_JANUS} ${BR_GUI} ${BR_ICONS} ${BR_THIRDPARTY_SRC} ${BR_RESOURCES} ${NATURALSTRINGCOMPARE_SRC} ${THIRDPARTY_RESOURCES})
@@ -64,6 +66,9 @@
 
 if(NOT BR_EMBEDDED)
   file(GLOB HEADERS *.h)
+#  file(GLOB HEADERS core/cluster.h)
+  install(FILES core/cluster.h core/resource.h core/opencvutils.h core/old_ml.hpp DESTINATION include/openbr/core)
+  install(FILES plugins/openbr_internal.h DESTINATION include/openbr/plugins)
   install(FILES ${HEADERS} DESTINATION include/openbr)
 endif()
 
diff --git a/openbr/core/boost.cpp b/openbr/core/boost.cpp
index 325cd21..d38f079 100644
--- a/openbr/core/boost.cpp
+++ b/openbr/core/boost.cpp
@@ -5,6 +5,11 @@
 #include "boost.h"
 #include "cxmisc.h"
 
+//SAB begin
+#define __BEGIN__ __CV_BEGIN__
+#define __END__ __CV_END__
+//SAB end
+
 using namespace std;
 using namespace br;
 using namespace cv;
@@ -19,8 +24,18 @@
     return log( val/(1. - val) );
 }
 
-#define CV_CMP_NUM_IDX(i,j) (aux[i] < aux[j])
-static CV_IMPLEMENT_QSORT_EX( icvSortIntAux, int, CV_CMP_NUM_IDX, const float* )
+//SAB #define CV_CMP_NUM_IDX(i,j) (aux[i] < aux[j])
+//SAB static CV_IMPLEMENT_QSORT_EX( icvSortIntAux, int, CV_CMP_NUM_IDX, const float* )
+//SAB begin
+template<typename T, typename Idx>
+class LessThanIdx
+{
+public:
+    LessThanIdx( const T* _arr ) : arr(_arr) {}
+    bool operator()(Idx a, Idx b) const { return arr[a] < arr[b]; }
+    const T* arr;
+};
+//SAB end
 
 #define CV_THRESHOLD_EPS (0.00001F)
 
@@ -610,7 +625,8 @@
             }
         }
 
-        icvSortIntAux( sortedIndicesBuf, nodeSampleCount, &sampleValues[0] );
+//SAB        icvSortIntAux( sortedIndicesBuf, nodeSampleCount, &sampleValues[0] );
+        std::sort(sortedIndicesBuf, sortedIndicesBuf + nodeSampleCount, LessThanIdx<float, int>(&sampleValues[0]) );
 
         for (int i = 0; i < nodeSampleCount; i++)
             ordValuesBuf[i] = (&sampleValues[0])[sortedIndicesBuf[i]];
@@ -686,7 +702,8 @@
 
     void sortBuffer(uint64_t fi, float *valCachePtr) const
     {
-        icvSortIntAux(idst + fi*sampleCount, sampleCount, valCachePtr);
+//SAB        icvSortIntAux(idst + fi*sampleCount, sampleCount, valCachePtr);
+        std::sort(idst + (size_t)fi*sampleCount, idst + (size_t)(fi + 1)*sampleCount, LessThanIdx<float, int>(valCachePtr) );
     }
 
     virtual void operator()(const Range& range) const
diff --git a/openbr/core/boost.h b/openbr/core/boost.h
index 1b54e3a..ca28252 100644
--- a/openbr/core/boost.h
+++ b/openbr/core/boost.h
@@ -1,7 +1,8 @@
 #ifndef _BOOST_H_
 #define _BOOST_H_
 
-#include "ml.h"
+//SAB #include "ml.h"
+#include "old_ml.hpp"
 #include <openbr/openbr_plugin.h>
 
 namespace br
diff --git a/openbr/core/cluster.h b/openbr/core/cluster.h
index ad50470..03ae23e 100644
--- a/openbr/core/cluster.h
+++ b/openbr/core/cluster.h
@@ -29,10 +29,10 @@
     typedef QList<int> Cluster; // List of indices into galleries
     typedef QVector<Cluster> Clusters;
 
-    // generate k-NN graph from pre-computed similarity matrices 
+    // generate k-NN graph from pre-computed similarity matrices
     Neighborhood knnFromSimmat(const QStringList &simmats, int k = 20);
     Neighborhood knnFromSimmat(const QList<cv::Mat> &simmats, int k = 20);
- 
+
     // Generate k-NN graph from a gallery, using the current algorithm for comparison.
     // direct serialization to file system.
     void  knnFromGallery(const QString &galleryName, const QString & outFile, int k = 20);
@@ -52,7 +52,7 @@
     Clusters ClusterGraph(const QString & knnName, float aggressiveness, const QString &csv = "");
 
     // Given a similarity matrix, compute the k-NN graph, then perform rank-order clustering.
-    Clusters ClusterSimmat(const QList<cv::Mat> &simmats, float aggressiveness, const QString &csv = "");
+    BR_EXPORT Clusters ClusterSimmat(const QList<cv::Mat> &simmats, float aggressiveness, const QString &csv = "");
     Clusters ClusterSimmat(const QStringList &simmats, float aggressiveness, const QString &csv = "");
 
     // evaluate clustering results in csv, reading ground truth data from gallery input, using truth_property
diff --git a/openbr/core/eval.cpp b/openbr/core/eval.cpp
index 80b0197..0f02d17 100755
--- a/openbr/core/eval.cpp
+++ b/openbr/core/eval.cpp
@@ -23,6 +23,7 @@
 #include <QMapIterator>
 #include <cmath>
 #include <opencv2/highgui/highgui.hpp>
+#include <opencv2/imgproc.hpp> //SAB
 
 using namespace cv;
 using namespace EvalUtils;
@@ -225,7 +226,7 @@
                     genuineSearches[comparison.query] = -1;
                 }
                 impostorCount++;
-            }                           
+            }
         }
     }
 
diff --git a/openbr/core/evalutils.cpp b/openbr/core/evalutils.cpp
index dae094c..f1bb0c1 100644
--- a/openbr/core/evalutils.cpp
+++ b/openbr/core/evalutils.cpp
@@ -4,6 +4,7 @@
 #include "openbr/core/common.h"
 
 #include <opencv2/highgui/highgui.hpp>
+#include <opencv2/imgproc.hpp> //SAB
 
 using namespace std;
 using namespace br;
diff --git a/openbr/core/old_ml.hpp b/openbr/core/old_ml.hpp
new file mode 100644
index 0000000..833e794
--- /dev/null
+++ b/openbr/core/old_ml.hpp
@@ -0,0 +1,2053 @@
+/*M///////////////////////////////////////////////////////////////////////////////////////
+//
+//  IMPORTANT: READ BEFORE DOWNLOADING, COPYING, INSTALLING OR USING.
+//
+//  By downloading, copying, installing or using the software you agree to this license.
+//  If you do not agree to this license, do not download, install,
+//  copy or use the software.
+//
+//
+//                        Intel License Agreement
+//
+// Copyright (C) 2000, Intel Corporation, all rights reserved.
+// Third party copyrights are property of their respective owners.
+//
+// Redistribution and use in source and binary forms, with or without modification,
+// are permitted provided that the following conditions are met:
+//
+//   * Redistribution's of source code must retain the above copyright notice,
+//     this list of conditions and the following disclaimer.
+//
+//   * Redistribution's in binary form must reproduce the above copyright notice,
+//     this list of conditions and the following disclaimer in the documentation
+//     and/or other materials provided with the distribution.
+//
+//   * The name of Intel Corporation may not be used to endorse or promote products
+//     derived from this software without specific prior written permission.
+//
+// This software is provided by the copyright holders and contributors "as is" and
+// any express or implied warranties, including, but not limited to, the implied
+// warranties of merchantability and fitness for a particular purpose are disclaimed.
+// In no event shall the Intel Corporation or contributors be liable for any direct,
+// indirect, incidental, special, exemplary, or consequential damages
+// (including, but not limited to, procurement of substitute goods or services;
+// loss of use, data, or profits; or business interruption) however caused
+// and on any theory of liability, whether in contract, strict liability,
+// or tort (including negligence or otherwise) arising in any way out of
+// the use of this software, even if advised of the possibility of such damage.
+//
+//M*/
+
+#ifndef OPENCV_ML_HPP
+#define OPENCV_ML_HPP
+
+#ifdef __cplusplus
+#  include "opencv2/core.hpp"
+#endif
+
+#include "opencv2/core/core_c.h"
+#include <limits.h>
+
+#ifdef __cplusplus
+
+#include <map>
+#include <iostream>
+
+// Apple defines a check() macro somewhere in the debug headers
+// that interferes with a method definition in this header
+#undef check
+
+//SAB begin
+#define cvAlign cv::alignSize
+#define cvAlignPtr cv::alignPtr
+//SAB end
+
+/****************************************************************************************\
+*                               Main struct definitions                                  *
+\****************************************************************************************/
+
+/* log(2*PI) */
+#define CV_LOG2PI (1.8378770664093454835606594728112)
+
+/* columns of <trainData> matrix are training samples */
+#define CV_COL_SAMPLE 0
+
+/* rows of <trainData> matrix are training samples */
+#define CV_ROW_SAMPLE 1
+
+#define CV_IS_ROW_SAMPLE(flags) ((flags) & CV_ROW_SAMPLE)
+
+struct CvVectors
+{
+    int type;
+    int dims, count;
+    CvVectors* next;
+    union
+    {
+        uchar** ptr;
+        float** fl;
+        double** db;
+    } data;
+};
+
+#if 0
+/* A structure, representing the lattice range of statmodel parameters.
+   It is used for optimizing statmodel parameters by cross-validation method.
+   The lattice is logarithmic, so <step> must be greater then 1. */
+typedef struct CvParamLattice
+{
+    double min_val;
+    double max_val;
+    double step;
+}
+CvParamLattice;
+
+CV_INLINE CvParamLattice cvParamLattice( double min_val, double max_val,
+                                         double log_step )
+{
+    CvParamLattice pl;
+    pl.min_val = MIN( min_val, max_val );
+    pl.max_val = MAX( min_val, max_val );
+    pl.step = MAX( log_step, 1. );
+    return pl;
+}
+
+CV_INLINE CvParamLattice cvDefaultParamLattice( void )
+{
+    CvParamLattice pl = {0,0,0};
+    return pl;
+}
+#endif
+
+/* Variable type */
+#define CV_VAR_NUMERICAL    0
+#define CV_VAR_ORDERED      0
+#define CV_VAR_CATEGORICAL  1
+
+#define CV_TYPE_NAME_ML_SVM         "opencv-ml-svm"
+#define CV_TYPE_NAME_ML_KNN         "opencv-ml-knn"
+#define CV_TYPE_NAME_ML_NBAYES      "opencv-ml-bayesian"
+#define CV_TYPE_NAME_ML_BOOSTING    "opencv-ml-boost-tree"
+#define CV_TYPE_NAME_ML_TREE        "opencv-ml-tree"
+#define CV_TYPE_NAME_ML_ANN_MLP     "opencv-ml-ann-mlp"
+#define CV_TYPE_NAME_ML_CNN         "opencv-ml-cnn"
+#define CV_TYPE_NAME_ML_RTREES      "opencv-ml-random-trees"
+#define CV_TYPE_NAME_ML_ERTREES     "opencv-ml-extremely-randomized-trees"
+#define CV_TYPE_NAME_ML_GBT         "opencv-ml-gradient-boosting-trees"
+
+#define CV_TRAIN_ERROR  0
+#define CV_TEST_ERROR   1
+
+class CvStatModel
+{
+public:
+    CvStatModel();
+    virtual ~CvStatModel();
+
+    virtual void clear();
+
+    CV_WRAP virtual void save( const char* filename, const char* name=0 ) const;
+    CV_WRAP virtual void load( const char* filename, const char* name=0 );
+
+    virtual void write( CvFileStorage* storage, const char* name ) const;
+    virtual void read( CvFileStorage* storage, CvFileNode* node );
+
+protected:
+    const char* default_model_name;
+};
+
+/****************************************************************************************\
+*                                 Normal Bayes Classifier                                *
+\****************************************************************************************/
+
+/* The structure, representing the grid range of statmodel parameters.
+   It is used for optimizing statmodel accuracy by varying model parameters,
+   the accuracy estimate being computed by cross-validation.
+   The grid is logarithmic, so <step> must be greater then 1. */
+
+class CvMLData;
+
+struct CvParamGrid
+{
+    // SVM params type
+    enum { SVM_C=0, SVM_GAMMA=1, SVM_P=2, SVM_NU=3, SVM_COEF=4, SVM_DEGREE=5 };
+
+    CvParamGrid()
+    {
+        min_val = max_val = step = 0;
+    }
+
+    CvParamGrid( double min_val, double max_val, double log_step );
+    //CvParamGrid( int param_id );
+    bool check() const;
+
+    CV_PROP_RW double min_val;
+    CV_PROP_RW double max_val;
+    CV_PROP_RW double step;
+};
+
+inline CvParamGrid::CvParamGrid( double _min_val, double _max_val, double _log_step )
+{
+    min_val = _min_val;
+    max_val = _max_val;
+    step = _log_step;
+}
+
+class CvNormalBayesClassifier : public CvStatModel
+{
+public:
+    CV_WRAP CvNormalBayesClassifier();
+    virtual ~CvNormalBayesClassifier();
+
+    CvNormalBayesClassifier( const CvMat* trainData, const CvMat* responses,
+        const CvMat* varIdx=0, const CvMat* sampleIdx=0 );
+
+    virtual bool train( const CvMat* trainData, const CvMat* responses,
+        const CvMat* varIdx = 0, const CvMat* sampleIdx=0, bool update=false );
+
+    virtual float predict( const CvMat* samples, CV_OUT CvMat* results=0, CV_OUT CvMat* results_prob=0 ) const;
+    CV_WRAP virtual void clear();
+
+    CV_WRAP CvNormalBayesClassifier( const cv::Mat& trainData, const cv::Mat& responses,
+                            const cv::Mat& varIdx=cv::Mat(), const cv::Mat& sampleIdx=cv::Mat() );
+    CV_WRAP virtual bool train( const cv::Mat& trainData, const cv::Mat& responses,
+                       const cv::Mat& varIdx = cv::Mat(), const cv::Mat& sampleIdx=cv::Mat(),
+                       bool update=false );
+    CV_WRAP virtual float predict( const cv::Mat& samples, CV_OUT cv::Mat* results=0, CV_OUT cv::Mat* results_prob=0 ) const;
+
+    virtual void write( CvFileStorage* storage, const char* name ) const;
+    virtual void read( CvFileStorage* storage, CvFileNode* node );
+
+protected:
+    int     var_count, var_all;
+    CvMat*  var_idx;
+    CvMat*  cls_labels;
+    CvMat** count;
+    CvMat** sum;
+    CvMat** productsum;
+    CvMat** avg;
+    CvMat** inv_eigen_values;
+    CvMat** cov_rotate_mats;
+    CvMat*  c;
+};
+
+
+/****************************************************************************************\
+*                          K-Nearest Neighbour Classifier                                *
+\****************************************************************************************/
+
+// k Nearest Neighbors
+class CvKNearest : public CvStatModel
+{
+public:
+
+    CV_WRAP CvKNearest();
+    virtual ~CvKNearest();
+
+    CvKNearest( const CvMat* trainData, const CvMat* responses,
+                const CvMat* sampleIdx=0, bool isRegression=false, int max_k=32 );
+
+    virtual bool train( const CvMat* trainData, const CvMat* responses,
+                        const CvMat* sampleIdx=0, bool is_regression=false,
+                        int maxK=32, bool updateBase=false );
+
+    virtual float find_nearest( const CvMat* samples, int k, CV_OUT CvMat* results=0,
+        const float** neighbors=0, CV_OUT CvMat* neighborResponses=0, CV_OUT CvMat* dist=0 ) const;
+
+    CV_WRAP CvKNearest( const cv::Mat& trainData, const cv::Mat& responses,
+               const cv::Mat& sampleIdx=cv::Mat(), bool isRegression=false, int max_k=32 );
+
+    CV_WRAP virtual bool train( const cv::Mat& trainData, const cv::Mat& responses,
+                       const cv::Mat& sampleIdx=cv::Mat(), bool isRegression=false,
+                       int maxK=32, bool updateBase=false );
+
+    virtual float find_nearest( const cv::Mat& samples, int k, cv::Mat* results=0,
+                                const float** neighbors=0, cv::Mat* neighborResponses=0,
+                                cv::Mat* dist=0 ) const;
+    CV_WRAP virtual float find_nearest( const cv::Mat& samples, int k, CV_OUT cv::Mat& results,
+                                        CV_OUT cv::Mat& neighborResponses, CV_OUT cv::Mat& dists) const;
+
+    virtual void clear();
+    int get_max_k() const;
+    int get_var_count() const;
+    int get_sample_count() const;
+    bool is_regression() const;
+
+    virtual float write_results( int k, int k1, int start, int end,
+        const float* neighbor_responses, const float* dist, CvMat* _results,
+        CvMat* _neighbor_responses, CvMat* _dist, Cv32suf* sort_buf ) const;
+
+    virtual void find_neighbors_direct( const CvMat* _samples, int k, int start, int end,
+        float* neighbor_responses, const float** neighbors, float* dist ) const;
+
+protected:
+
+    int max_k, var_count;
+    int total;
+    bool regression;
+    CvVectors* samples;
+};
+
+/****************************************************************************************\
+*                                   Support Vector Machines                              *
+\****************************************************************************************/
+
+// SVM training parameters
+struct CvSVMParams
+{
+    CvSVMParams();
+    CvSVMParams( int svm_type, int kernel_type,
+                 double degree, double gamma, double coef0,
+                 double Cvalue, double nu, double p,
+                 CvMat* class_weights, CvTermCriteria term_crit );
+
+    CV_PROP_RW int         svm_type;
+    CV_PROP_RW int         kernel_type;
+    CV_PROP_RW double      degree; // for poly
+    CV_PROP_RW double      gamma;  // for poly/rbf/sigmoid/chi2
+    CV_PROP_RW double      coef0;  // for poly/sigmoid
+
+    CV_PROP_RW double      C;  // for CV_SVM_C_SVC, CV_SVM_EPS_SVR and CV_SVM_NU_SVR
+    CV_PROP_RW double      nu; // for CV_SVM_NU_SVC, CV_SVM_ONE_CLASS, and CV_SVM_NU_SVR
+    CV_PROP_RW double      p; // for CV_SVM_EPS_SVR
+    CvMat*      class_weights; // for CV_SVM_C_SVC
+    CV_PROP_RW CvTermCriteria term_crit; // termination criteria
+};
+
+
+struct CvSVMKernel
+{
+    typedef void (CvSVMKernel::*Calc)( int vec_count, int vec_size, const float** vecs,
+                                       const float* another, float* results );
+    CvSVMKernel();
+    CvSVMKernel( const CvSVMParams* params, Calc _calc_func );
+    virtual bool create( const CvSVMParams* params, Calc _calc_func );
+    virtual ~CvSVMKernel();
+
+    virtual void clear();
+    virtual void calc( int vcount, int n, const float** vecs, const float* another, float* results );
+
+    const CvSVMParams* params;
+    Calc calc_func;
+
+    virtual void calc_non_rbf_base( int vec_count, int vec_size, const float** vecs,
+                                    const float* another, float* results,
+                                    double alpha, double beta );
+//SAB    virtual void calc_intersec( int vcount, int var_count, const float** vecs,
+//SAB                            const float* another, float* results );
+//SAB    virtual void calc_chi2( int vec_count, int vec_size, const float** vecs,
+//SAB                              const float* another, float* results );
+    virtual void calc_linear( int vec_count, int vec_size, const float** vecs,
+                              const float* another, float* results );
+    virtual void calc_rbf( int vec_count, int vec_size, const float** vecs,
+                           const float* another, float* results );
+    virtual void calc_poly( int vec_count, int vec_size, const float** vecs,
+                            const float* another, float* results );
+    virtual void calc_sigmoid( int vec_count, int vec_size, const float** vecs,
+                               const float* another, float* results );
+};
+
+
+struct CvSVMKernelRow
+{
+    CvSVMKernelRow* prev;
+    CvSVMKernelRow* next;
+    float* data;
+};
+
+
+struct CvSVMSolutionInfo
+{
+    double obj;
+    double rho;
+    double upper_bound_p;
+    double upper_bound_n;
+    double r;   // for Solver_NU
+};
+
+class CvSVMSolver
+{
+public:
+    typedef bool (CvSVMSolver::*SelectWorkingSet)( int& i, int& j );
+    typedef float* (CvSVMSolver::*GetRow)( int i, float* row, float* dst, bool existed );
+    typedef void (CvSVMSolver::*CalcRho)( double& rho, double& r );
+
+    CvSVMSolver();
+
+    CvSVMSolver( int count, int var_count, const float** samples, schar* y,
+                 int alpha_count, double* alpha, double Cp, double Cn,
+                 CvMemStorage* storage, CvSVMKernel* kernel, GetRow get_row,
+                 SelectWorkingSet select_working_set, CalcRho calc_rho );
+    virtual bool create( int count, int var_count, const float** samples, schar* y,
+                 int alpha_count, double* alpha, double Cp, double Cn,
+                 CvMemStorage* storage, CvSVMKernel* kernel, GetRow get_row,
+                 SelectWorkingSet select_working_set, CalcRho calc_rho );
+    virtual ~CvSVMSolver();
+
+    virtual void clear();
+    virtual bool solve_generic( CvSVMSolutionInfo& si );
+
+    virtual bool solve_c_svc( int count, int var_count, const float** samples, schar* y,
+                              double Cp, double Cn, CvMemStorage* storage,
+                              CvSVMKernel* kernel, double* alpha, CvSVMSolutionInfo& si );
+    virtual bool solve_nu_svc( int count, int var_count, const float** samples, schar* y,
+                               CvMemStorage* storage, CvSVMKernel* kernel,
+                               double* alpha, CvSVMSolutionInfo& si );
+    virtual bool solve_one_class( int count, int var_count, const float** samples,
+                                  CvMemStorage* storage, CvSVMKernel* kernel,
+                                  double* alpha, CvSVMSolutionInfo& si );
+
+    virtual bool solve_eps_svr( int count, int var_count, const float** samples, const float* y,
+                                CvMemStorage* storage, CvSVMKernel* kernel,
+                                double* alpha, CvSVMSolutionInfo& si );
+
+    virtual bool solve_nu_svr( int count, int var_count, const float** samples, const float* y,
+                               CvMemStorage* storage, CvSVMKernel* kernel,
+                               double* alpha, CvSVMSolutionInfo& si );
+
+    virtual float* get_row_base( int i, bool* _existed );
+    virtual float* get_row( int i, float* dst );
+
+    int sample_count;
+    int var_count;
+    int cache_size;
+    int cache_line_size;
+    const float** samples;
+    const CvSVMParams* params;
+    CvMemStorage* storage;
+    CvSVMKernelRow lru_list;
+    CvSVMKernelRow* rows;
+
+    int alpha_count;
+
+    double* G;
+    double* alpha;
+
+    // -1 - lower bound, 0 - free, 1 - upper bound
+    schar* alpha_status;
+
+    schar* y;
+    double* b;
+    float* buf[2];
+    double eps;
+    int max_iter;
+    double C[2];  // C[0] == Cn, C[1] == Cp
+    CvSVMKernel* kernel;
+
+    SelectWorkingSet select_working_set_func;
+    CalcRho calc_rho_func;
+    GetRow get_row_func;
+
+    virtual bool select_working_set( int& i, int& j );
+    virtual bool select_working_set_nu_svm( int& i, int& j );
+    virtual void calc_rho( double& rho, double& r );
+    virtual void calc_rho_nu_svm( double& rho, double& r );
+
+    virtual float* get_row_svc( int i, float* row, float* dst, bool existed );
+    virtual float* get_row_one_class( int i, float* row, float* dst, bool existed );
+    virtual float* get_row_svr( int i, float* row, float* dst, bool existed );
+};
+
+
+struct CvSVMDecisionFunc
+{
+    double rho;
+    int sv_count;
+    double* alpha;
+    int* sv_index;
+};
+
+
+// SVM model
+class CvSVM : public CvStatModel
+{
+public:
+    // SVM type
+    enum { C_SVC=100, NU_SVC=101, ONE_CLASS=102, EPS_SVR=103, NU_SVR=104 };
+
+    // SVM kernel type
+    enum { LINEAR=0, POLY=1, RBF=2, SIGMOID=3, CHI2=4, INTER=5 };
+
+    // SVM params type
+    enum { C=0, GAMMA=1, P=2, NU=3, COEF=4, DEGREE=5 };
+
+    CV_WRAP CvSVM();
+    virtual ~CvSVM();
+
+    CvSVM( const CvMat* trainData, const CvMat* responses,
+           const CvMat* varIdx=0, const CvMat* sampleIdx=0,
+           CvSVMParams params=CvSVMParams() );
+
+    virtual bool train( const CvMat* trainData, const CvMat* responses,
+                        const CvMat* varIdx=0, const CvMat* sampleIdx=0,
+                        CvSVMParams params=CvSVMParams() );
+
+    virtual bool train_auto( const CvMat* trainData, const CvMat* responses,
+        const CvMat* varIdx, const CvMat* sampleIdx, CvSVMParams params,
+        int kfold = 10,
+        CvParamGrid Cgrid      = get_default_grid(CvSVM::C),
+        CvParamGrid gammaGrid  = get_default_grid(CvSVM::GAMMA),
+        CvParamGrid pGrid      = get_default_grid(CvSVM::P),
+        CvParamGrid nuGrid     = get_default_grid(CvSVM::NU),
+        CvParamGrid coeffGrid  = get_default_grid(CvSVM::COEF),
+        CvParamGrid degreeGrid = get_default_grid(CvSVM::DEGREE),
+        bool balanced=false );
+
+    virtual float predict( const CvMat* sample, bool returnDFVal=false ) const;
+//SAB    virtual float predict( const CvMat* samples, CV_OUT CvMat* results, bool returnDFVal=false ) const;
+    virtual float predict( const CvMat* samples, CV_OUT CvMat* results) const;
+
+    CV_WRAP CvSVM( const cv::Mat& trainData, const cv::Mat& responses,
+          const cv::Mat& varIdx=cv::Mat(), const cv::Mat& sampleIdx=cv::Mat(),
+          CvSVMParams params=CvSVMParams() );
+
+    CV_WRAP virtual bool train( const cv::Mat& trainData, const cv::Mat& responses,
+                       const cv::Mat& varIdx=cv::Mat(), const cv::Mat& sampleIdx=cv::Mat(),
+                       CvSVMParams params=CvSVMParams() );
+
+    CV_WRAP virtual bool train_auto( const cv::Mat& trainData, const cv::Mat& responses,
+                            const cv::Mat& varIdx, const cv::Mat& sampleIdx, CvSVMParams params,
+                            int k_fold = 10,
+                            CvParamGrid Cgrid      = CvSVM::get_default_grid(CvSVM::C),
+                            CvParamGrid gammaGrid  = CvSVM::get_default_grid(CvSVM::GAMMA),
+                            CvParamGrid pGrid      = CvSVM::get_default_grid(CvSVM::P),
+                            CvParamGrid nuGrid     = CvSVM::get_default_grid(CvSVM::NU),
+                            CvParamGrid coeffGrid  = CvSVM::get_default_grid(CvSVM::COEF),
+                            CvParamGrid degreeGrid = CvSVM::get_default_grid(CvSVM::DEGREE),
+                            bool balanced=false);
+    CV_WRAP virtual float predict( const cv::Mat& sample, bool returnDFVal=false ) const;
+    CV_WRAP_AS(predict_all) virtual void predict( cv::InputArray samples, cv::OutputArray results ) const;
+
+    CV_WRAP virtual int get_support_vector_count() const;
+    virtual const float* get_support_vector(int i) const;
+    virtual CvSVMParams get_params() const { return params; }
+    CV_WRAP virtual void clear();
+
+    virtual const CvSVMDecisionFunc* get_decision_function() const { return decision_func; }
+
+    static CvParamGrid get_default_grid( int param_id );
+
+    virtual void write( CvFileStorage* storage, const char* name ) const;
+    virtual void read( CvFileStorage* storage, CvFileNode* node );
+    CV_WRAP int get_var_count() const { return var_idx ? var_idx->cols : var_all; }
+
+protected:
+
+    virtual bool set_params( const CvSVMParams& params );
+    virtual bool train1( int sample_count, int var_count, const float** samples,
+                    const void* responses, double Cp, double Cn,
+                    CvMemStorage* _storage, double* alpha, double& rho );
+    virtual bool do_train( int svm_type, int sample_count, int var_count, const float** samples,
+                    const CvMat* responses, CvMemStorage* _storage, double* alpha );
+    virtual void create_kernel();
+    virtual void create_solver();
+
+    virtual float predict( const float* row_sample, int row_len, bool returnDFVal=false ) const;
+
+    virtual void write_params( CvFileStorage* fs ) const;
+    virtual void read_params( CvFileStorage* fs, CvFileNode* node );
+
+    void optimize_linear_svm();
+
+    CvSVMParams params;
+    CvMat* class_labels;
+    int var_all;
+    float** sv;
+    int sv_total;
+    CvMat* var_idx;
+    CvMat* class_weights;
+    CvSVMDecisionFunc* decision_func;
+    CvMemStorage* storage;
+
+    CvSVMSolver* solver;
+    CvSVMKernel* kernel;
+
+private:
+    CvSVM(const CvSVM&);
+    CvSVM& operator = (const CvSVM&);
+};
+
+/****************************************************************************************\
+*                                      Decision Tree                                     *
+\****************************************************************************************/\
+struct CvPair16u32s
+{
+    unsigned short* u;
+    int* i;
+};
+
+
+#define CV_DTREE_CAT_DIR(idx,subset) \
+    (2*((subset[(idx)>>5]&(1 << ((idx) & 31)))==0)-1)
+
+struct CvDTreeSplit
+{
+    int var_idx;
+    int condensed_idx;
+    int inversed;
+    float quality;
+    CvDTreeSplit* next;
+    union
+    {
+        int subset[2];
+        struct
+        {
+            float c;
+            int split_point;
+        }
+        ord;
+    };
+};
+
+struct CvDTreeNode
+{
+    int class_idx;
+    int Tn;
+    double value;
+
+    CvDTreeNode* parent;
+    CvDTreeNode* left;
+    CvDTreeNode* right;
+
+    CvDTreeSplit* split;
+
+    int sample_count;
+    int depth;
+    int* num_valid;
+    int offset;
+    int buf_idx;
+    double maxlr;
+
+    // global pruning data
+    int complexity;
+    double alpha;
+    double node_risk, tree_risk, tree_error;
+
+    // cross-validation pruning data
+    int* cv_Tn;
+    double* cv_node_risk;
+    double* cv_node_error;
+
+    int get_num_valid(int vi) { return num_valid ? num_valid[vi] : sample_count; }
+    void set_num_valid(int vi, int n) { if( num_valid ) num_valid[vi] = n; }
+};
+
+
+struct CvDTreeParams
+{
+    CV_PROP_RW int   max_categories;
+    CV_PROP_RW int   max_depth;
+    CV_PROP_RW int   min_sample_count;
+    CV_PROP_RW int   cv_folds;
+    CV_PROP_RW bool  use_surrogates;
+    CV_PROP_RW bool  use_1se_rule;
+    CV_PROP_RW bool  truncate_pruned_tree;
+    CV_PROP_RW float regression_accuracy;
+    const float* priors;
+
+    CvDTreeParams();
+    CvDTreeParams( int max_depth, int min_sample_count,
+                   float regression_accuracy, bool use_surrogates,
+                   int max_categories, int cv_folds,
+                   bool use_1se_rule, bool truncate_pruned_tree,
+                   const float* priors );
+};
+
+
+struct CvDTreeTrainData
+{
+    CvDTreeTrainData();
+    CvDTreeTrainData( const CvMat* trainData, int tflag,
+                      const CvMat* responses, const CvMat* varIdx=0,
+                      const CvMat* sampleIdx=0, const CvMat* varType=0,
+                      const CvMat* missingDataMask=0,
+                      const CvDTreeParams& params=CvDTreeParams(),
+                      bool _shared=false, bool _add_labels=false );
+    virtual ~CvDTreeTrainData();
+
+    virtual void set_data( const CvMat* trainData, int tflag,
+                          const CvMat* responses, const CvMat* varIdx=0,
+                          const CvMat* sampleIdx=0, const CvMat* varType=0,
+                          const CvMat* missingDataMask=0,
+                          const CvDTreeParams& params=CvDTreeParams(),
+                          bool _shared=false, bool _add_labels=false,
+                          bool _update_data=false );
+    virtual void do_responses_copy();
+
+    virtual void get_vectors( const CvMat* _subsample_idx,
+         float* values, uchar* missing, float* responses, bool get_class_idx=false );
+
+    virtual CvDTreeNode* subsample_data( const CvMat* _subsample_idx );
+
+    virtual void write_params( CvFileStorage* fs ) const;
+    virtual void read_params( CvFileStorage* fs, CvFileNode* node );
+
+    // release all the data
+    virtual void clear();
+
+    int get_num_classes() const;
+    int get_var_type(int vi) const;
+    int get_work_var_count() const {return work_var_count;}
+
+    virtual const float* get_ord_responses( CvDTreeNode* n, float* values_buf, int* sample_indices_buf );
+    virtual const int* get_class_labels( CvDTreeNode* n, int* labels_buf );
+    virtual const int* get_cv_labels( CvDTreeNode* n, int* labels_buf );
+    virtual const int* get_sample_indices( CvDTreeNode* n, int* indices_buf );
+    virtual const int* get_cat_var_data( CvDTreeNode* n, int vi, int* cat_values_buf );
+    virtual void get_ord_var_data( CvDTreeNode* n, int vi, float* ord_values_buf, int* sorted_indices_buf,
+                                   const float** ord_values, const int** sorted_indices, int* sample_indices_buf );
+    virtual int get_child_buf_idx( CvDTreeNode* n );
+
+    ////////////////////////////////////
+
+    virtual bool set_params( const CvDTreeParams& params );
+    virtual CvDTreeNode* new_node( CvDTreeNode* parent, int count,
+                                   int storage_idx, int offset );
+
+    virtual CvDTreeSplit* new_split_ord( int vi, float cmp_val,
+                int split_point, int inversed, float quality );
+    virtual CvDTreeSplit* new_split_cat( int vi, float quality );
+    virtual void free_node_data( CvDTreeNode* node );
+    virtual void free_train_data();
+    virtual void free_node( CvDTreeNode* node );
+
+    int sample_count, var_all, var_count, max_c_count;
+    int ord_var_count, cat_var_count, work_var_count;
+    bool have_labels, have_priors;
+    bool is_classifier;
+    int tflag;
+
+    const CvMat* train_data;
+    const CvMat* responses;
+    CvMat* responses_copy; // used in Boosting
+
+    int buf_count, buf_size; // buf_size is obsolete, please do not use it, use expression ((int64)buf->rows * (int64)buf->cols / buf_count) instead
+    bool shared;
+    int is_buf_16u;
+
+    CvMat* cat_count;
+    CvMat* cat_ofs;
+    CvMat* cat_map;
+
+    CvMat* counts;
+    CvMat* buf;
+    inline size_t get_length_subbuf() const
+    {
+        size_t res = (size_t)(work_var_count + 1) * (size_t)sample_count;
+        return res;
+    }
+
+    CvMat* direction;
+    CvMat* split_buf;
+
+    CvMat* var_idx;
+    CvMat* var_type; // i-th element =
+                     //   k<0  - ordered
+                     //   k>=0 - categorical, see k-th element of cat_* arrays
+    CvMat* priors;
+    CvMat* priors_mult;
+
+    CvDTreeParams params;
+
+    CvMemStorage* tree_storage;
+    CvMemStorage* temp_storage;
+
+    CvDTreeNode* data_root;
+
+    CvSet* node_heap;
+    CvSet* split_heap;
+    CvSet* cv_heap;
+    CvSet* nv_heap;
+
+    cv::RNG* rng;
+};
+
+class CvDTree;
+class CvForestTree;
+
+namespace cv
+{
+    struct DTreeBestSplitFinder;
+    struct ForestTreeBestSplitFinder;
+}
+
+class CvDTree : public CvStatModel
+{
+public:
+    CV_WRAP CvDTree();
+    virtual ~CvDTree();
+
+    virtual bool train( const CvMat* trainData, int tflag,
+                        const CvMat* responses, const CvMat* varIdx=0,
+                        const CvMat* sampleIdx=0, const CvMat* varType=0,
+                        const CvMat* missingDataMask=0,
+                        CvDTreeParams params=CvDTreeParams() );
+
+    virtual bool train( CvMLData* trainData, CvDTreeParams params=CvDTreeParams() );
+
+    // type in {CV_TRAIN_ERROR, CV_TEST_ERROR}
+    virtual float calc_error( CvMLData* trainData, int type, std::vector<float> *resp = 0 );
+
+    virtual bool train( CvDTreeTrainData* trainData, const CvMat* subsampleIdx );
+
+    virtual CvDTreeNode* predict( const CvMat* sample, const CvMat* missingDataMask=0,
+                                  bool preprocessedInput=false ) const;
+
+    CV_WRAP virtual bool train( const cv::Mat& trainData, int tflag,
+                       const cv::Mat& responses, const cv::Mat& varIdx=cv::Mat(),
+                       const cv::Mat& sampleIdx=cv::Mat(), const cv::Mat& varType=cv::Mat(),
+                       const cv::Mat& missingDataMask=cv::Mat(),
+                       CvDTreeParams params=CvDTreeParams() );
+
+    CV_WRAP virtual CvDTreeNode* predict( const cv::Mat& sample, const cv::Mat& missingDataMask=cv::Mat(),
+                                  bool preprocessedInput=false ) const;
+    CV_WRAP virtual cv::Mat getVarImportance();
+
+    virtual const CvMat* get_var_importance();
+    CV_WRAP virtual void clear();
+
+    virtual void read( CvFileStorage* fs, CvFileNode* node );
+    virtual void write( CvFileStorage* fs, const char* name ) const;
+
+    // special read & write methods for trees in the tree ensembles
+    virtual void read( CvFileStorage* fs, CvFileNode* node,
+                       CvDTreeTrainData* data );
+    virtual void write( CvFileStorage* fs ) const;
+
+    const CvDTreeNode* get_root() const;
+    int get_pruned_tree_idx() const;
+    CvDTreeTrainData* get_data();
+
+protected:
+    friend struct cv::DTreeBestSplitFinder;
+
+    virtual bool do_train( const CvMat* _subsample_idx );
+
+    virtual void try_split_node( CvDTreeNode* n );
+    virtual void split_node_data( CvDTreeNode* n );
+    virtual CvDTreeSplit* find_best_split( CvDTreeNode* n );
+    virtual CvDTreeSplit* find_split_ord_class( CvDTreeNode* n, int vi,
+                            float init_quality = 0, CvDTreeSplit* _split = 0, uchar* ext_buf = 0 );
+    virtual CvDTreeSplit* find_split_cat_class( CvDTreeNode* n, int vi,
+                            float init_quality = 0, CvDTreeSplit* _split = 0, uchar* ext_buf = 0 );
+    virtual CvDTreeSplit* find_split_ord_reg( CvDTreeNode* n, int vi,
+                            float init_quality = 0, CvDTreeSplit* _split = 0, uchar* ext_buf = 0 );
+    virtual CvDTreeSplit* find_split_cat_reg( CvDTreeNode* n, int vi,
+                            float init_quality = 0, CvDTreeSplit* _split = 0, uchar* ext_buf = 0 );
+    virtual CvDTreeSplit* find_surrogate_split_ord( CvDTreeNode* n, int vi, uchar* ext_buf = 0 );
+    virtual CvDTreeSplit* find_surrogate_split_cat( CvDTreeNode* n, int vi, uchar* ext_buf = 0 );
+    virtual double calc_node_dir( CvDTreeNode* node );
+    virtual void complete_node_dir( CvDTreeNode* node );
+    virtual void cluster_categories( const int* vectors, int vector_count,
+        int var_count, int* sums, int k, int* cluster_labels );
+
+    virtual void calc_node_value( CvDTreeNode* node );
+
+    virtual void prune_cv();
+    virtual double update_tree_rnc( int T, int fold );
+    virtual int cut_tree( int T, int fold, double min_alpha );
+    virtual void free_prune_data(bool cut_tree);
+    virtual void free_tree();
+
+    virtual void write_node( CvFileStorage* fs, CvDTreeNode* node ) const;
+    virtual void write_split( CvFileStorage* fs, CvDTreeSplit* split ) const;
+    virtual CvDTreeNode* read_node( CvFileStorage* fs, CvFileNode* node, CvDTreeNode* parent );
+    virtual CvDTreeSplit* read_split( CvFileStorage* fs, CvFileNode* node );
+    virtual void write_tree_nodes( CvFileStorage* fs ) const;
+    virtual void read_tree_nodes( CvFileStorage* fs, CvFileNode* node );
+
+    CvDTreeNode* root;
+    CvMat* var_importance;
+    CvDTreeTrainData* data;
+    CvMat train_data_hdr, responses_hdr;
+    cv::Mat train_data_mat, responses_mat;
+
+public:
+    int pruned_tree_idx;
+};
+
+
+/****************************************************************************************\
+*                                   Random Trees Classifier                              *
+\****************************************************************************************/
+
+class CvRTrees;
+
+class CvForestTree: public CvDTree
+{
+public:
+    CvForestTree();
+    virtual ~CvForestTree();
+
+    using CvDTree::train;
+    virtual bool train( CvDTreeTrainData* trainData, const CvMat* _subsample_idx, CvRTrees* forest );
+
+    virtual int get_var_count() const {return data ? data->var_count : 0;}
+    virtual void read( CvFileStorage* fs, CvFileNode* node, CvRTrees* forest, CvDTreeTrainData* _data );
+
+    /* dummy methods to avoid warnings: BEGIN */
+    virtual bool train( const CvMat* trainData, int tflag,
+                        const CvMat* responses, const CvMat* varIdx=0,
+                        const CvMat* sampleIdx=0, const CvMat* varType=0,
+                        const CvMat* missingDataMask=0,
+                        CvDTreeParams params=CvDTreeParams() );
+
+    virtual bool train( CvDTreeTrainData* trainData, const CvMat* _subsample_idx );
+    virtual void read( CvFileStorage* fs, CvFileNode* node );
+    virtual void read( CvFileStorage* fs, CvFileNode* node,
+                       CvDTreeTrainData* data );
+    /* dummy methods to avoid warnings: END */
+
+protected:
+    friend struct cv::ForestTreeBestSplitFinder;
+
+    virtual CvDTreeSplit* find_best_split( CvDTreeNode* n );
+    CvRTrees* forest;
+};
+
+
+struct CvRTParams : public CvDTreeParams
+{
+    //Parameters for the forest
+    CV_PROP_RW bool calc_var_importance; // true <=> RF processes variable importance
+    CV_PROP_RW int nactive_vars;
+    CV_PROP_RW CvTermCriteria term_crit;
+
+    CvRTParams();
+    CvRTParams( int max_depth, int min_sample_count,
+                float regression_accuracy, bool use_surrogates,
+                int max_categories, const float* priors, bool calc_var_importance,
+                int nactive_vars, int max_num_of_trees_in_the_forest,
+                float forest_accuracy, int termcrit_type );
+};
+
+
+class CvRTrees : public CvStatModel
+{
+public:
+    CV_WRAP CvRTrees();
+    virtual ~CvRTrees();
+    virtual bool train( const CvMat* trainData, int tflag,
+                        const CvMat* responses, const CvMat* varIdx=0,
+                        const CvMat* sampleIdx=0, const CvMat* varType=0,
+                        const CvMat* missingDataMask=0,
+                        CvRTParams params=CvRTParams() );
+
+    virtual bool train( CvMLData* data, CvRTParams params=CvRTParams() );
+    virtual float predict( const CvMat* sample, const CvMat* missing = 0 ) const;
+    virtual float predict_prob( const CvMat* sample, const CvMat* missing = 0 ) const;
+
+    CV_WRAP virtual bool train( const cv::Mat& trainData, int tflag,
+                       const cv::Mat& responses, const cv::Mat& varIdx=cv::Mat(),
+                       const cv::Mat& sampleIdx=cv::Mat(), const cv::Mat& varType=cv::Mat(),
+                       const cv::Mat& missingDataMask=cv::Mat(),
+                       CvRTParams params=CvRTParams() );
+    CV_WRAP virtual float predict( const cv::Mat& sample, const cv::Mat& missing = cv::Mat() ) const;
+    CV_WRAP virtual float predict_prob( const cv::Mat& sample, const cv::Mat& missing = cv::Mat() ) const;
+    CV_WRAP virtual cv::Mat getVarImportance();
+
+    CV_WRAP virtual void clear();
+
+    virtual const CvMat* get_var_importance();
+    virtual float get_proximity( const CvMat* sample1, const CvMat* sample2,
+        const CvMat* missing1 = 0, const CvMat* missing2 = 0 ) const;
+
+    virtual float calc_error( CvMLData* data, int type , std::vector<float>* resp = 0 ); // type in {CV_TRAIN_ERROR, CV_TEST_ERROR}
+
+    virtual float get_train_error();
+
+    virtual void read( CvFileStorage* fs, CvFileNode* node );
+    virtual void write( CvFileStorage* fs, const char* name ) const;
+
+    CvMat* get_active_var_mask();
+    CvRNG* get_rng();
+
+    int get_tree_count() const;
+    CvForestTree* get_tree(int i) const;
+
+protected:
+    virtual cv::String getName() const;
+
+    virtual bool grow_forest( const CvTermCriteria term_crit );
+
+    // array of the trees of the forest
+    CvForestTree** trees;
+    CvDTreeTrainData* data;
+    CvMat train_data_hdr, responses_hdr;
+    cv::Mat train_data_mat, responses_mat;
+    int ntrees;
+    int nclasses;
+    double oob_error;
+    CvMat* var_importance;
+    int nsamples;
+
+    cv::RNG* rng;
+    CvMat* active_var_mask;
+};
+
+/****************************************************************************************\
+*                           Extremely randomized trees Classifier                        *
+\****************************************************************************************/
+struct CvERTreeTrainData : public CvDTreeTrainData
+{
+    virtual void set_data( const CvMat* trainData, int tflag,
+                          const CvMat* responses, const CvMat* varIdx=0,
+                          const CvMat* sampleIdx=0, const CvMat* varType=0,
+                          const CvMat* missingDataMask=0,
+                          const CvDTreeParams& params=CvDTreeParams(),
+                          bool _shared=false, bool _add_labels=false,
+                          bool _update_data=false );
+    virtual void get_ord_var_data( CvDTreeNode* n, int vi, float* ord_values_buf, int* missing_buf,
+                                   const float** ord_values, const int** missing, int* sample_buf = 0 );
+    virtual const int* get_sample_indices( CvDTreeNode* n, int* indices_buf );
+    virtual const int* get_cv_labels( CvDTreeNode* n, int* labels_buf );
+    virtual const int* get_cat_var_data( CvDTreeNode* n, int vi, int* cat_values_buf );
+    virtual void get_vectors( const CvMat* _subsample_idx, float* values, uchar* missing,
+                              float* responses, bool get_class_idx=false );
+    virtual CvDTreeNode* subsample_data( const CvMat* _subsample_idx );
+    const CvMat* missing_mask;
+};
+
+class CvForestERTree : public CvForestTree
+{
+protected:
+    virtual double calc_node_dir( CvDTreeNode* node );
+    virtual CvDTreeSplit* find_split_ord_class( CvDTreeNode* n, int vi,
+        float init_quality = 0, CvDTreeSplit* _split = 0, uchar* ext_buf = 0 );
+    virtual CvDTreeSplit* find_split_cat_class( CvDTreeNode* n, int vi,
+        float init_quality = 0, CvDTreeSplit* _split = 0, uchar* ext_buf = 0 );
+    virtual CvDTreeSplit* find_split_ord_reg( CvDTreeNode* n, int vi,
+        float init_quality = 0, CvDTreeSplit* _split = 0, uchar* ext_buf = 0 );
+    virtual CvDTreeSplit* find_split_cat_reg( CvDTreeNode* n, int vi,
+        float init_quality = 0, CvDTreeSplit* _split = 0, uchar* ext_buf = 0 );
+    virtual void split_node_data( CvDTreeNode* n );
+};
+
+#if 0 // SAB
+class CvERTrees : public CvRTrees
+{
+public:
+    CV_WRAP CvERTrees();
+    virtual ~CvERTrees();
+    virtual bool train( const CvMat* trainData, int tflag,
+                        const CvMat* responses, const CvMat* varIdx=0,
+                        const CvMat* sampleIdx=0, const CvMat* varType=0,
+                        const CvMat* missingDataMask=0,
+                        CvRTParams params=CvRTParams());
+    CV_WRAP virtual bool train( const cv::Mat& trainData, int tflag,
+                       const cv::Mat& responses, const cv::Mat& varIdx=cv::Mat(),
+                       const cv::Mat& sampleIdx=cv::Mat(), const cv::Mat& varType=cv::Mat(),
+                       const cv::Mat& missingDataMask=cv::Mat(),
+                       CvRTParams params=CvRTParams());
+    virtual bool train( CvMLData* data, CvRTParams params=CvRTParams() );
+protected:
+    virtual cv::String getName() const;
+    virtual bool grow_forest( const CvTermCriteria term_crit );
+};
+#endif //#if 0
+
+
+/****************************************************************************************\
+*                                   Boosted tree classifier                              *
+\****************************************************************************************/
+
+struct CvBoostParams : public CvDTreeParams
+{
+    CV_PROP_RW int boost_type;
+    CV_PROP_RW int weak_count;
+    CV_PROP_RW int split_criteria;
+    CV_PROP_RW double weight_trim_rate;
+
+    CvBoostParams();
+    CvBoostParams( int boost_type, int weak_count, double weight_trim_rate,
+                   int max_depth, bool use_surrogates, const float* priors );
+};
+
+
+class CvBoost;
+
+class CvBoostTree: public CvDTree
+{
+public:
+    CvBoostTree();
+    virtual ~CvBoostTree();
+
+    using CvDTree::train;
+    virtual bool train( CvDTreeTrainData* trainData,
+                        const CvMat* subsample_idx, CvBoost* ensemble );
+
+    virtual void scale( double s );
+    virtual void read( CvFileStorage* fs, CvFileNode* node,
+                       CvBoost* ensemble, CvDTreeTrainData* _data );
+    virtual void clear();
+
+    /* dummy methods to avoid warnings: BEGIN */
+    virtual bool train( const CvMat* trainData, int tflag,
+                        const CvMat* responses, const CvMat* varIdx=0,
+                        const CvMat* sampleIdx=0, const CvMat* varType=0,
+                        const CvMat* missingDataMask=0,
+                        CvDTreeParams params=CvDTreeParams() );
+    virtual bool train( CvDTreeTrainData* trainData, const CvMat* _subsample_idx );
+
+    virtual void read( CvFileStorage* fs, CvFileNode* node );
+    virtual void read( CvFileStorage* fs, CvFileNode* node,
+                       CvDTreeTrainData* data );
+    /* dummy methods to avoid warnings: END */
+
+protected:
+
+    virtual void try_split_node( CvDTreeNode* n );
+    virtual CvDTreeSplit* find_surrogate_split_ord( CvDTreeNode* n, int vi, uchar* ext_buf = 0 );
+    virtual CvDTreeSplit* find_surrogate_split_cat( CvDTreeNode* n, int vi, uchar* ext_buf = 0 );
+    virtual CvDTreeSplit* find_split_ord_class( CvDTreeNode* n, int vi,
+        float init_quality = 0, CvDTreeSplit* _split = 0, uchar* ext_buf = 0 );
+    virtual CvDTreeSplit* find_split_cat_class( CvDTreeNode* n, int vi,
+        float init_quality = 0, CvDTreeSplit* _split = 0, uchar* ext_buf = 0 );
+    virtual CvDTreeSplit* find_split_ord_reg( CvDTreeNode* n, int vi,
+        float init_quality = 0, CvDTreeSplit* _split = 0, uchar* ext_buf = 0 );
+    virtual CvDTreeSplit* find_split_cat_reg( CvDTreeNode* n, int vi,
+        float init_quality = 0, CvDTreeSplit* _split = 0, uchar* ext_buf = 0 );
+    virtual void calc_node_value( CvDTreeNode* n );
+    virtual double calc_node_dir( CvDTreeNode* n );
+
+    CvBoost* ensemble;
+};
+
+
+class CvBoost : public CvStatModel
+{
+public:
+    // Boosting type
+    enum { DISCRETE=0, REAL=1, LOGIT=2, GENTLE=3 };
+
+    // Splitting criteria
+    enum { DEFAULT=0, GINI=1, MISCLASS=3, SQERR=4 };
+
+    CV_WRAP CvBoost();
+    virtual ~CvBoost();
+
+    CvBoost( const CvMat* trainData, int tflag,
+             const CvMat* responses, const CvMat* varIdx=0,
+             const CvMat* sampleIdx=0, const CvMat* varType=0,
+             const CvMat* missingDataMask=0,
+             CvBoostParams params=CvBoostParams() );
+
+    virtual bool train( const CvMat* trainData, int tflag,
+             const CvMat* responses, const CvMat* varIdx=0,
+             const CvMat* sampleIdx=0, const CvMat* varType=0,
+             const CvMat* missingDataMask=0,
+             CvBoostParams params=CvBoostParams(),
+             bool update=false );
+
+    virtual bool train( CvMLData* data,
+             CvBoostParams params=CvBoostParams(),
+             bool update=false );
+
+    virtual float predict( const CvMat* sample, const CvMat* missing=0,
+                           CvMat* weak_responses=0, CvSlice slice=CV_WHOLE_SEQ,
+                           bool raw_mode=false, bool return_sum=false ) const;
+
+    CV_WRAP CvBoost( const cv::Mat& trainData, int tflag,
+            const cv::Mat& responses, const cv::Mat& varIdx=cv::Mat(),
+            const cv::Mat& sampleIdx=cv::Mat(), const cv::Mat& varType=cv::Mat(),
+            const cv::Mat& missingDataMask=cv::Mat(),
+            CvBoostParams params=CvBoostParams() );
+
+    CV_WRAP virtual bool train( const cv::Mat& trainData, int tflag,
+                       const cv::Mat& responses, const cv::Mat& varIdx=cv::Mat(),
+                       const cv::Mat& sampleIdx=cv::Mat(), const cv::Mat& varType=cv::Mat(),
+                       const cv::Mat& missingDataMask=cv::Mat(),
+                       CvBoostParams params=CvBoostParams(),
+                       bool update=false );
+
+    CV_WRAP virtual float predict( const cv::Mat& sample, const cv::Mat& missing=cv::Mat(),
+                                   const cv::Range& slice=cv::Range::all(), bool rawMode=false,
+                                   bool returnSum=false ) const;
+
+    virtual float calc_error( CvMLData* _data, int type , std::vector<float> *resp = 0 ); // type in {CV_TRAIN_ERROR, CV_TEST_ERROR}
+
+    CV_WRAP virtual void prune( CvSlice slice );
+
+    CV_WRAP virtual void clear();
+
+    virtual void write( CvFileStorage* storage, const char* name ) const;
+    virtual void read( CvFileStorage* storage, CvFileNode* node );
+    virtual const CvMat* get_active_vars(bool absolute_idx=true);
+
+    CvSeq* get_weak_predictors();
+
+    CvMat* get_weights();
+    CvMat* get_subtree_weights();
+    CvMat* get_weak_response();
+    const CvBoostParams& get_params() const;
+    const CvDTreeTrainData* get_data() const;
+
+protected:
+
+    virtual bool set_params( const CvBoostParams& params );
+    virtual void update_weights( CvBoostTree* tree );
+    virtual void trim_weights();
+    virtual void write_params( CvFileStorage* fs ) const;
+    virtual void read_params( CvFileStorage* fs, CvFileNode* node );
+
+    virtual void initialize_weights(double (&p)[2]);
+
+    CvDTreeTrainData* data;
+    CvMat train_data_hdr, responses_hdr;
+    cv::Mat train_data_mat, responses_mat;
+    CvBoostParams params;
+    CvSeq* weak;
+
+    CvMat* active_vars;
+    CvMat* active_vars_abs;
+    bool have_active_cat_vars;
+
+    CvMat* orig_response;
+    CvMat* sum_response;
+    CvMat* weak_eval;
+    CvMat* subsample_mask;
+    CvMat* weights;
+    CvMat* subtree_weights;
+    bool have_subsample;
+};
+
+
+/****************************************************************************************\
+*                                   Gradient Boosted Trees                               *
+\****************************************************************************************/
+
+// DataType: STRUCT CvGBTreesParams
+// Parameters of GBT (Gradient Boosted trees model), including single
+// tree settings and ensemble parameters.
+//
+// weak_count          - count of trees in the ensemble
+// loss_function_type  - loss function used for ensemble training
+// subsample_portion   - portion of whole training set used for
+//                       every single tree training.
+//                       subsample_portion value is in (0.0, 1.0].
+//                       subsample_portion == 1.0 when whole dataset is
+//                       used on each step. Count of sample used on each
+//                       step is computed as
+//                       int(total_samples_count * subsample_portion).
+// shrinkage           - regularization parameter.
+//                       Each tree prediction is multiplied on shrinkage value.
+
+
+#if 0 // SAB
+struct CvGBTreesParams : public CvDTreeParams
+{
+    CV_PROP_RW int weak_count;
+    CV_PROP_RW int loss_function_type;
+    CV_PROP_RW float subsample_portion;
+    CV_PROP_RW float shrinkage;
+
+    CvGBTreesParams();
+    CvGBTreesParams( int loss_function_type, int weak_count, float shrinkage,
+        float subsample_portion, int max_depth, bool use_surrogates );
+};
+
+// DataType: CLASS CvGBTrees
+// Gradient Boosting Trees (GBT) algorithm implementation.
+//
+// data             - training dataset
+// params           - parameters of the CvGBTrees
+// weak             - array[0..(class_count-1)] of CvSeq
+//                    for storing tree ensembles
+// orig_response    - original responses of the training set samples
+// sum_response     - predictions of the current model on the training dataset.
+//                    this matrix is updated on every iteration.
+// sum_response_tmp - predictions of the model on the training set on the next
+//                    step. On every iteration values of sum_responses_tmp are
+//                    computed via sum_responses values. When the current
+//                    step is complete sum_response values become equal to
+//                    sum_responses_tmp.
+// sampleIdx       - indices of samples used for training the ensemble.
+//                    CvGBTrees training procedure takes a set of samples
+//                    (train_data) and a set of responses (responses).
+//                    Only pairs (train_data[i], responses[i]), where i is
+//                    in sample_idx are used for training the ensemble.
+// subsample_train  - indices of samples used for training a single decision
+//                    tree on the current step. This indices are countered
+//                    relatively to the sample_idx, so that pairs
+//                    (train_data[sample_idx[i]], responses[sample_idx[i]])
+//                    are used for training a decision tree.
+//                    Training set is randomly splited
+//                    in two parts (subsample_train and subsample_test)
+//                    on every iteration accordingly to the portion parameter.
+// subsample_test   - relative indices of samples from the training set,
+//                    which are not used for training a tree on the current
+//                    step.
+// missing          - mask of the missing values in the training set. This
+//                    matrix has the same size as train_data. 1 - missing
+//                    value, 0 - not a missing value.
+// class_labels     - output class labels map.
+// rng              - random number generator. Used for splitting the
+//                    training set.
+// class_count      - count of output classes.
+//                    class_count == 1 in the case of regression,
+//                    and > 1 in the case of classification.
+// delta            - Huber loss function parameter.
+// base_value       - start point of the gradient descent procedure.
+//                    model prediction is
+//                    f(x) = f_0 + sum_{i=1..weak_count-1}(f_i(x)), where
+//                    f_0 is the base value.
+
+
+class CvGBTrees : public CvStatModel
+{
+public:
+
+    /*
+    // DataType: ENUM
+    // Loss functions implemented in CvGBTrees.
+    //
+    // SQUARED_LOSS
+    // problem: regression
+    // loss = (x - x')^2
+    //
+    // ABSOLUTE_LOSS
+    // problem: regression
+    // loss = abs(x - x')
+    //
+    // HUBER_LOSS
+    // problem: regression
+    // loss = delta*( abs(x - x') - delta/2), if abs(x - x') > delta
+    //           1/2*(x - x')^2, if abs(x - x') <= delta,
+    //           where delta is the alpha-quantile of pseudo responses from
+    //           the training set.
+    //
+    // DEVIANCE_LOSS
+    // problem: classification
+    //
+    */
+    enum {SQUARED_LOSS=0, ABSOLUTE_LOSS, HUBER_LOSS=3, DEVIANCE_LOSS};
+
+
+    /*
+    // Default constructor. Creates a model only (without training).
+    // Should be followed by one form of the train(...) function.
+    //
+    // API
+    // CvGBTrees();
+
+    // INPUT
+    // OUTPUT
+    // RESULT
+    */
+    CV_WRAP CvGBTrees();
+
+
+    /*
+    // Full form constructor. Creates a gradient boosting model and does the
+    // train.
+    //
+    // API
+    // CvGBTrees( const CvMat* trainData, int tflag,
+             const CvMat* responses, const CvMat* varIdx=0,
+             const CvMat* sampleIdx=0, const CvMat* varType=0,
+             const CvMat* missingDataMask=0,
+             CvGBTreesParams params=CvGBTreesParams() );
+
+    // INPUT
+    // trainData    - a set of input feature vectors.
+    //                  size of matrix is
+    //                  <count of samples> x <variables count>
+    //                  or <variables count> x <count of samples>
+    //                  depending on the tflag parameter.
+    //                  matrix values are float.
+    // tflag         - a flag showing how do samples stored in the
+    //                  trainData matrix row by row (tflag=CV_ROW_SAMPLE)
+    //                  or column by column (tflag=CV_COL_SAMPLE).
+    // responses     - a vector of responses corresponding to the samples
+    //                  in trainData.
+    // varIdx       - indices of used variables. zero value means that all
+    //                  variables are active.
+    // sampleIdx    - indices of used samples. zero value means that all
+    //                  samples from trainData are in the training set.
+    // varType      - vector of <variables count> length. gives every
+    //                  variable type CV_VAR_CATEGORICAL or CV_VAR_ORDERED.
+    //                  varType = 0 means all variables are numerical.
+    // missingDataMask  - a mask of misiing values in trainData.
+    //                  missingDataMask = 0 means that there are no missing
+    //                  values.
+    // params         - parameters of GTB algorithm.
+    // OUTPUT
+    // RESULT
+    */
+    CvGBTrees( const CvMat* trainData, int tflag,
+             const CvMat* responses, const CvMat* varIdx=0,
+             const CvMat* sampleIdx=0, const CvMat* varType=0,
+             const CvMat* missingDataMask=0,
+             CvGBTreesParams params=CvGBTreesParams() );
+
+
+    /*
+    // Destructor.
+    */
+    virtual ~CvGBTrees();
+
+
+    /*
+    // Gradient tree boosting model training
+    //
+    // API
+    // virtual bool train( const CvMat* trainData, int tflag,
+             const CvMat* responses, const CvMat* varIdx=0,
+             const CvMat* sampleIdx=0, const CvMat* varType=0,
+             const CvMat* missingDataMask=0,
+             CvGBTreesParams params=CvGBTreesParams(),
+             bool update=false );
+
+    // INPUT
+    // trainData    - a set of input feature vectors.
+    //                  size of matrix is
+    //                  <count of samples> x <variables count>
+    //                  or <variables count> x <count of samples>
+    //                  depending on the tflag parameter.
+    //                  matrix values are float.
+    // tflag         - a flag showing how do samples stored in the
+    //                  trainData matrix row by row (tflag=CV_ROW_SAMPLE)
+    //                  or column by column (tflag=CV_COL_SAMPLE).
+    // responses     - a vector of responses corresponding to the samples
+    //                  in trainData.
+    // varIdx       - indices of used variables. zero value means that all
+    //                  variables are active.
+    // sampleIdx    - indices of used samples. zero value means that all
+    //                  samples from trainData are in the training set.
+    // varType      - vector of <variables count> length. gives every
+    //                  variable type CV_VAR_CATEGORICAL or CV_VAR_ORDERED.
+    //                  varType = 0 means all variables are numerical.
+    // missingDataMask  - a mask of misiing values in trainData.
+    //                  missingDataMask = 0 means that there are no missing
+    //                  values.
+    // params         - parameters of GTB algorithm.
+    // update         - is not supported now. (!)
+    // OUTPUT
+    // RESULT
+    // Error state.
+    */
+    virtual bool train( const CvMat* trainData, int tflag,
+             const CvMat* responses, const CvMat* varIdx=0,
+             const CvMat* sampleIdx=0, const CvMat* varType=0,
+             const CvMat* missingDataMask=0,
+             CvGBTreesParams params=CvGBTreesParams(),
+             bool update=false );
+
+
+    /*
+    // Gradient tree boosting model training
+    //
+    // API
+    // virtual bool train( CvMLData* data,
+             CvGBTreesParams params=CvGBTreesParams(),
+             bool update=false ) {return false;}
+
+    // INPUT
+    // data          - training set.
+    // params        - parameters of GTB algorithm.
+    // update        - is not supported now. (!)
+    // OUTPUT
+    // RESULT
+    // Error state.
+    */
+    virtual bool train( CvMLData* data,
+             CvGBTreesParams params=CvGBTreesParams(),
+             bool update=false );
+
+
+    /*
+    // Response value prediction
+    //
+    // API
+    // virtual float predict_serial( const CvMat* sample, const CvMat* missing=0,
+             CvMat* weak_responses=0, CvSlice slice = CV_WHOLE_SEQ,
+             int k=-1 ) const;
+
+    // INPUT
+    // sample         - input sample of the same type as in the training set.
+    // missing        - missing values mask. missing=0 if there are no
+    //                   missing values in sample vector.
+    // weak_responses  - predictions of all of the trees.
+    //                   not implemented (!)
+    // slice           - part of the ensemble used for prediction.
+    //                   slice = CV_WHOLE_SEQ when all trees are used.
+    // k               - number of ensemble used.
+    //                   k is in {-1,0,1,..,<count of output classes-1>}.
+    //                   in the case of classification problem
+    //                   <count of output classes-1> ensembles are built.
+    //                   If k = -1 ordinary prediction is the result,
+    //                   otherwise function gives the prediction of the
+    //                   k-th ensemble only.
+    // OUTPUT
+    // RESULT
+    // Predicted value.
+    */
+    virtual float predict_serial( const CvMat* sample, const CvMat* missing=0,
+            CvMat* weakResponses=0, CvSlice slice = CV_WHOLE_SEQ,
+            int k=-1 ) const;
+
+    /*
+    // Response value prediction.
+    // Parallel version (in the case of TBB existence)
+    //
+    // API
+    // virtual float predict( const CvMat* sample, const CvMat* missing=0,
+             CvMat* weak_responses=0, CvSlice slice = CV_WHOLE_SEQ,
+             int k=-1 ) const;
+
+    // INPUT
+    // sample         - input sample of the same type as in the training set.
+    // missing        - missing values mask. missing=0 if there are no
+    //                   missing values in sample vector.
+    // weak_responses  - predictions of all of the trees.
+    //                   not implemented (!)
+    // slice           - part of the ensemble used for prediction.
+    //                   slice = CV_WHOLE_SEQ when all trees are used.
+    // k               - number of ensemble used.
+    //                   k is in {-1,0,1,..,<count of output classes-1>}.
+    //                   in the case of classification problem
+    //                   <count of output classes-1> ensembles are built.
+    //                   If k = -1 ordinary prediction is the result,
+    //                   otherwise function gives the prediction of the
+    //                   k-th ensemble only.
+    // OUTPUT
+    // RESULT
+    // Predicted value.
+    */
+    virtual float predict( const CvMat* sample, const CvMat* missing=0,
+            CvMat* weakResponses=0, CvSlice slice = CV_WHOLE_SEQ,
+            int k=-1 ) const;
+
+    /*
+    // Deletes all the data.
+    //
+    // API
+    // virtual void clear();
+
+    // INPUT
+    // OUTPUT
+    // delete data, weak, orig_response, sum_response,
+    //        weak_eval, subsample_train, subsample_test,
+    //        sample_idx, missing, lass_labels
+    // delta = 0.0
+    // RESULT
+    */
+    CV_WRAP virtual void clear();
+
+    /*
+    // Compute error on the train/test set.
+    //
+    // API
+    // virtual float calc_error( CvMLData* _data, int type,
+    //        std::vector<float> *resp = 0 );
+    //
+    // INPUT
+    // data  - dataset
+    // type  - defines which error is to compute: train (CV_TRAIN_ERROR) or
+    //         test (CV_TEST_ERROR).
+    // OUTPUT
+    // resp  - vector of predictions
+    // RESULT
+    // Error value.
+    */
+    virtual float calc_error( CvMLData* _data, int type,
+            std::vector<float> *resp = 0 );
+
+    /*
+    //
+    // Write parameters of the gtb model and data. Write learned model.
+    //
+    // API
+    // virtual void write( CvFileStorage* fs, const char* name ) const;
+    //
+    // INPUT
+    // fs     - file storage to read parameters from.
+    // name   - model name.
+    // OUTPUT
+    // RESULT
+    */
+    virtual void write( CvFileStorage* fs, const char* name ) const;
+
+
+    /*
+    //
+    // Read parameters of the gtb model and data. Read learned model.
+    //
+    // API
+    // virtual void read( CvFileStorage* fs, CvFileNode* node );
+    //
+    // INPUT
+    // fs     - file storage to read parameters from.
+    // node   - file node.
+    // OUTPUT
+    // RESULT
+    */
+    virtual void read( CvFileStorage* fs, CvFileNode* node );
+
+
+    // new-style C++ interface
+    CV_WRAP CvGBTrees( const cv::Mat& trainData, int tflag,
+              const cv::Mat& responses, const cv::Mat& varIdx=cv::Mat(),
+              const cv::Mat& sampleIdx=cv::Mat(), const cv::Mat& varType=cv::Mat(),
+              const cv::Mat& missingDataMask=cv::Mat(),
+              CvGBTreesParams params=CvGBTreesParams() );
+
+    CV_WRAP virtual bool train( const cv::Mat& trainData, int tflag,
+                       const cv::Mat& responses, const cv::Mat& varIdx=cv::Mat(),
+                       const cv::Mat& sampleIdx=cv::Mat(), const cv::Mat& varType=cv::Mat(),
+                       const cv::Mat& missingDataMask=cv::Mat(),
+                       CvGBTreesParams params=CvGBTreesParams(),
+                       bool update=false );
+
+    CV_WRAP virtual float predict( const cv::Mat& sample, const cv::Mat& missing=cv::Mat(),
+                           const cv::Range& slice = cv::Range::all(),
+                           int k=-1 ) const;
+
+protected:
+
+    /*
+    // Compute the gradient vector components.
+    //
+    // API
+    // virtual void find_gradient( const int k = 0);
+
+    // INPUT
+    // k        - used for classification problem, determining current
+    //            tree ensemble.
+    // OUTPUT
+    // changes components of data->responses
+    // which correspond to samples used for training
+    // on the current step.
+    // RESULT
+    */
+    virtual void find_gradient( const int k = 0);
+
+
+    /*
+    //
+    // Change values in tree leaves according to the used loss function.
+    //
+    // API
+    // virtual void change_values(CvDTree* tree, const int k = 0);
+    //
+    // INPUT
+    // tree      - decision tree to change.
+    // k         - used for classification problem, determining current
+    //             tree ensemble.
+    // OUTPUT
+    // changes 'value' fields of the trees' leaves.
+    // changes sum_response_tmp.
+    // RESULT
+    */
+    virtual void change_values(CvDTree* tree, const int k = 0);
+
+
+    /*
+    //
+    // Find optimal constant prediction value according to the used loss
+    // function.
+    // The goal is to find a constant which gives the minimal summary loss
+    // on the _Idx samples.
+    //
+    // API
+    // virtual float find_optimal_value( const CvMat* _Idx );
+    //
+    // INPUT
+    // _Idx        - indices of the samples from the training set.
+    // OUTPUT
+    // RESULT
+    // optimal constant value.
+    */
+    virtual float find_optimal_value( const CvMat* _Idx );
+
+
+    /*
+    //
+    // Randomly split the whole training set in two parts according
+    // to params.portion.
+    //
+    // API
+    // virtual void do_subsample();
+    //
+    // INPUT
+    // OUTPUT
+    // subsample_train - indices of samples used for training
+    // subsample_test  - indices of samples used for test
+    // RESULT
+    */
+    virtual void do_subsample();
+
+
+    /*
+    //
+    // Internal recursive function giving an array of subtree tree leaves.
+    //
+    // API
+    // void leaves_get( CvDTreeNode** leaves, int& count, CvDTreeNode* node );
+    //
+    // INPUT
+    // node         - current leaf.
+    // OUTPUT
+    // count        - count of leaves in the subtree.
+    // leaves       - array of pointers to leaves.
+    // RESULT
+    */
+    void leaves_get( CvDTreeNode** leaves, int& count, CvDTreeNode* node );
+
+
+    /*
+    //
+    // Get leaves of the tree.
+    //
+    // API
+    // CvDTreeNode** GetLeaves( const CvDTree* dtree, int& len );
+    //
+    // INPUT
+    // dtree            - decision tree.
+    // OUTPUT
+    // len              - count of the leaves.
+    // RESULT
+    // CvDTreeNode**    - array of pointers to leaves.
+    */
+    CvDTreeNode** GetLeaves( const CvDTree* dtree, int& len );
+
+
+    /*
+    //
+    // Is it a regression or a classification.
+    //
+    // API
+    // bool problem_type();
+    //
+    // INPUT
+    // OUTPUT
+    // RESULT
+    // false if it is a classification problem,
+    // true - if regression.
+    */
+    virtual bool problem_type() const;
+
+
+    /*
+    //
+    // Write parameters of the gtb model.
+    //
+    // API
+    // virtual void write_params( CvFileStorage* fs ) const;
+    //
+    // INPUT
+    // fs           - file storage to write parameters to.
+    // OUTPUT
+    // RESULT
+    */
+    virtual void write_params( CvFileStorage* fs ) const;
+
+
+    /*
+    //
+    // Read parameters of the gtb model and data.
+    //
+    // API
+    // virtual void read_params( CvFileStorage* fs );
+    //
+    // INPUT
+    // fs           - file storage to read parameters from.
+    // OUTPUT
+    // params       - parameters of the gtb model.
+    // data         - contains information about the structure
+    //                of the data set (count of variables,
+    //                their types, etc.).
+    // class_labels - output class labels map.
+    // RESULT
+    */
+    virtual void read_params( CvFileStorage* fs, CvFileNode* fnode );
+    int get_len(const CvMat* mat) const;
+
+
+    CvDTreeTrainData* data;
+    CvGBTreesParams params;
+
+    CvSeq** weak;
+    CvMat* orig_response;
+    CvMat* sum_response;
+    CvMat* sum_response_tmp;
+    CvMat* sample_idx;
+    CvMat* subsample_train;
+    CvMat* subsample_test;
+    CvMat* missing;
+    CvMat* class_labels;
+
+    cv::RNG* rng;
+
+    int class_count;
+    float delta;
+    float base_value;
+
+};
+#endif //#if 0
+
+
+
+/****************************************************************************************\
+*                              Artificial Neural Networks (ANN)                          *
+\****************************************************************************************/
+
+/////////////////////////////////// Multi-Layer Perceptrons //////////////////////////////
+
+struct CvANN_MLP_TrainParams
+{
+    CvANN_MLP_TrainParams();
+    CvANN_MLP_TrainParams( CvTermCriteria term_crit, int train_method,
+                           double param1, double param2=0 );
+    ~CvANN_MLP_TrainParams();
+
+    enum { BACKPROP=0, RPROP=1 };
+
+    CV_PROP_RW CvTermCriteria term_crit;
+    CV_PROP_RW int train_method;
+
+    // backpropagation parameters
+    CV_PROP_RW double bp_dw_scale, bp_moment_scale;
+
+    // rprop parameters
+    CV_PROP_RW double rp_dw0, rp_dw_plus, rp_dw_minus, rp_dw_min, rp_dw_max;
+};
+
+
+class CvANN_MLP : public CvStatModel
+{
+public:
+    CV_WRAP CvANN_MLP();
+    CvANN_MLP( const CvMat* layerSizes,
+               int activateFunc=CvANN_MLP::SIGMOID_SYM,
+               double fparam1=0, double fparam2=0 );
+
+    virtual ~CvANN_MLP();
+
+    virtual void create( const CvMat* layerSizes,
+                         int activateFunc=CvANN_MLP::SIGMOID_SYM,
+                         double fparam1=0, double fparam2=0 );
+
+    virtual int train( const CvMat* inputs, const CvMat* outputs,
+                       const CvMat* sampleWeights, const CvMat* sampleIdx=0,
+                       CvANN_MLP_TrainParams params = CvANN_MLP_TrainParams(),
+                       int flags=0 );
+    virtual float predict( const CvMat* inputs, CV_OUT CvMat* outputs ) const;
+
+    CV_WRAP CvANN_MLP( const cv::Mat& layerSizes,
+              int activateFunc=CvANN_MLP::SIGMOID_SYM,
+              double fparam1=0, double fparam2=0 );
+
+    CV_WRAP virtual void create( const cv::Mat& layerSizes,
+                        int activateFunc=CvANN_MLP::SIGMOID_SYM,
+                        double fparam1=0, double fparam2=0 );
+
+    CV_WRAP virtual int train( const cv::Mat& inputs, const cv::Mat& outputs,
+                      const cv::Mat& sampleWeights, const cv::Mat& sampleIdx=cv::Mat(),
+                      CvANN_MLP_TrainParams params = CvANN_MLP_TrainParams(),
+                      int flags=0 );
+
+    CV_WRAP virtual float predict( const cv::Mat& inputs, CV_OUT cv::Mat& outputs ) const;
+
+    CV_WRAP virtual void clear();
+
+    // possible activation functions
+    enum { IDENTITY = 0, SIGMOID_SYM = 1, GAUSSIAN = 2 };
+
+    // available training flags
+    enum { UPDATE_WEIGHTS = 1, NO_INPUT_SCALE = 2, NO_OUTPUT_SCALE = 4 };
+
+    virtual void read( CvFileStorage* fs, CvFileNode* node );
+    virtual void write( CvFileStorage* storage, const char* name ) const;
+
+    int get_layer_count() { return layer_sizes ? layer_sizes->cols : 0; }
+    const CvMat* get_layer_sizes() { return layer_sizes; }
+    double* get_weights(int layer)
+    {
+        return layer_sizes && weights &&
+            (unsigned)layer <= (unsigned)layer_sizes->cols ? weights[layer] : 0;
+    }
+
+    virtual void calc_activ_func_deriv( CvMat* xf, CvMat* deriv, const double* bias ) const;
+
+protected:
+
+    virtual bool prepare_to_train( const CvMat* _inputs, const CvMat* _outputs,
+            const CvMat* _sample_weights, const CvMat* sampleIdx,
+            CvVectors* _ivecs, CvVectors* _ovecs, double** _sw, int _flags );
+
+    // sequential random backpropagation
+    virtual int train_backprop( CvVectors _ivecs, CvVectors _ovecs, const double* _sw );
+
+    // RPROP algorithm
+    virtual int train_rprop( CvVectors _ivecs, CvVectors _ovecs, const double* _sw );
+
+    virtual void calc_activ_func( CvMat* xf, const double* bias ) const;
+    virtual void set_activ_func( int _activ_func=SIGMOID_SYM,
+                                 double _f_param1=0, double _f_param2=0 );
+    virtual void init_weights();
+    virtual void scale_input( const CvMat* _src, CvMat* _dst ) const;
+    virtual void scale_output( const CvMat* _src, CvMat* _dst ) const;
+    virtual void calc_input_scale( const CvVectors* vecs, int flags );
+    virtual void calc_output_scale( const CvVectors* vecs, int flags );
+
+    virtual void write_params( CvFileStorage* fs ) const;
+    virtual void read_params( CvFileStorage* fs, CvFileNode* node );
+
+    CvMat* layer_sizes;
+    CvMat* wbuf;
+    CvMat* sample_weights;
+    double** weights;
+    double f_param1, f_param2;
+    double min_val, max_val, min_val1, max_val1;
+    int activ_func;
+    int max_count, max_buf_sz;
+    CvANN_MLP_TrainParams params;
+    cv::RNG* rng;
+};
+
+/****************************************************************************************\
+*                                      Data                                             *
+\****************************************************************************************/
+
+#define CV_COUNT     0
+#define CV_PORTION   1
+
+struct CvTrainTestSplit
+{
+    CvTrainTestSplit();
+    CvTrainTestSplit( int train_sample_count, bool mix = true);
+    CvTrainTestSplit( float train_sample_portion, bool mix = true);
+
+    union
+    {
+        int count;
+        float portion;
+    } train_sample_part;
+    int train_sample_part_mode;
+
+    bool mix;
+};
+
+class CvMLData
+{
+public:
+    CvMLData();
+    virtual ~CvMLData();
+
+    // returns:
+    // 0 - OK
+    // -1 - file can not be opened or is not correct
+    int read_csv( const char* filename );
+
+    const CvMat* get_values() const;
+    const CvMat* get_responses();
+    const CvMat* get_missing() const;
+
+    void set_header_lines_number( int n );
+    int get_header_lines_number() const;
+
+    void set_response_idx( int idx ); // old response become predictors, new response_idx = idx
+                                      // if idx < 0 there will be no response
+    int get_response_idx() const;
+
+    void set_train_test_split( const CvTrainTestSplit * spl );
+    const CvMat* get_train_sample_idx() const;
+    const CvMat* get_test_sample_idx() const;
+    void mix_train_and_test_idx();
+
+    const CvMat* get_var_idx();
+    void chahge_var_idx( int vi, bool state ); // misspelled (saved for back compitability),
+                                               // use change_var_idx
+    void change_var_idx( int vi, bool state ); // state == true to set vi-variable as predictor
+
+    const CvMat* get_var_types();
+    int get_var_type( int var_idx ) const;
+    // following 2 methods enable to change vars type
+    // use these methods to assign CV_VAR_CATEGORICAL type for categorical variable
+    // with numerical labels; in the other cases var types are correctly determined automatically
+    void set_var_types( const char* str );  // str examples:
+                                            // "ord[0-17],cat[18]", "ord[0,2,4,10-12], cat[1,3,5-9,13,14]",
+                                            // "cat", "ord" (all vars are categorical/ordered)
+    void change_var_type( int var_idx, int type); // type in { CV_VAR_ORDERED, CV_VAR_CATEGORICAL }
+
+    void set_delimiter( char ch );
+    char get_delimiter() const;
+
+    void set_miss_ch( char ch );
+    char get_miss_ch() const;
+
+    const std::map<cv::String, int>& get_class_labels_map() const;
+
+protected:
+    virtual void clear();
+
+    void str_to_flt_elem( const char* token, float& flt_elem, int& type);
+    void free_train_test_idx();
+
+    char delimiter;
+    char miss_ch;
+    //char flt_separator;
+
+    CvMat* values;
+    CvMat* missing;
+    CvMat* var_types;
+    CvMat* var_idx_mask;
+
+    CvMat* response_out; // header
+    CvMat* var_idx_out; // mat
+    CvMat* var_types_out; // mat
+
+    int header_lines_number;
+
+    int response_idx;
+
+    int train_sample_count;
+    bool mix;
+
+    int total_class_count;
+    std::map<cv::String, int> class_map;
+
+    CvMat* train_sample_idx;
+    CvMat* test_sample_idx;
+    int* sample_idx; // data of train_sample_idx and test_sample_idx
+
+    cv::RNG* rng;
+};
+
+
+namespace cv
+{
+
+typedef CvStatModel StatModel;
+typedef CvParamGrid ParamGrid;
+typedef CvNormalBayesClassifier NormalBayesClassifier;
+typedef CvKNearest KNearest;
+typedef CvSVMParams SVMParams;
+typedef CvSVMKernel SVMKernel;
+typedef CvSVMSolver SVMSolver;
+typedef CvSVM SVM;
+typedef CvDTreeParams DTreeParams;
+typedef CvMLData TrainData;
+typedef CvDTree DecisionTree;
+typedef CvForestTree ForestTree;
+typedef CvRTParams RandomTreeParams;
+typedef CvRTrees RandomTrees;
+typedef CvERTreeTrainData ERTreeTRainData;
+typedef CvForestERTree ERTree;
+// SAB typedef CvERTrees ERTrees;
+typedef CvBoostParams BoostParams;
+typedef CvBoostTree BoostTree;
+typedef CvBoost Boost;
+typedef CvANN_MLP_TrainParams ANN_MLP_TrainParams;
+typedef CvANN_MLP NeuralNet_MLP;
+// SAB typedef CvGBTreesParams GradientBoostingTreeParams;
+// SAB typedef CvGBTrees GradientBoostingTrees;
+
+template<> void DefaultDeleter<CvDTreeSplit>::operator ()(CvDTreeSplit* obj) const;
+}
+
+#endif // __cplusplus
+#endif // OPENCV_ML_HPP
+
+/* End of file. */
diff --git a/openbr/core/old_ml_ann_mlp.cpp b/openbr/core/old_ml_ann_mlp.cpp
new file mode 100644
index 0000000..966ac77
--- /dev/null
+++ b/openbr/core/old_ml_ann_mlp.cpp
@@ -0,0 +1,1622 @@
+/*M///////////////////////////////////////////////////////////////////////////////////////
+//
+//  IMPORTANT: READ BEFORE DOWNLOADING, COPYING, INSTALLING OR USING.
+//
+//  By downloading, copying, installing or using the software you agree to this license.
+//  If you do not agree to this license, do not download, install,
+//  copy or use the software.
+//
+//
+//                        Intel License Agreement
+//
+// Copyright (C) 2000, Intel Corporation, all rights reserved.
+// Third party copyrights are property of their respective owners.
+//
+// Redistribution and use in source and binary forms, with or without modification,
+// are permitted provided that the following conditions are met:
+//
+//   * Redistribution's of source code must retain the above copyright notice,
+//     this list of conditions and the following disclaimer.
+//
+//   * Redistribution's in binary form must reproduce the above copyright notice,
+//     this list of conditions and the following disclaimer in the documentation
+//     and/or other materials provided with the distribution.
+//
+//   * The name of Intel Corporation may not be used to endorse or promote products
+//     derived from this software without specific prior written permission.
+//
+// This software is provided by the copyright holders and contributors "as is" and
+// any express or implied warranties, including, but not limited to, the implied
+// warranties of merchantability and fitness for a particular purpose are disclaimed.
+// In no event shall the Intel Corporation or contributors be liable for any direct,
+// indirect, incidental, special, exemplary, or consequential damages
+// (including, but not limited to, procurement of substitute goods or services;
+// loss of use, data, or profits; or business interruption) however caused
+// and on any theory of liability, whether in contract, strict liability,
+// or tort (including negligence or otherwise) arising in any way out of
+// the use of this software, even if advised of the possibility of such damage.
+//
+//M*/
+
+#include "old_ml_precomp.hpp"
+
+CvANN_MLP_TrainParams::CvANN_MLP_TrainParams()
+{
+    term_crit = cvTermCriteria( CV_TERMCRIT_ITER + CV_TERMCRIT_EPS, 1000, 0.01 );
+    train_method = RPROP;
+    bp_dw_scale = bp_moment_scale = 0.1;
+    rp_dw0 = 0.1; rp_dw_plus = 1.2; rp_dw_minus = 0.5;
+    rp_dw_min = FLT_EPSILON; rp_dw_max = 50.;
+}
+
+
+CvANN_MLP_TrainParams::CvANN_MLP_TrainParams( CvTermCriteria _term_crit,
+                                              int _train_method,
+                                              double _param1, double _param2 )
+{
+    term_crit = _term_crit;
+    train_method = _train_method;
+    bp_dw_scale = bp_moment_scale = 0.1;
+    rp_dw0 = 1.; rp_dw_plus = 1.2; rp_dw_minus = 0.5;
+    rp_dw_min = FLT_EPSILON; rp_dw_max = 50.;
+
+    if( train_method == RPROP )
+    {
+        rp_dw0 = _param1;
+        if( rp_dw0 < FLT_EPSILON )
+            rp_dw0 = 1.;
+        rp_dw_min = _param2;
+        rp_dw_min = MAX( rp_dw_min, 0 );
+    }
+    else if( train_method == BACKPROP )
+    {
+        bp_dw_scale = _param1;
+        if( bp_dw_scale <= 0 )
+            bp_dw_scale = 0.1;
+        bp_dw_scale = MAX( bp_dw_scale, 1e-3 );
+        bp_dw_scale = MIN( bp_dw_scale, 1 );
+        bp_moment_scale = _param2;
+        if( bp_moment_scale < 0 )
+            bp_moment_scale = 0.1;
+        bp_moment_scale = MIN( bp_moment_scale, 1 );
+    }
+    else
+        train_method = RPROP;
+}
+
+
+CvANN_MLP_TrainParams::~CvANN_MLP_TrainParams()
+{
+}
+
+
+CvANN_MLP::CvANN_MLP()
+{
+    layer_sizes = wbuf = 0;
+    min_val = max_val = min_val1 = max_val1 = 0.;
+    weights = 0;
+    rng = &cv::theRNG();
+    default_model_name = "my_nn";
+    clear();
+}
+
+
+CvANN_MLP::CvANN_MLP( const CvMat* _layer_sizes,
+                      int _activ_func,
+                      double _f_param1, double _f_param2 )
+{
+    layer_sizes = wbuf = 0;
+    min_val = max_val = min_val1 = max_val1 = 0.;
+    weights = 0;
+    rng = &cv::theRNG();
+    default_model_name = "my_nn";
+    create( _layer_sizes, _activ_func, _f_param1, _f_param2 );
+}
+
+
+CvANN_MLP::~CvANN_MLP()
+{
+    clear();
+}
+
+
+void CvANN_MLP::clear()
+{
+    cvReleaseMat( &layer_sizes );
+    cvReleaseMat( &wbuf );
+    cvFree( &weights );
+    activ_func = SIGMOID_SYM;
+    f_param1 = f_param2 = 1;
+    max_buf_sz = 1 << 12;
+}
+
+
+void CvANN_MLP::set_activ_func( int _activ_func, double _f_param1, double _f_param2 )
+{
+    CV_FUNCNAME( "CvANN_MLP::set_activ_func" );
+
+    __BEGIN__;
+
+    if( _activ_func < 0 || _activ_func > GAUSSIAN )
+        CV_ERROR( CV_StsOutOfRange, "Unknown activation function" );
+
+    activ_func = _activ_func;
+
+    switch( activ_func )
+    {
+    case SIGMOID_SYM:
+        max_val = 0.95; min_val = -max_val;
+        max_val1 = 0.98; min_val1 = -max_val1;
+        if( fabs(_f_param1) < FLT_EPSILON )
+            _f_param1 = 2./3;
+        if( fabs(_f_param2) < FLT_EPSILON )
+            _f_param2 = 1.7159;
+        break;
+    case GAUSSIAN:
+        max_val = 1.; min_val = 0.05;
+        max_val1 = 1.; min_val1 = 0.02;
+        if( fabs(_f_param1) < FLT_EPSILON )
+            _f_param1 = 1.;
+        if( fabs(_f_param2) < FLT_EPSILON )
+            _f_param2 = 1.;
+        break;
+    default:
+        min_val = max_val = min_val1 = max_val1 = 0.;
+        _f_param1 = 1.;
+        _f_param2 = 0.;
+    }
+
+    f_param1 = _f_param1;
+    f_param2 = _f_param2;
+
+    __END__;
+}
+
+
+void CvANN_MLP::init_weights()
+{
+    int i, j, k;
+
+    for( i = 1; i < layer_sizes->cols; i++ )
+    {
+        int n1 = layer_sizes->data.i[i-1];
+        int n2 = layer_sizes->data.i[i];
+        double val = 0, G = n2 > 2 ? 0.7*pow((double)n1,1./(n2-1)) : 1.;
+        double* w = weights[i];
+
+        // initialize weights using Nguyen-Widrow algorithm
+        for( j = 0; j < n2; j++ )
+        {
+            double s = 0;
+            for( k = 0; k <= n1; k++ )
+            {
+                val = rng->uniform(0., 1.)*2-1.;
+                w[k*n2 + j] = val;
+                s += fabs(val);
+            }
+
+            if( i < layer_sizes->cols - 1 )
+            {
+                s = 1./(s - fabs(val));
+                for( k = 0; k <= n1; k++ )
+                    w[k*n2 + j] *= s;
+                w[n1*n2 + j] *= G*(-1+j*2./n2);
+            }
+        }
+    }
+}
+
+
+void CvANN_MLP::create( const CvMat* _layer_sizes, int _activ_func,
+                        double _f_param1, double _f_param2 )
+{
+    CV_FUNCNAME( "CvANN_MLP::create" );
+
+    __BEGIN__;
+
+    int i, l_step, l_count, buf_sz = 0;
+    int *l_src, *l_dst;
+
+    clear();
+
+    if( !CV_IS_MAT(_layer_sizes) ||
+        (_layer_sizes->cols != 1 && _layer_sizes->rows != 1) ||
+        CV_MAT_TYPE(_layer_sizes->type) != CV_32SC1 )
+        CV_ERROR( CV_StsBadArg,
+        "The array of layer neuron counters must be an integer vector" );
+
+    CV_CALL( set_activ_func( _activ_func, _f_param1, _f_param2 ));
+
+    l_count = _layer_sizes->rows + _layer_sizes->cols - 1;
+    l_src = _layer_sizes->data.i;
+    l_step = CV_IS_MAT_CONT(_layer_sizes->type) ? 1 :
+                _layer_sizes->step / sizeof(l_src[0]);
+    CV_CALL( layer_sizes = cvCreateMat( 1, l_count, CV_32SC1 ));
+    l_dst = layer_sizes->data.i;
+
+    max_count = 0;
+    for( i = 0; i < l_count; i++ )
+    {
+        int n = l_src[i*l_step];
+        if( n < 1 + (0 < i && i < l_count-1))
+            CV_ERROR( CV_StsOutOfRange,
+            "there should be at least one input and one output "
+            "and every hidden layer must have more than 1 neuron" );
+        l_dst[i] = n;
+        max_count = MAX( max_count, n );
+        if( i > 0 )
+            buf_sz += (l_dst[i-1]+1)*n;
+    }
+
+    buf_sz += (l_dst[0] + l_dst[l_count-1]*2)*2;
+
+    CV_CALL( wbuf = cvCreateMat( 1, buf_sz, CV_64F ));
+    CV_CALL( weights = (double**)cvAlloc( (l_count+2)*sizeof(weights[0]) ));
+
+    weights[0] = wbuf->data.db;
+    weights[1] = weights[0] + l_dst[0]*2;
+    for( i = 1; i < l_count; i++ )
+        weights[i+1] = weights[i] + (l_dst[i-1] + 1)*l_dst[i];
+    weights[l_count+1] = weights[l_count] + l_dst[l_count-1]*2;
+
+    __END__;
+}
+
+
+float CvANN_MLP::predict( const CvMat* _inputs, CvMat* _outputs ) const
+{
+    int i, j, n, dn = 0, l_count, dn0, buf_sz, min_buf_sz;
+
+    if( !layer_sizes )
+        CV_Error( CV_StsError, "The network has not been initialized" );
+
+    if( !CV_IS_MAT(_inputs) || !CV_IS_MAT(_outputs) ||
+        !CV_ARE_TYPES_EQ(_inputs,_outputs) ||
+        (CV_MAT_TYPE(_inputs->type) != CV_32FC1 &&
+        CV_MAT_TYPE(_inputs->type) != CV_64FC1) ||
+        _inputs->rows != _outputs->rows )
+        CV_Error( CV_StsBadArg, "Both input and output must be floating-point matrices "
+                                "of the same type and have the same number of rows" );
+
+    if( _inputs->cols != layer_sizes->data.i[0] )
+        CV_Error( CV_StsBadSize, "input matrix must have the same number of columns as "
+                                 "the number of neurons in the input layer" );
+
+    if( _outputs->cols != layer_sizes->data.i[layer_sizes->cols - 1] )
+        CV_Error( CV_StsBadSize, "output matrix must have the same number of columns as "
+                                 "the number of neurons in the output layer" );
+    n = dn0 = _inputs->rows;
+    min_buf_sz = 2*max_count;
+    buf_sz = n*min_buf_sz;
+
+    if( buf_sz > max_buf_sz )
+    {
+        dn0 = max_buf_sz/min_buf_sz;
+        dn0 = MAX( dn0, 1 );
+        buf_sz = dn0*min_buf_sz;
+    }
+
+    cv::AutoBuffer<double> buf(buf_sz);
+    l_count = layer_sizes->cols;
+
+    for( i = 0; i < n; i += dn )
+    {
+        CvMat hdr[2], _w, *layer_in = &hdr[0], *layer_out = &hdr[1], *temp;
+        dn = MIN( dn0, n - i );
+
+        cvGetRows( _inputs, layer_in, i, i + dn );
+        cvInitMatHeader( layer_out, dn, layer_in->cols, CV_64F, &buf[0] );
+
+        scale_input( layer_in, layer_out );
+        CV_SWAP( layer_in, layer_out, temp );
+
+        for( j = 1; j < l_count; j++ )
+        {
+            double* data = buf + (j&1 ? max_count*dn0 : 0);
+            int cols = layer_sizes->data.i[j];
+
+            cvInitMatHeader( layer_out, dn, cols, CV_64F, data );
+            cvInitMatHeader( &_w, layer_in->cols, layer_out->cols, CV_64F, weights[j] );
+            cvGEMM( layer_in, &_w, 1, 0, 0, layer_out );
+            calc_activ_func( layer_out, _w.data.db + _w.rows*_w.cols );
+
+            CV_SWAP( layer_in, layer_out, temp );
+        }
+
+        cvGetRows( _outputs, layer_out, i, i + dn );
+        scale_output( layer_in, layer_out );
+    }
+
+    return 0.f;
+}
+
+
+void CvANN_MLP::scale_input( const CvMat* _src, CvMat* _dst ) const
+{
+    int i, j, cols = _src->cols;
+    double* dst = _dst->data.db;
+    const double* w = weights[0];
+    int step = _src->step;
+
+    if( CV_MAT_TYPE( _src->type ) == CV_32F )
+    {
+        const float* src = _src->data.fl;
+        step /= sizeof(src[0]);
+
+        for( i = 0; i < _src->rows; i++, src += step, dst += cols )
+            for( j = 0; j < cols; j++ )
+                dst[j] = src[j]*w[j*2] + w[j*2+1];
+    }
+    else
+    {
+        const double* src = _src->data.db;
+        step /= sizeof(src[0]);
+
+        for( i = 0; i < _src->rows; i++, src += step, dst += cols )
+            for( j = 0; j < cols; j++ )
+                dst[j] = src[j]*w[j*2] + w[j*2+1];
+    }
+}
+
+
+void CvANN_MLP::scale_output( const CvMat* _src, CvMat* _dst ) const
+{
+    int i, j, cols = _src->cols;
+    const double* src = _src->data.db;
+    const double* w = weights[layer_sizes->cols];
+    int step = _dst->step;
+
+    if( CV_MAT_TYPE( _dst->type ) == CV_32F )
+    {
+        float* dst = _dst->data.fl;
+        step /= sizeof(dst[0]);
+
+        for( i = 0; i < _src->rows; i++, src += cols, dst += step )
+            for( j = 0; j < cols; j++ )
+                dst[j] = (float)(src[j]*w[j*2] + w[j*2+1]);
+    }
+    else
+    {
+        double* dst = _dst->data.db;
+        step /= sizeof(dst[0]);
+
+        for( i = 0; i < _src->rows; i++, src += cols, dst += step )
+            for( j = 0; j < cols; j++ )
+                dst[j] = src[j]*w[j*2] + w[j*2+1];
+    }
+}
+
+
+void CvANN_MLP::calc_activ_func( CvMat* sums, const double* bias ) const
+{
+    int i, j, n = sums->rows, cols = sums->cols;
+    double* data = sums->data.db;
+    double scale = 0, scale2 = f_param2;
+
+    switch( activ_func )
+    {
+    case IDENTITY:
+        scale = 1.;
+        break;
+    case SIGMOID_SYM:
+        scale = -f_param1;
+        break;
+    case GAUSSIAN:
+        scale = -f_param1*f_param1;
+        break;
+    default:
+        ;
+    }
+
+    assert( CV_IS_MAT_CONT(sums->type) );
+
+    if( activ_func != GAUSSIAN )
+    {
+        for( i = 0; i < n; i++, data += cols )
+            for( j = 0; j < cols; j++ )
+                data[j] = (data[j] + bias[j])*scale;
+
+        if( activ_func == IDENTITY )
+            return;
+    }
+    else
+    {
+        for( i = 0; i < n; i++, data += cols )
+            for( j = 0; j < cols; j++ )
+            {
+                double t = data[j] + bias[j];
+                data[j] = t*t*scale;
+            }
+    }
+
+    cvExp( sums, sums );
+
+    n *= cols;
+    data -= n;
+
+    switch( activ_func )
+    {
+    case SIGMOID_SYM:
+        for( i = 0; i <= n - 4; i += 4 )
+        {
+            double x0 = 1.+data[i], x1 = 1.+data[i+1], x2 = 1.+data[i+2], x3 = 1.+data[i+3];
+            double a = x0*x1, b = x2*x3, d = scale2/(a*b), t0, t1;
+            a *= d; b *= d;
+            t0 = (2 - x0)*b*x1; t1 = (2 - x1)*b*x0;
+            data[i] = t0; data[i+1] = t1;
+            t0 = (2 - x2)*a*x3; t1 = (2 - x3)*a*x2;
+            data[i+2] = t0; data[i+3] = t1;
+        }
+
+        for( ; i < n; i++ )
+        {
+            double t = scale2*(1. - data[i])/(1. + data[i]);
+            data[i] = t;
+        }
+        break;
+
+    case GAUSSIAN:
+        for( i = 0; i < n; i++ )
+            data[i] = scale2*data[i];
+        break;
+
+    default:
+        ;
+    }
+}
+
+
+void CvANN_MLP::calc_activ_func_deriv( CvMat* _xf, CvMat* _df,
+                                       const double* bias ) const
+{
+    int i, j, n = _xf->rows, cols = _xf->cols;
+    double* xf = _xf->data.db;
+    double* df = _df->data.db;
+    double scale, scale2 = f_param2;
+    assert( CV_IS_MAT_CONT( _xf->type & _df->type ) );
+
+    if( activ_func == IDENTITY )
+    {
+        for( i = 0; i < n; i++, xf += cols, df += cols )
+            for( j = 0; j < cols; j++ )
+            {
+                xf[j] += bias[j];
+                df[j] = 1;
+            }
+        return;
+    }
+    else if( activ_func == GAUSSIAN )
+    {
+        scale = -f_param1*f_param1;
+        scale2 *= scale;
+        for( i = 0; i < n; i++, xf += cols, df += cols )
+            for( j = 0; j < cols; j++ )
+            {
+                double t = xf[j] + bias[j];
+                df[j] = t*2*scale2;
+                xf[j] = t*t*scale;
+            }
+        cvExp( _xf, _xf );
+
+        n *= cols;
+        xf -= n; df -= n;
+
+        for( i = 0; i < n; i++ )
+            df[i] *= xf[i];
+    }
+    else
+    {
+        scale = f_param1;
+        for( i = 0; i < n; i++, xf += cols, df += cols )
+            for( j = 0; j < cols; j++ )
+            {
+                xf[j] = (xf[j] + bias[j])*scale;
+                df[j] = -fabs(xf[j]);
+            }
+
+        cvExp( _df, _df );
+
+        n *= cols;
+        xf -= n; df -= n;
+
+        // ((1+exp(-ax))^-1)'=a*((1+exp(-ax))^-2)*exp(-ax);
+        // ((1-exp(-ax))/(1+exp(-ax)))'=(a*exp(-ax)*(1+exp(-ax)) + a*exp(-ax)*(1-exp(-ax)))/(1+exp(-ax))^2=
+        // 2*a*exp(-ax)/(1+exp(-ax))^2
+        scale *= 2*f_param2;
+        for( i = 0; i < n; i++ )
+        {
+            int s0 = xf[i] > 0 ? 1 : -1;
+            double t0 = 1./(1. + df[i]);
+            double t1 = scale*df[i]*t0*t0;
+            t0 *= scale2*(1. - df[i])*s0;
+            df[i] = t1;
+            xf[i] = t0;
+        }
+    }
+}
+
+
+void CvANN_MLP::calc_input_scale( const CvVectors* vecs, int flags )
+{
+    bool reset_weights = (flags & UPDATE_WEIGHTS) == 0;
+    bool no_scale = (flags & NO_INPUT_SCALE) != 0;
+    double* scale = weights[0];
+    int count = vecs->count;
+
+    if( reset_weights )
+    {
+        int i, j, vcount = layer_sizes->data.i[0];
+        int type = vecs->type;
+        double a = no_scale ? 1. : 0.;
+
+        for( j = 0; j < vcount; j++ )
+            scale[2*j] = a, scale[j*2+1] = 0.;
+
+        if( no_scale )
+            return;
+
+        for( i = 0; i < count; i++ )
+        {
+            const float* f = vecs->data.fl[i];
+            const double* d = vecs->data.db[i];
+            for( j = 0; j < vcount; j++ )
+            {
+                double t = type == CV_32F ? (double)f[j] : d[j];
+                scale[j*2] += t;
+                scale[j*2+1] += t*t;
+            }
+        }
+
+        for( j = 0; j < vcount; j++ )
+        {
+            double s = scale[j*2], s2 = scale[j*2+1];
+            double m = s/count, sigma2 = s2/count - m*m;
+            scale[j*2] = sigma2 < DBL_EPSILON ? 1 : 1./sqrt(sigma2);
+            scale[j*2+1] = -m*scale[j*2];
+        }
+    }
+}
+
+
+void CvANN_MLP::calc_output_scale( const CvVectors* vecs, int flags )
+{
+    int i, j, vcount = layer_sizes->data.i[layer_sizes->cols-1];
+    int type = vecs->type;
+    double m = min_val, M = max_val, m1 = min_val1, M1 = max_val1;
+    bool reset_weights = (flags & UPDATE_WEIGHTS) == 0;
+    bool no_scale = (flags & NO_OUTPUT_SCALE) != 0;
+    int l_count = layer_sizes->cols;
+    double* scale = weights[l_count];
+    double* inv_scale = weights[l_count+1];
+    int count = vecs->count;
+
+    CV_FUNCNAME( "CvANN_MLP::calc_output_scale" );
+
+    __BEGIN__;
+
+    if( reset_weights )
+    {
+        double a0 = no_scale ? 1 : DBL_MAX, b0 = no_scale ? 0 : -DBL_MAX;
+
+        for( j = 0; j < vcount; j++ )
+        {
+            scale[2*j] = inv_scale[2*j] = a0;
+            scale[j*2+1] = inv_scale[2*j+1] = b0;
+        }
+
+        if( no_scale )
+            EXIT;
+    }
+
+    for( i = 0; i < count; i++ )
+    {
+        const float* f = vecs->data.fl[i];
+        const double* d = vecs->data.db[i];
+
+        for( j = 0; j < vcount; j++ )
+        {
+            double t = type == CV_32F ? (double)f[j] : d[j];
+
+            if( reset_weights )
+            {
+                double mj = scale[j*2], Mj = scale[j*2+1];
+                if( mj > t ) mj = t;
+                if( Mj < t ) Mj = t;
+
+                scale[j*2] = mj;
+                scale[j*2+1] = Mj;
+            }
+            else
+            {
+                t = t*inv_scale[j*2] + inv_scale[2*j+1];
+                if( t < m1 || t > M1 )
+                    CV_ERROR( CV_StsOutOfRange,
+                    "Some of new output training vector components run exceed the original range too much" );
+            }
+        }
+    }
+
+    if( reset_weights )
+        for( j = 0; j < vcount; j++ )
+        {
+            // map mj..Mj to m..M
+            double mj = scale[j*2], Mj = scale[j*2+1];
+            double a, b;
+            double delta = Mj - mj;
+            if( delta < DBL_EPSILON )
+                a = 1, b = (M + m - Mj - mj)*0.5;
+            else
+                a = (M - m)/delta, b = m - mj*a;
+            inv_scale[j*2] = a; inv_scale[j*2+1] = b;
+            a = 1./a; b = -b*a;
+            scale[j*2] = a; scale[j*2+1] = b;
+        }
+
+    __END__;
+}
+
+
+bool CvANN_MLP::prepare_to_train( const CvMat* _inputs, const CvMat* _outputs,
+            const CvMat* _sample_weights, const CvMat* _sample_idx,
+            CvVectors* _ivecs, CvVectors* _ovecs, double** _sw, int _flags )
+{
+    bool ok = false;
+    CvMat* sample_idx = 0;
+    CvVectors ivecs, ovecs;
+    double* sw = 0;
+    int count = 0;
+
+    CV_FUNCNAME( "CvANN_MLP::prepare_to_train" );
+
+    ivecs.data.ptr = ovecs.data.ptr = 0;
+    assert( _ivecs && _ovecs );
+
+    __BEGIN__;
+
+    const int* sidx = 0;
+    int i, sw_type = 0, sw_count = 0;
+    int sw_step = 0;
+    double sw_sum = 0;
+
+    if( !layer_sizes )
+        CV_ERROR( CV_StsError,
+        "The network has not been created. Use method create or the appropriate constructor" );
+
+    if( !CV_IS_MAT(_inputs) || (CV_MAT_TYPE(_inputs->type) != CV_32FC1 &&
+        CV_MAT_TYPE(_inputs->type) != CV_64FC1) || _inputs->cols != layer_sizes->data.i[0] )
+        CV_ERROR( CV_StsBadArg,
+        "input training data should be a floating-point matrix with"
+        "the number of rows equal to the number of training samples and "
+        "the number of columns equal to the size of 0-th (input) layer" );
+
+    if( !CV_IS_MAT(_outputs) || (CV_MAT_TYPE(_outputs->type) != CV_32FC1 &&
+        CV_MAT_TYPE(_outputs->type) != CV_64FC1) ||
+        _outputs->cols != layer_sizes->data.i[layer_sizes->cols - 1] )
+        CV_ERROR( CV_StsBadArg,
+        "output training data should be a floating-point matrix with"
+        "the number of rows equal to the number of training samples and "
+        "the number of columns equal to the size of last (output) layer" );
+
+    if( _inputs->rows != _outputs->rows )
+        CV_ERROR( CV_StsUnmatchedSizes, "The numbers of input and output samples do not match" );
+
+    if( _sample_idx )
+    {
+        CV_CALL( sample_idx = cvPreprocessIndexArray( _sample_idx, _inputs->rows ));
+        sidx = sample_idx->data.i;
+        count = sample_idx->cols + sample_idx->rows - 1;
+    }
+    else
+        count = _inputs->rows;
+
+    if( _sample_weights )
+    {
+        if( !CV_IS_MAT(_sample_weights) )
+            CV_ERROR( CV_StsBadArg, "sample_weights (if passed) must be a valid matrix" );
+
+        sw_type = CV_MAT_TYPE(_sample_weights->type);
+        sw_count = _sample_weights->cols + _sample_weights->rows - 1;
+
+        if( (sw_type != CV_32FC1 && sw_type != CV_64FC1) ||
+            (_sample_weights->cols != 1 && _sample_weights->rows != 1) ||
+            (sw_count != count && sw_count != _inputs->rows) )
+            CV_ERROR( CV_StsBadArg,
+            "sample_weights must be 1d floating-point vector containing weights "
+            "of all or selected training samples" );
+
+        sw_step = CV_IS_MAT_CONT(_sample_weights->type) ? 1 :
+            _sample_weights->step/CV_ELEM_SIZE(sw_type);
+
+        CV_CALL( sw = (double*)cvAlloc( count*sizeof(sw[0]) ));
+    }
+
+    CV_CALL( ivecs.data.ptr = (uchar**)cvAlloc( count*sizeof(ivecs.data.ptr[0]) ));
+    CV_CALL( ovecs.data.ptr = (uchar**)cvAlloc( count*sizeof(ovecs.data.ptr[0]) ));
+
+    ivecs.type = CV_MAT_TYPE(_inputs->type);
+    ovecs.type = CV_MAT_TYPE(_outputs->type);
+    ivecs.count = ovecs.count = count;
+
+    for( i = 0; i < count; i++ )
+    {
+        int idx = sidx ? sidx[i] : i;
+        ivecs.data.ptr[i] = _inputs->data.ptr + idx*_inputs->step;
+        ovecs.data.ptr[i] = _outputs->data.ptr + idx*_outputs->step;
+        if( sw )
+        {
+            int si = sw_count == count ? i : idx;
+            double w = sw_type == CV_32FC1 ?
+                (double)_sample_weights->data.fl[si*sw_step] :
+                _sample_weights->data.db[si*sw_step];
+            sw[i] = w;
+            if( w < 0 )
+                CV_ERROR( CV_StsOutOfRange, "some of sample weights are negative" );
+            sw_sum += w;
+        }
+    }
+
+    // normalize weights
+    if( sw )
+    {
+        sw_sum = sw_sum > DBL_EPSILON ? 1./sw_sum : 0;
+        for( i = 0; i < count; i++ )
+            sw[i] *= sw_sum;
+    }
+
+    calc_input_scale( &ivecs, _flags );
+    CV_CALL( calc_output_scale( &ovecs, _flags ));
+
+    ok = true;
+
+    __END__;
+
+    if( !ok )
+    {
+        cvFree( &ivecs.data.ptr );
+        cvFree( &ovecs.data.ptr );
+        cvFree( &sw );
+    }
+
+    cvReleaseMat( &sample_idx );
+    *_ivecs = ivecs;
+    *_ovecs = ovecs;
+    *_sw = sw;
+
+    return ok;
+}
+
+
+int CvANN_MLP::train( const CvMat* _inputs, const CvMat* _outputs,
+                      const CvMat* _sample_weights, const CvMat* _sample_idx,
+                      CvANN_MLP_TrainParams _params, int flags )
+{
+    const int MAX_ITER = 1000;
+    const double DEFAULT_EPSILON = FLT_EPSILON;
+
+    double* sw = 0;
+    CvVectors x0, u;
+    int iter = -1;
+
+    x0.data.ptr = u.data.ptr = 0;
+
+    CV_FUNCNAME( "CvANN_MLP::train" );
+
+    __BEGIN__;
+
+    int max_iter;
+    double epsilon;
+
+    params = _params;
+
+    // initialize training data
+    CV_CALL( prepare_to_train( _inputs, _outputs, _sample_weights,
+                               _sample_idx, &x0, &u, &sw, flags ));
+
+    // ... and link weights
+    if( !(flags & UPDATE_WEIGHTS) )
+        init_weights();
+
+    max_iter = params.term_crit.type & CV_TERMCRIT_ITER ? params.term_crit.max_iter : MAX_ITER;
+    max_iter = MAX( max_iter, 1 );
+
+    epsilon = params.term_crit.type & CV_TERMCRIT_EPS ? params.term_crit.epsilon : DEFAULT_EPSILON;
+    epsilon = MAX(epsilon, DBL_EPSILON);
+
+    params.term_crit.type = CV_TERMCRIT_ITER + CV_TERMCRIT_EPS;
+    params.term_crit.max_iter = max_iter;
+    params.term_crit.epsilon = epsilon;
+
+    if( params.train_method == CvANN_MLP_TrainParams::BACKPROP )
+    {
+        CV_CALL( iter = train_backprop( x0, u, sw ));
+    }
+    else
+    {
+        CV_CALL( iter = train_rprop( x0, u, sw ));
+    }
+
+    __END__;
+
+    cvFree( &x0.data.ptr );
+    cvFree( &u.data.ptr );
+    cvFree( &sw );
+
+    return iter;
+}
+
+
+int CvANN_MLP::train_backprop( CvVectors x0, CvVectors u, const double* sw )
+{
+    CvMat* dw = 0;
+    CvMat* buf = 0;
+    double **x = 0, **df = 0;
+    CvMat* _idx = 0;
+    int iter = -1, count = x0.count;
+
+    CV_FUNCNAME( "CvANN_MLP::train_backprop" );
+
+    __BEGIN__;
+
+    int i, j, k, ivcount, ovcount, l_count, total = 0, max_iter;
+    double *buf_ptr;
+    double prev_E = DBL_MAX*0.5, E = 0, epsilon;
+
+    max_iter = params.term_crit.max_iter*count;
+    epsilon = params.term_crit.epsilon*count;
+
+    l_count = layer_sizes->cols;
+    ivcount = layer_sizes->data.i[0];
+    ovcount = layer_sizes->data.i[l_count-1];
+
+    // allocate buffers
+    for( i = 0; i < l_count; i++ )
+        total += layer_sizes->data.i[i] + 1;
+
+    CV_CALL( dw = cvCreateMat( wbuf->rows, wbuf->cols, wbuf->type ));
+    cvZero( dw );
+    CV_CALL( buf = cvCreateMat( 1, (total + max_count)*2, CV_64F ));
+    CV_CALL( _idx = cvCreateMat( 1, count, CV_32SC1 ));
+    for( i = 0; i < count; i++ )
+        _idx->data.i[i] = i;
+
+    CV_CALL( x = (double**)cvAlloc( total*2*sizeof(x[0]) ));
+    df = x + total;
+    buf_ptr = buf->data.db;
+
+    for( j = 0; j < l_count; j++ )
+    {
+        x[j] = buf_ptr;
+        df[j] = x[j] + layer_sizes->data.i[j];
+        buf_ptr += (df[j] - x[j])*2;
+    }
+
+    // run back-propagation loop
+    /*
+        y_i = w_i*x_{i-1}
+        x_i = f(y_i)
+        E = 1/2*||u - x_N||^2
+        grad_N = (x_N - u)*f'(y_i)
+        dw_i(t) = momentum*dw_i(t-1) + dw_scale*x_{i-1}*grad_i
+        w_i(t+1) = w_i(t) + dw_i(t)
+        grad_{i-1} = w_i^t*grad_i
+    */
+    for( iter = 0; iter < max_iter; iter++ )
+    {
+        int idx = iter % count;
+        double* w = weights[0];
+        double sweight = sw ? count*sw[idx] : 1.;
+        CvMat _w, _dw, hdr1, hdr2, ghdr1, ghdr2, _df;
+        CvMat *x1 = &hdr1, *x2 = &hdr2, *grad1 = &ghdr1, *grad2 = &ghdr2, *temp;
+
+        if( idx == 0 )
+        {
+            //printf("%d. E = %g\n", iter/count, E);
+            if( fabs(prev_E - E) < epsilon )
+                break;
+            prev_E = E;
+            E = 0;
+
+            // shuffle indices
+            for( i = 0; i < count; i++ )
+            {
+                int tt;
+                j = (*rng)(count);
+                k = (*rng)(count);
+                CV_SWAP( _idx->data.i[j], _idx->data.i[k], tt );
+            }
+        }
+
+        idx = _idx->data.i[idx];
+
+        if( x0.type == CV_32F )
+        {
+            const float* x0data = x0.data.fl[idx];
+            for( j = 0; j < ivcount; j++ )
+                x[0][j] = x0data[j]*w[j*2] + w[j*2 + 1];
+        }
+        else
+        {
+            const double* x0data = x0.data.db[idx];
+            for( j = 0; j < ivcount; j++ )
+                x[0][j] = x0data[j]*w[j*2] + w[j*2 + 1];
+        }
+
+        cvInitMatHeader( x1, 1, ivcount, CV_64F, x[0] );
+
+        // forward pass, compute y[i]=w*x[i-1], x[i]=f(y[i]), df[i]=f'(y[i])
+        for( i = 1; i < l_count; i++ )
+        {
+            cvInitMatHeader( x2, 1, layer_sizes->data.i[i], CV_64F, x[i] );
+            cvInitMatHeader( &_w, x1->cols, x2->cols, CV_64F, weights[i] );
+            cvGEMM( x1, &_w, 1, 0, 0, x2 );
+            _df = *x2;
+            _df.data.db = df[i];
+            calc_activ_func_deriv( x2, &_df, _w.data.db + _w.rows*_w.cols );
+            CV_SWAP( x1, x2, temp );
+        }
+
+        cvInitMatHeader( grad1, 1, ovcount, CV_64F, buf_ptr );
+        *grad2 = *grad1;
+        grad2->data.db = buf_ptr + max_count;
+
+        w = weights[l_count+1];
+
+        // calculate error
+        if( u.type == CV_32F )
+        {
+            const float* udata = u.data.fl[idx];
+            for( k = 0; k < ovcount; k++ )
+            {
+                double t = udata[k]*w[k*2] + w[k*2+1] - x[l_count-1][k];
+                grad1->data.db[k] = t*sweight;
+                E += t*t;
+            }
+        }
+        else
+        {
+            const double* udata = u.data.db[idx];
+            for( k = 0; k < ovcount; k++ )
+            {
+                double t = udata[k]*w[k*2] + w[k*2+1] - x[l_count-1][k];
+                grad1->data.db[k] = t*sweight;
+                E += t*t;
+            }
+        }
+        E *= sweight;
+
+        // backward pass, update weights
+        for( i = l_count-1; i > 0; i-- )
+        {
+            int n1 = layer_sizes->data.i[i-1], n2 = layer_sizes->data.i[i];
+            cvInitMatHeader( &_df, 1, n2, CV_64F, df[i] );
+            cvMul( grad1, &_df, grad1 );
+            cvInitMatHeader( &_w, n1+1, n2, CV_64F, weights[i] );
+            cvInitMatHeader( &_dw, n1+1, n2, CV_64F, dw->data.db + (weights[i] - weights[0]) );
+            cvInitMatHeader( x1, n1+1, 1, CV_64F, x[i-1] );
+            x[i-1][n1] = 1.;
+            cvGEMM( x1, grad1, params.bp_dw_scale, &_dw, params.bp_moment_scale, &_dw );
+            cvAdd( &_w, &_dw, &_w );
+            if( i > 1 )
+            {
+                grad2->cols = n1;
+                _w.rows = n1;
+                cvGEMM( grad1, &_w, 1, 0, 0, grad2, CV_GEMM_B_T );
+            }
+            CV_SWAP( grad1, grad2, temp );
+        }
+    }
+
+    iter /= count;
+
+    __END__;
+
+    cvReleaseMat( &dw );
+    cvReleaseMat( &buf );
+    cvReleaseMat( &_idx );
+    cvFree( &x );
+
+    return iter;
+}
+
+struct rprop_loop : cv::ParallelLoopBody {
+  rprop_loop(const CvANN_MLP* _point, double**& _weights, int& _count, int& _ivcount, CvVectors* _x0,
+     int& _l_count, CvMat*& _layer_sizes, int& _ovcount, int& _max_count,
+     CvVectors* _u, const double*& _sw, double& _inv_count, CvMat*& _dEdw, int& _dcount0, double* _E, int _buf_sz)
+  {
+    point = _point;
+    weights = _weights;
+    count = _count;
+    ivcount = _ivcount;
+    x0 = _x0;
+    l_count = _l_count;
+    layer_sizes = _layer_sizes;
+    ovcount = _ovcount;
+    max_count = _max_count;
+    u = _u;
+    sw = _sw;
+    inv_count = _inv_count;
+    dEdw = _dEdw;
+    dcount0 = _dcount0;
+    E = _E;
+    buf_sz = _buf_sz;
+  }
+
+  const CvANN_MLP* point;
+  double** weights;
+  int count;
+  int ivcount;
+  CvVectors* x0;
+  int l_count;
+  CvMat* layer_sizes;
+  int ovcount;
+  int max_count;
+  CvVectors* u;
+  const double* sw;
+  double inv_count;
+  CvMat* dEdw;
+  int dcount0;
+  double* E;
+  int buf_sz;
+
+
+  void operator()( const cv::Range& range ) const
+  {
+    double* buf_ptr;
+    double** x = 0;
+    double **df = 0;
+    int total = 0;
+
+    for(int i = 0; i < l_count; i++ )
+        total += layer_sizes->data.i[i];
+    CvMat* buf;
+    buf = cvCreateMat( 1, buf_sz, CV_64F );
+    x = (double**)cvAlloc( total*2*sizeof(x[0]) );
+    df = x + total;
+    buf_ptr = buf->data.db;
+    for(int i = 0; i < l_count; i++ )
+    {
+        x[i] = buf_ptr;
+        df[i] = x[i] + layer_sizes->data.i[i]*dcount0;
+        buf_ptr += (df[i] - x[i])*2;
+    }
+
+    for(int si = range.start; si < range.end; si++ )
+    {
+        if (si % dcount0 != 0) continue;
+        int n1, n2, k;
+        double* w;
+        CvMat _w, _dEdw, hdr1, hdr2, ghdr1, ghdr2, _df;
+        CvMat *x1, *x2, *grad1, *grad2, *temp;
+        int dcount = 0;
+
+        dcount = MIN(count - si , dcount0 );
+        w = weights[0];
+        grad1 = &ghdr1; grad2 = &ghdr2;
+        x1 = &hdr1; x2 = &hdr2;
+
+        // grab and preprocess input data
+        if( x0->type == CV_32F )
+    {
+            for(int i = 0; i < dcount; i++ )
+            {
+                const float* x0data = x0->data.fl[si+i];
+                double* xdata = x[0]+i*ivcount;
+                for(int j = 0; j < ivcount; j++ )
+                    xdata[j] = x0data[j]*w[j*2] + w[j*2+1];
+            }
+    }
+        else
+            for(int i = 0; i < dcount; i++ )
+            {
+                const double* x0data = x0->data.db[si+i];
+                double* xdata = x[0]+i*ivcount;
+                for(int j = 0; j < ivcount; j++ )
+                    xdata[j] = x0data[j]*w[j*2] + w[j*2+1];
+            }
+        cvInitMatHeader( x1, dcount, ivcount, CV_64F, x[0] );
+
+        // forward pass, compute y[i]=w*x[i-1], x[i]=f(y[i]), df[i]=f'(y[i])
+        for(int i = 1; i < l_count; i++ )
+        {
+            cvInitMatHeader( x2, dcount, layer_sizes->data.i[i], CV_64F, x[i] );
+            cvInitMatHeader( &_w, x1->cols, x2->cols, CV_64F, weights[i] );
+            cvGEMM( x1, &_w, 1, 0, 0, x2 );
+            _df = *x2;
+            _df.data.db = df[i];
+            point->calc_activ_func_deriv( x2, &_df, _w.data.db + _w.rows*_w.cols );
+            CV_SWAP( x1, x2, temp );
+        }
+        cvInitMatHeader( grad1, dcount, ovcount, CV_64F, buf_ptr );
+
+        w = weights[l_count+1];
+        grad2->data.db = buf_ptr + max_count*dcount;
+
+        // calculate error
+        if( u->type == CV_32F )
+            for(int i = 0; i < dcount; i++ )
+            {
+                const float* udata = u->data.fl[si+i];
+                const double* xdata = x[l_count-1] + i*ovcount;
+                double* gdata = grad1->data.db + i*ovcount;
+                double sweight = sw ? sw[si+i] : inv_count, E1 = 0;
+
+                for(int j = 0; j < ovcount; j++ )
+                {
+                    double t = udata[j]*w[j*2] + w[j*2+1] - xdata[j];
+                    gdata[j] = t*sweight;
+                    E1 += t*t;
+                }
+                *E += sweight*E1;
+            }
+        else
+            for(int i = 0; i < dcount; i++ )
+            {
+                const double* udata = u->data.db[si+i];
+                const double* xdata = x[l_count-1] + i*ovcount;
+                double* gdata = grad1->data.db + i*ovcount;
+                double sweight = sw ? sw[si+i] : inv_count, E1 = 0;
+
+                for(int j = 0; j < ovcount; j++ )
+                {
+                    double t = udata[j]*w[j*2] + w[j*2+1] - xdata[j];
+                    gdata[j] = t*sweight;
+                    E1 += t*t;
+                }
+                *E += sweight*E1;
+            }
+
+        // backward pass, update dEdw
+        static cv::Mutex mutex;
+
+        for(int i = l_count-1; i > 0; i-- )
+        {
+            n1 = layer_sizes->data.i[i-1]; n2 = layer_sizes->data.i[i];
+            cvInitMatHeader( &_df, dcount, n2, CV_64F, df[i] );
+            cvMul( grad1, &_df, grad1 );
+
+            {
+                cv::AutoLock lock(mutex);
+                cvInitMatHeader( &_dEdw, n1, n2, CV_64F, dEdw->data.db+(weights[i]-weights[0]) );
+                cvInitMatHeader( x1, dcount, n1, CV_64F, x[i-1] );
+                cvGEMM( x1, grad1, 1, &_dEdw, 1, &_dEdw, CV_GEMM_A_T );
+
+                // update bias part of dEdw
+                for( k = 0; k < dcount; k++ )
+                {
+                    double* dst = _dEdw.data.db + n1*n2;
+                    const double* src = grad1->data.db + k*n2;
+                    for(int j = 0; j < n2; j++ )
+                        dst[j] += src[j];
+                }
+
+                if (i > 1)
+                    cvInitMatHeader( &_w, n1, n2, CV_64F, weights[i] );
+           }
+
+           cvInitMatHeader( grad2, dcount, n1, CV_64F, grad2->data.db );
+           if( i > 1 )
+               cvGEMM( grad1, &_w, 1, 0, 0, grad2, CV_GEMM_B_T );
+           CV_SWAP( grad1, grad2, temp );
+        }
+    }
+    cvFree(&x);
+    cvReleaseMat( &buf );
+}
+
+};
+
+
+int CvANN_MLP::train_rprop( CvVectors x0, CvVectors u, const double* sw )
+{
+    const int max_buf_size = 1 << 16;
+    CvMat* dw = 0;
+    CvMat* dEdw = 0;
+    CvMat* prev_dEdw_sign = 0;
+    CvMat* buf = 0;
+    double **x = 0, **df = 0;
+    int iter = -1, count = x0.count;
+
+    CV_FUNCNAME( "CvANN_MLP::train" );
+
+    __BEGIN__;
+
+    int i, ivcount, ovcount, l_count, total = 0, max_iter, buf_sz, dcount0;
+    double *buf_ptr;
+    double prev_E = DBL_MAX*0.5, epsilon;
+    double dw_plus, dw_minus, dw_min, dw_max;
+    double inv_count;
+
+    max_iter = params.term_crit.max_iter;
+    epsilon = params.term_crit.epsilon;
+    dw_plus = params.rp_dw_plus;
+    dw_minus = params.rp_dw_minus;
+    dw_min = params.rp_dw_min;
+    dw_max = params.rp_dw_max;
+
+    l_count = layer_sizes->cols;
+    ivcount = layer_sizes->data.i[0];
+    ovcount = layer_sizes->data.i[l_count-1];
+
+    // allocate buffers
+    for( i = 0; i < l_count; i++ )
+        total += layer_sizes->data.i[i];
+
+    CV_CALL( dw = cvCreateMat( wbuf->rows, wbuf->cols, wbuf->type ));
+    cvSet( dw, cvScalarAll(params.rp_dw0) );
+    CV_CALL( dEdw = cvCreateMat( wbuf->rows, wbuf->cols, wbuf->type ));
+    cvZero( dEdw );
+    CV_CALL( prev_dEdw_sign = cvCreateMat( wbuf->rows, wbuf->cols, CV_8SC1 ));
+    cvZero( prev_dEdw_sign );
+
+    inv_count = 1./count;
+    dcount0 = max_buf_size/(2*total);
+    dcount0 = MAX( dcount0, 1 );
+    dcount0 = MIN( dcount0, count );
+    buf_sz = dcount0*(total + max_count)*2;
+
+    CV_CALL( buf = cvCreateMat( 1, buf_sz, CV_64F ));
+
+    CV_CALL( x = (double**)cvAlloc( total*2*sizeof(x[0]) ));
+    df = x + total;
+    buf_ptr = buf->data.db;
+
+    for( i = 0; i < l_count; i++ )
+    {
+        x[i] = buf_ptr;
+        df[i] = x[i] + layer_sizes->data.i[i]*dcount0;
+        buf_ptr += (df[i] - x[i])*2;
+    }
+
+    // run rprop loop
+    /*
+        y_i(t) = w_i(t)*x_{i-1}(t)
+        x_i(t) = f(y_i(t))
+        E = sum_over_all_samples(1/2*||u - x_N||^2)
+        grad_N = (x_N - u)*f'(y_i)
+
+                      MIN(dw_i{jk}(t)*dw_plus, dw_max), if dE/dw_i{jk}(t)*dE/dw_i{jk}(t-1) > 0
+        dw_i{jk}(t) = MAX(dw_i{jk}(t)*dw_minus, dw_min), if dE/dw_i{jk}(t)*dE/dw_i{jk}(t-1) < 0
+                      dw_i{jk}(t-1) else
+
+        if (dE/dw_i{jk}(t)*dE/dw_i{jk}(t-1) < 0)
+           dE/dw_i{jk}(t)<-0
+        else
+           w_i{jk}(t+1) = w_i{jk}(t) + dw_i{jk}(t)
+        grad_{i-1}(t) = w_i^t(t)*grad_i(t)
+    */
+    for( iter = 0; iter < max_iter; iter++ )
+    {
+        int n1, n2, j, k;
+        double E = 0;
+
+        // first, iterate through all the samples and compute dEdw
+        cv::parallel_for_(cv::Range(0, count),
+            rprop_loop(this, weights, count, ivcount, &x0, l_count, layer_sizes,
+                       ovcount, max_count, &u, sw, inv_count, dEdw, dcount0, &E, buf_sz)
+        );
+
+        // now update weights
+        for( i = 1; i < l_count; i++ )
+        {
+            n1 = layer_sizes->data.i[i-1]; n2 = layer_sizes->data.i[i];
+            for( k = 0; k <= n1; k++ )
+            {
+                double* wk = weights[i]+k*n2;
+                size_t delta = wk - weights[0];
+                double* dwk = dw->data.db + delta;
+                double* dEdwk = dEdw->data.db + delta;
+                char* prevEk = (char*)(prev_dEdw_sign->data.ptr + delta);
+
+                for( j = 0; j < n2; j++ )
+                {
+                    double Eval = dEdwk[j];
+                    double dval = dwk[j];
+                    double wval = wk[j];
+                    int s = CV_SIGN(Eval);
+                    int ss = prevEk[j]*s;
+                    if( ss > 0 )
+                    {
+                        dval *= dw_plus;
+                        dval = MIN( dval, dw_max );
+                        dwk[j] = dval;
+                        wk[j] = wval + dval*s;
+                    }
+                    else if( ss < 0 )
+                    {
+                        dval *= dw_minus;
+                        dval = MAX( dval, dw_min );
+                        prevEk[j] = 0;
+                        dwk[j] = dval;
+                        wk[j] = wval + dval*s;
+                    }
+                    else
+                    {
+                        prevEk[j] = (char)s;
+                        wk[j] = wval + dval*s;
+                    }
+                    dEdwk[j] = 0.;
+                }
+            }
+        }
+
+        //printf("%d. E = %g\n", iter, E);
+        if( fabs(prev_E - E) < epsilon )
+            break;
+        prev_E = E;
+        E = 0;
+    }
+
+    __END__;
+
+    cvReleaseMat( &dw );
+    cvReleaseMat( &dEdw );
+    cvReleaseMat( &prev_dEdw_sign );
+    cvReleaseMat( &buf );
+    cvFree( &x );
+
+    return iter;
+}
+
+
+void CvANN_MLP::write_params( CvFileStorage* fs ) const
+{
+    //CV_FUNCNAME( "CvANN_MLP::write_params" );
+
+    __BEGIN__;
+
+    const char* activ_func_name = activ_func == IDENTITY ? "IDENTITY" :
+                            activ_func == SIGMOID_SYM ? "SIGMOID_SYM" :
+                            activ_func == GAUSSIAN ? "GAUSSIAN" : 0;
+
+    if( activ_func_name )
+        cvWriteString( fs, "activation_function", activ_func_name );
+    else
+        cvWriteInt( fs, "activation_function", activ_func );
+
+    if( activ_func != IDENTITY )
+    {
+        cvWriteReal( fs, "f_param1", f_param1 );
+        cvWriteReal( fs, "f_param2", f_param2 );
+    }
+
+    cvWriteReal( fs, "min_val", min_val );
+    cvWriteReal( fs, "max_val", max_val );
+    cvWriteReal( fs, "min_val1", min_val1 );
+    cvWriteReal( fs, "max_val1", max_val1 );
+
+    cvStartWriteStruct( fs, "training_params", CV_NODE_MAP );
+    if( params.train_method == CvANN_MLP_TrainParams::BACKPROP )
+    {
+        cvWriteString( fs, "train_method", "BACKPROP" );
+        cvWriteReal( fs, "dw_scale", params.bp_dw_scale );
+        cvWriteReal( fs, "moment_scale", params.bp_moment_scale );
+    }
+    else if( params.train_method == CvANN_MLP_TrainParams::RPROP )
+    {
+        cvWriteString( fs, "train_method", "RPROP" );
+        cvWriteReal( fs, "dw0", params.rp_dw0 );
+        cvWriteReal( fs, "dw_plus", params.rp_dw_plus );
+        cvWriteReal( fs, "dw_minus", params.rp_dw_minus );
+        cvWriteReal( fs, "dw_min", params.rp_dw_min );
+        cvWriteReal( fs, "dw_max", params.rp_dw_max );
+    }
+
+    cvStartWriteStruct( fs, "term_criteria", CV_NODE_MAP + CV_NODE_FLOW );
+    if( params.term_crit.type & CV_TERMCRIT_EPS )
+        cvWriteReal( fs, "epsilon", params.term_crit.epsilon );
+    if( params.term_crit.type & CV_TERMCRIT_ITER )
+        cvWriteInt( fs, "iterations", params.term_crit.max_iter );
+    cvEndWriteStruct( fs );
+
+    cvEndWriteStruct( fs );
+
+    __END__;
+}
+
+
+void CvANN_MLP::write( CvFileStorage* fs, const char* name ) const
+{
+    CV_FUNCNAME( "CvANN_MLP::write" );
+
+    __BEGIN__;
+
+    int i, l_count = layer_sizes->cols;
+
+    if( !layer_sizes )
+        CV_ERROR( CV_StsError, "The network has not been initialized" );
+
+    cvStartWriteStruct( fs, name, CV_NODE_MAP, CV_TYPE_NAME_ML_ANN_MLP );
+
+    cvWrite( fs, "layer_sizes", layer_sizes );
+
+    write_params( fs );
+
+    cvStartWriteStruct( fs, "input_scale", CV_NODE_SEQ + CV_NODE_FLOW );
+    cvWriteRawData( fs, weights[0], layer_sizes->data.i[0]*2, "d" );
+    cvEndWriteStruct( fs );
+
+    cvStartWriteStruct( fs, "output_scale", CV_NODE_SEQ + CV_NODE_FLOW );
+    cvWriteRawData( fs, weights[l_count], layer_sizes->data.i[l_count-1]*2, "d" );
+    cvEndWriteStruct( fs );
+
+    cvStartWriteStruct( fs, "inv_output_scale", CV_NODE_SEQ + CV_NODE_FLOW );
+    cvWriteRawData( fs, weights[l_count+1], layer_sizes->data.i[l_count-1]*2, "d" );
+    cvEndWriteStruct( fs );
+
+    cvStartWriteStruct( fs, "weights", CV_NODE_SEQ );
+    for( i = 1; i < l_count; i++ )
+    {
+        cvStartWriteStruct( fs, 0, CV_NODE_SEQ + CV_NODE_FLOW );
+        cvWriteRawData( fs, weights[i], (layer_sizes->data.i[i-1]+1)*layer_sizes->data.i[i], "d" );
+        cvEndWriteStruct( fs );
+    }
+
+    cvEndWriteStruct( fs );
+
+    __END__;
+}
+
+
+void CvANN_MLP::read_params( CvFileStorage* fs, CvFileNode* node )
+{
+    //CV_FUNCNAME( "CvANN_MLP::read_params" );
+
+    __BEGIN__;
+
+    const char* activ_func_name = cvReadStringByName( fs, node, "activation_function", 0 );
+    CvFileNode* tparams_node;
+
+    if( activ_func_name )
+        activ_func = strcmp( activ_func_name, "SIGMOID_SYM" ) == 0 ? SIGMOID_SYM :
+                     strcmp( activ_func_name, "IDENTITY" ) == 0 ? IDENTITY :
+                     strcmp( activ_func_name, "GAUSSIAN" ) == 0 ? GAUSSIAN : 0;
+    else
+        activ_func = cvReadIntByName( fs, node, "activation_function" );
+
+    f_param1 = cvReadRealByName( fs, node, "f_param1", 0 );
+    f_param2 = cvReadRealByName( fs, node, "f_param2", 0 );
+
+    set_activ_func( activ_func, f_param1, f_param2 );
+
+    min_val = cvReadRealByName( fs, node, "min_val", 0. );
+    max_val = cvReadRealByName( fs, node, "max_val", 1. );
+    min_val1 = cvReadRealByName( fs, node, "min_val1", 0. );
+    max_val1 = cvReadRealByName( fs, node, "max_val1", 1. );
+
+    tparams_node = cvGetFileNodeByName( fs, node, "training_params" );
+    params = CvANN_MLP_TrainParams();
+
+    if( tparams_node )
+    {
+        const char* tmethod_name = cvReadStringByName( fs, tparams_node, "train_method", "" );
+        CvFileNode* tcrit_node;
+
+        if( strcmp( tmethod_name, "BACKPROP" ) == 0 )
+        {
+            params.train_method = CvANN_MLP_TrainParams::BACKPROP;
+            params.bp_dw_scale = cvReadRealByName( fs, tparams_node, "dw_scale", 0 );
+            params.bp_moment_scale = cvReadRealByName( fs, tparams_node, "moment_scale", 0 );
+        }
+        else if( strcmp( tmethod_name, "RPROP" ) == 0 )
+        {
+            params.train_method = CvANN_MLP_TrainParams::RPROP;
+            params.rp_dw0 = cvReadRealByName( fs, tparams_node, "dw0", 0 );
+            params.rp_dw_plus = cvReadRealByName( fs, tparams_node, "dw_plus", 0 );
+            params.rp_dw_minus = cvReadRealByName( fs, tparams_node, "dw_minus", 0 );
+            params.rp_dw_min = cvReadRealByName( fs, tparams_node, "dw_min", 0 );
+            params.rp_dw_max = cvReadRealByName( fs, tparams_node, "dw_max", 0 );
+        }
+
+        tcrit_node = cvGetFileNodeByName( fs, tparams_node, "term_criteria" );
+        if( tcrit_node )
+        {
+            params.term_crit.epsilon = cvReadRealByName( fs, tcrit_node, "epsilon", -1 );
+            params.term_crit.max_iter = cvReadIntByName( fs, tcrit_node, "iterations", -1 );
+            params.term_crit.type = (params.term_crit.epsilon >= 0 ? CV_TERMCRIT_EPS : 0) +
+                                   (params.term_crit.max_iter >= 0 ? CV_TERMCRIT_ITER : 0);
+        }
+    }
+
+    __END__;
+}
+
+
+void CvANN_MLP::read( CvFileStorage* fs, CvFileNode* node )
+{
+    CvMat* _layer_sizes = 0;
+
+    CV_FUNCNAME( "CvANN_MLP::read" );
+
+    __BEGIN__;
+
+    CvFileNode* w;
+    CvSeqReader reader;
+    int i, l_count;
+
+    _layer_sizes = (CvMat*)cvReadByName( fs, node, "layer_sizes" );
+    CV_CALL( create( _layer_sizes, SIGMOID_SYM, 0, 0 ));
+
+    cvReleaseMat( &_layer_sizes );
+    _layer_sizes = NULL;
+
+    l_count = layer_sizes->cols;
+
+    CV_CALL( read_params( fs, node ));
+
+    w = cvGetFileNodeByName( fs, node, "input_scale" );
+    if( !w || CV_NODE_TYPE(w->tag) != CV_NODE_SEQ ||
+        w->data.seq->total != layer_sizes->data.i[0]*2 )
+        CV_ERROR( CV_StsParseError, "input_scale tag is not found or is invalid" );
+
+    CV_CALL( cvReadRawData( fs, w, weights[0], "d" ));
+
+    w = cvGetFileNodeByName( fs, node, "output_scale" );
+    if( !w || CV_NODE_TYPE(w->tag) != CV_NODE_SEQ ||
+        w->data.seq->total != layer_sizes->data.i[l_count-1]*2 )
+        CV_ERROR( CV_StsParseError, "output_scale tag is not found or is invalid" );
+
+    CV_CALL( cvReadRawData( fs, w, weights[l_count], "d" ));
+
+    w = cvGetFileNodeByName( fs, node, "inv_output_scale" );
+    if( !w || CV_NODE_TYPE(w->tag) != CV_NODE_SEQ ||
+        w->data.seq->total != layer_sizes->data.i[l_count-1]*2 )
+        CV_ERROR( CV_StsParseError, "inv_output_scale tag is not found or is invalid" );
+
+    CV_CALL( cvReadRawData( fs, w, weights[l_count+1], "d" ));
+
+    w = cvGetFileNodeByName( fs, node, "weights" );
+    if( !w || CV_NODE_TYPE(w->tag) != CV_NODE_SEQ ||
+        w->data.seq->total != l_count - 1 )
+        CV_ERROR( CV_StsParseError, "weights tag is not found or is invalid" );
+
+    cvStartReadSeq( w->data.seq, &reader );
+
+    for( i = 1; i < l_count; i++ )
+    {
+        w = (CvFileNode*)reader.ptr;
+        CV_CALL( cvReadRawData( fs, w, weights[i], "d" ));
+        CV_NEXT_SEQ_ELEM( reader.seq->elem_size, reader );
+    }
+
+    __END__;
+}
+
+using namespace cv;
+
+CvANN_MLP::CvANN_MLP( const Mat& _layer_sizes, int _activ_func,
+                      double _f_param1, double _f_param2 )
+{
+    layer_sizes = wbuf = 0;
+    min_val = max_val = min_val1 = max_val1 = 0.;
+    weights = 0;
+    rng = &cv::theRNG();
+    default_model_name = "my_nn";
+    create( _layer_sizes, _activ_func, _f_param1, _f_param2 );
+}
+
+void CvANN_MLP::create( const Mat& _layer_sizes, int _activ_func,
+                       double _f_param1, double _f_param2 )
+{
+    CvMat cvlayer_sizes = _layer_sizes;
+    create( &cvlayer_sizes, _activ_func, _f_param1, _f_param2 );
+}
+
+int CvANN_MLP::train( const Mat& _inputs, const Mat& _outputs,
+                     const Mat& _sample_weights, const Mat& _sample_idx,
+                     CvANN_MLP_TrainParams _params, int flags )
+{
+    CvMat inputs = _inputs, outputs = _outputs, sweights = _sample_weights, sidx = _sample_idx;
+    return train(&inputs, &outputs, sweights.data.ptr ? &sweights : 0,
+                 sidx.data.ptr ? &sidx : 0, _params, flags);
+}
+
+float CvANN_MLP::predict( const Mat& _inputs, Mat& _outputs ) const
+{
+    CV_Assert(layer_sizes != 0);
+    _outputs.create(_inputs.rows, layer_sizes->data.i[layer_sizes->cols-1], _inputs.type());
+    CvMat inputs = _inputs, outputs = _outputs;
+
+    return predict(&inputs, &outputs);
+}
+
+/* End of file. */
diff --git a/openbr/core/old_ml_boost.cpp b/openbr/core/old_ml_boost.cpp
new file mode 100644
index 0000000..14066b8
--- /dev/null
+++ b/openbr/core/old_ml_boost.cpp
@@ -0,0 +1,2173 @@
+/*M///////////////////////////////////////////////////////////////////////////////////////
+//
+//  IMPORTANT: READ BEFORE DOWNLOADING, COPYING, INSTALLING OR USING.
+//
+//  By downloading, copying, installing or using the software you agree to this license.
+//  If you do not agree to this license, do not download, install,
+//  copy or use the software.
+//
+//
+//                        Intel License Agreement
+//
+// Copyright (C) 2000, Intel Corporation, all rights reserved.
+// Third party copyrights are property of their respective owners.
+//
+// Redistribution and use in source and binary forms, with or without modification,
+// are permitted provided that the following conditions are met:
+//
+//   * Redistribution's of source code must retain the above copyright notice,
+//     this list of conditions and the following disclaimer.
+//
+//   * Redistribution's in binary form must reproduce the above copyright notice,
+//     this list of conditions and the following disclaimer in the documentation
+//     and/or other materials provided with the distribution.
+//
+//   * The name of Intel Corporation may not be used to endorse or promote products
+//     derived from this software without specific prior written permission.
+//
+// This software is provided by the copyright holders and contributors "as is" and
+// any express or implied warranties, including, but not limited to, the implied
+// warranties of merchantability and fitness for a particular purpose are disclaimed.
+// In no event shall the Intel Corporation or contributors be liable for any direct,
+// indirect, incidental, special, exemplary, or consequential damages
+// (including, but not limited to, procurement of substitute goods or services;
+// loss of use, data, or profits; or business interruption) however caused
+// and on any theory of liability, whether in contract, strict liability,
+// or tort (including negligence or otherwise) arising in any way out of
+// the use of this software, even if advised of the possibility of such damage.
+//
+//M*/
+
+#include "old_ml_precomp.hpp"
+
+static inline double
+log_ratio( double val )
+{
+    const double eps = 1e-5;
+
+    val = MAX( val, eps );
+    val = MIN( val, 1. - eps );
+    return log( val/(1. - val) );
+}
+
+
+CvBoostParams::CvBoostParams()
+{
+    boost_type = CvBoost::REAL;
+    weak_count = 100;
+    weight_trim_rate = 0.95;
+    cv_folds = 0;
+    max_depth = 1;
+}
+
+
+CvBoostParams::CvBoostParams( int _boost_type, int _weak_count,
+                                        double _weight_trim_rate, int _max_depth,
+                                        bool _use_surrogates, const float* _priors )
+{
+    boost_type = _boost_type;
+    weak_count = _weak_count;
+    weight_trim_rate = _weight_trim_rate;
+    split_criteria = CvBoost::DEFAULT;
+    cv_folds = 0;
+    max_depth = _max_depth;
+    use_surrogates = _use_surrogates;
+    priors = _priors;
+}
+
+
+
+///////////////////////////////// CvBoostTree ///////////////////////////////////
+
+CvBoostTree::CvBoostTree()
+{
+    ensemble = 0;
+}
+
+
+CvBoostTree::~CvBoostTree()
+{
+    clear();
+}
+
+
+void
+CvBoostTree::clear()
+{
+    CvDTree::clear();
+    ensemble = 0;
+}
+
+
+bool
+CvBoostTree::train( CvDTreeTrainData* _train_data,
+                    const CvMat* _subsample_idx, CvBoost* _ensemble )
+{
+    clear();
+    ensemble = _ensemble;
+    data = _train_data;
+    data->shared = true;
+    return do_train( _subsample_idx );
+}
+
+
+bool
+CvBoostTree::train( const CvMat*, int, const CvMat*, const CvMat*,
+                    const CvMat*, const CvMat*, const CvMat*, CvDTreeParams )
+{
+    assert(0);
+    return false;
+}
+
+
+bool
+CvBoostTree::train( CvDTreeTrainData*, const CvMat* )
+{
+    assert(0);
+    return false;
+}
+
+
+void
+CvBoostTree::scale( double _scale )
+{
+    CvDTreeNode* node = root;
+
+    // traverse the tree and scale all the node values
+    for(;;)
+    {
+        CvDTreeNode* parent;
+        for(;;)
+        {
+            node->value *= _scale;
+            if( !node->left )
+                break;
+            node = node->left;
+        }
+
+        for( parent = node->parent; parent && parent->right == node;
+            node = parent, parent = parent->parent )
+            ;
+
+        if( !parent )
+            break;
+
+        node = parent->right;
+    }
+}
+
+
+void
+CvBoostTree::try_split_node( CvDTreeNode* node )
+{
+    CvDTree::try_split_node( node );
+
+    if( !node->left )
+    {
+        // if the node has not been split,
+        // store the responses for the corresponding training samples
+        double* weak_eval = ensemble->get_weak_response()->data.db;
+        cv::AutoBuffer<int> inn_buf(node->sample_count);
+//SAB        const int* labels = data->get_cv_labels(node, inn_buf.data());
+        const int* labels = data->get_cv_labels(node, inn_buf);
+        int i, count = node->sample_count;
+        double value = node->value;
+
+        for( i = 0; i < count; i++ )
+            weak_eval[labels[i]] = value;
+    }
+}
+
+
+double
+CvBoostTree::calc_node_dir( CvDTreeNode* node )
+{
+    char* dir = (char*)data->direction->data.ptr;
+    const double* weights = ensemble->get_subtree_weights()->data.db;
+    int i, n = node->sample_count, vi = node->split->var_idx;
+    double L, R;
+
+    assert( !node->split->inversed );
+
+    if( data->get_var_type(vi) >= 0 ) // split on categorical var
+    {
+        cv::AutoBuffer<int> inn_buf(n);
+//SAB        const int* cat_labels = data->get_cat_var_data(node, vi, inn_buf.data());
+        const int* cat_labels = data->get_cat_var_data(node, vi, inn_buf);
+        const int* subset = node->split->subset;
+        double sum = 0, sum_abs = 0;
+
+        for( i = 0; i < n; i++ )
+        {
+            int idx = ((cat_labels[i] == 65535) && data->is_buf_16u) ? -1 : cat_labels[i];
+            double w = weights[i];
+            int d = idx >= 0 ? CV_DTREE_CAT_DIR(idx,subset) : 0;
+            sum += d*w; sum_abs += (d & 1)*w;
+            dir[i] = (char)d;
+        }
+
+        R = (sum_abs + sum) * 0.5;
+        L = (sum_abs - sum) * 0.5;
+    }
+    else // split on ordered var
+    {
+        cv::AutoBuffer<uchar> inn_buf(2*n*sizeof(int)+n*sizeof(float));
+//SAB        float* values_buf = (float*)inn_buf.data();
+        float* values_buf = (float*)(uchar*)inn_buf;
+        int* sorted_indices_buf = (int*)(values_buf + n);
+        int* sample_indices_buf = sorted_indices_buf + n;
+        const float* values = 0;
+        const int* sorted_indices = 0;
+        data->get_ord_var_data( node, vi, values_buf, sorted_indices_buf, &values, &sorted_indices, sample_indices_buf );
+        int split_point = node->split->ord.split_point;
+        int n1 = node->get_num_valid(vi);
+
+        assert( 0 <= split_point && split_point < n1-1 );
+        L = R = 0;
+
+        for( i = 0; i <= split_point; i++ )
+        {
+            int idx = sorted_indices[i];
+            double w = weights[idx];
+            dir[idx] = (char)-1;
+            L += w;
+        }
+
+        for( ; i < n1; i++ )
+        {
+            int idx = sorted_indices[i];
+            double w = weights[idx];
+            dir[idx] = (char)1;
+            R += w;
+        }
+
+        for( ; i < n; i++ )
+            dir[sorted_indices[i]] = (char)0;
+    }
+
+    node->maxlr = MAX( L, R );
+    return node->split->quality/(L + R);
+}
+
+
+CvDTreeSplit*
+CvBoostTree::find_split_ord_class( CvDTreeNode* node, int vi, float init_quality,
+                                    CvDTreeSplit* _split, uchar* _ext_buf )
+{
+    const float epsilon = FLT_EPSILON*2;
+
+    const double* weights = ensemble->get_subtree_weights()->data.db;
+    int n = node->sample_count;
+    int n1 = node->get_num_valid(vi);
+
+    cv::AutoBuffer<uchar> inn_buf;
+    if( !_ext_buf )
+        inn_buf.allocate(n*(3*sizeof(int)+sizeof(float)));
+//SAB    uchar* ext_buf = _ext_buf ? _ext_buf : inn_buf.data();
+    uchar* ext_buf = _ext_buf ? _ext_buf : inn_buf;
+    float* values_buf = (float*)ext_buf;
+    int* sorted_indices_buf = (int*)(values_buf + n);
+    int* sample_indices_buf = sorted_indices_buf + n;
+    const float* values = 0;
+    const int* sorted_indices = 0;
+    data->get_ord_var_data( node, vi, values_buf, sorted_indices_buf, &values, &sorted_indices, sample_indices_buf );
+    int* responses_buf = sorted_indices_buf + n;
+    const int* responses = data->get_class_labels( node, responses_buf );
+    const double* rcw0 = weights + n;
+    double lcw[2] = {0,0}, rcw[2];
+    int i, best_i = -1;
+    double best_val = init_quality;
+    int boost_type = ensemble->get_params().boost_type;
+    int split_criteria = ensemble->get_params().split_criteria;
+
+    rcw[0] = rcw0[0]; rcw[1] = rcw0[1];
+    for( i = n1; i < n; i++ )
+    {
+        int idx = sorted_indices[i];
+        double w = weights[idx];
+        rcw[responses[idx]] -= w;
+    }
+
+    if( split_criteria != CvBoost::GINI && split_criteria != CvBoost::MISCLASS )
+        split_criteria = boost_type == CvBoost::DISCRETE ? CvBoost::MISCLASS : CvBoost::GINI;
+
+    if( split_criteria == CvBoost::GINI )
+    {
+        double L = 0, R = rcw[0] + rcw[1];
+        double lsum2 = 0, rsum2 = rcw[0]*rcw[0] + rcw[1]*rcw[1];
+
+        for( i = 0; i < n1 - 1; i++ )
+        {
+            int idx = sorted_indices[i];
+            double w = weights[idx], w2 = w*w;
+            double lv, rv;
+            idx = responses[idx];
+            L += w; R -= w;
+            lv = lcw[idx]; rv = rcw[idx];
+            lsum2 += 2*lv*w + w2;
+            rsum2 -= 2*rv*w - w2;
+            lcw[idx] = lv + w; rcw[idx] = rv - w;
+
+            if( values[i] + epsilon < values[i+1] )
+            {
+                double val = (lsum2*R + rsum2*L)/(L*R);
+                if( best_val < val )
+                {
+                    best_val = val;
+                    best_i = i;
+                }
+            }
+        }
+    }
+    else
+    {
+        for( i = 0; i < n1 - 1; i++ )
+        {
+            int idx = sorted_indices[i];
+            double w = weights[idx];
+            idx = responses[idx];
+            lcw[idx] += w;
+            rcw[idx] -= w;
+
+            if( values[i] + epsilon < values[i+1] )
+            {
+                double val = lcw[0] + rcw[1], val2 = lcw[1] + rcw[0];
+                val = MAX(val, val2);
+                if( best_val < val )
+                {
+                    best_val = val;
+                    best_i = i;
+                }
+            }
+        }
+    }
+
+    CvDTreeSplit* split = 0;
+    if( best_i >= 0 )
+    {
+        split = _split ? _split : data->new_split_ord( 0, 0.0f, 0, 0, 0.0f );
+        split->var_idx = vi;
+        split->ord.c = (values[best_i] + values[best_i+1])*0.5f;
+        split->ord.split_point = best_i;
+        split->inversed = 0;
+        split->quality = (float)best_val;
+    }
+    return split;
+}
+
+template<typename T>
+class LessThanPtr
+{
+public:
+    bool operator()(T* a, T* b) const { return *a < *b; }
+};
+
+CvDTreeSplit*
+CvBoostTree::find_split_cat_class( CvDTreeNode* node, int vi, float init_quality, CvDTreeSplit* _split, uchar* _ext_buf )
+{
+    int ci = data->get_var_type(vi);
+    int n = node->sample_count;
+    int mi = data->cat_count->data.i[ci];
+
+    int base_size = (2*mi+3)*sizeof(double) + mi*sizeof(double*);
+    cv::AutoBuffer<uchar> inn_buf((2*mi+3)*sizeof(double) + mi*sizeof(double*));
+    if( !_ext_buf)
+        inn_buf.allocate( base_size + 2*n*sizeof(int) );
+//SAB    uchar* base_buf = inn_buf.data();
+    uchar* base_buf = inn_buf;
+    uchar* ext_buf = _ext_buf ? _ext_buf : base_buf + base_size;
+
+    int* cat_labels_buf = (int*)ext_buf;
+    const int* cat_labels = data->get_cat_var_data(node, vi, cat_labels_buf);
+    int* responses_buf = cat_labels_buf + n;
+    const int* responses = data->get_class_labels(node, responses_buf);
+    double lcw[2]={0,0}, rcw[2]={0,0};
+
+    double* cjk = (double*)cv::alignPtr(base_buf,sizeof(double))+2;
+    const double* weights = ensemble->get_subtree_weights()->data.db;
+    double** dbl_ptr = (double**)(cjk + 2*mi);
+    int i, j, k, idx;
+    double L = 0, R;
+    double best_val = init_quality;
+    int best_subset = -1, subset_i;
+    int boost_type = ensemble->get_params().boost_type;
+    int split_criteria = ensemble->get_params().split_criteria;
+
+    // init array of counters:
+    // c_{jk} - number of samples that have vi-th input variable = j and response = k.
+    for( j = -1; j < mi; j++ )
+        cjk[j*2] = cjk[j*2+1] = 0;
+
+    for( i = 0; i < n; i++ )
+    {
+        double w = weights[i];
+        j = ((cat_labels[i] == 65535) && data->is_buf_16u) ? -1 : cat_labels[i];
+        k = responses[i];
+        cjk[j*2 + k] += w;
+    }
+
+    for( j = 0; j < mi; j++ )
+    {
+        rcw[0] += cjk[j*2];
+        rcw[1] += cjk[j*2+1];
+        dbl_ptr[j] = cjk + j*2 + 1;
+    }
+
+    R = rcw[0] + rcw[1];
+
+    if( split_criteria != CvBoost::GINI && split_criteria != CvBoost::MISCLASS )
+        split_criteria = boost_type == CvBoost::DISCRETE ? CvBoost::MISCLASS : CvBoost::GINI;
+
+    // sort rows of c_jk by increasing c_j,1
+    // (i.e. by the weight of samples in j-th category that belong to class 1)
+    std::sort(dbl_ptr, dbl_ptr + mi, LessThanPtr<double>());
+
+    for( subset_i = 0; subset_i < mi-1; subset_i++ )
+    {
+        idx = (int)(dbl_ptr[subset_i] - cjk)/2;
+        const double* crow = cjk + idx*2;
+        double w0 = crow[0], w1 = crow[1];
+        double weight = w0 + w1;
+
+        if( weight < FLT_EPSILON )
+            continue;
+
+        lcw[0] += w0; rcw[0] -= w0;
+        lcw[1] += w1; rcw[1] -= w1;
+
+        if( split_criteria == CvBoost::GINI )
+        {
+            double lsum2 = lcw[0]*lcw[0] + lcw[1]*lcw[1];
+            double rsum2 = rcw[0]*rcw[0] + rcw[1]*rcw[1];
+
+            L += weight;
+            R -= weight;
+
+            if( L > FLT_EPSILON && R > FLT_EPSILON )
+            {
+                double val = (lsum2*R + rsum2*L)/(L*R);
+                if( best_val < val )
+                {
+                    best_val = val;
+                    best_subset = subset_i;
+                }
+            }
+        }
+        else
+        {
+            double val = lcw[0] + rcw[1];
+            double val2 = lcw[1] + rcw[0];
+
+            val = MAX(val, val2);
+            if( best_val < val )
+            {
+                best_val = val;
+                best_subset = subset_i;
+            }
+        }
+    }
+
+    CvDTreeSplit* split = 0;
+    if( best_subset >= 0 )
+    {
+        split = _split ? _split : data->new_split_cat( 0, -1.0f);
+        split->var_idx = vi;
+        split->quality = (float)best_val;
+        memset( split->subset, 0, (data->max_c_count + 31)/32 * sizeof(int));
+        for( i = 0; i <= best_subset; i++ )
+        {
+            idx = (int)(dbl_ptr[i] - cjk) >> 1;
+            split->subset[idx >> 5] |= 1 << (idx & 31);
+        }
+    }
+    return split;
+}
+
+
+CvDTreeSplit*
+CvBoostTree::find_split_ord_reg( CvDTreeNode* node, int vi, float init_quality, CvDTreeSplit* _split, uchar* _ext_buf )
+{
+    const float epsilon = FLT_EPSILON*2;
+    const double* weights = ensemble->get_subtree_weights()->data.db;
+    int n = node->sample_count;
+    int n1 = node->get_num_valid(vi);
+
+    cv::AutoBuffer<uchar> inn_buf;
+    if( !_ext_buf )
+        inn_buf.allocate(2*n*(sizeof(int)+sizeof(float)));
+//SAB    uchar* ext_buf = _ext_buf ? _ext_buf : inn_buf.data();
+    uchar* ext_buf = _ext_buf ? _ext_buf : inn_buf;
+
+    float* values_buf = (float*)ext_buf;
+    int* indices_buf = (int*)(values_buf + n);
+    int* sample_indices_buf = indices_buf + n;
+    const float* values = 0;
+    const int* indices = 0;
+    data->get_ord_var_data( node, vi, values_buf, indices_buf, &values, &indices, sample_indices_buf );
+    float* responses_buf = (float*)(indices_buf + n);
+    const float* responses = data->get_ord_responses( node, responses_buf, sample_indices_buf );
+
+    int i, best_i = -1;
+    double L = 0, R = weights[n];
+    double best_val = init_quality, lsum = 0, rsum = node->value*R;
+
+    // compensate for missing values
+    for( i = n1; i < n; i++ )
+    {
+        int idx = indices[i];
+        double w = weights[idx];
+        rsum -= responses[idx]*w;
+        R -= w;
+    }
+
+    // find the optimal split
+    for( i = 0; i < n1 - 1; i++ )
+    {
+        int idx = indices[i];
+        double w = weights[idx];
+        double t = responses[idx]*w;
+        L += w; R -= w;
+        lsum += t; rsum -= t;
+
+        if( values[i] + epsilon < values[i+1] )
+        {
+            double val = (lsum*lsum*R + rsum*rsum*L)/(L*R);
+            if( best_val < val )
+            {
+                best_val = val;
+                best_i = i;
+            }
+        }
+    }
+
+    CvDTreeSplit* split = 0;
+    if( best_i >= 0 )
+    {
+        split = _split ? _split : data->new_split_ord( 0, 0.0f, 0, 0, 0.0f );
+        split->var_idx = vi;
+        split->ord.c = (values[best_i] + values[best_i+1])*0.5f;
+        split->ord.split_point = best_i;
+        split->inversed = 0;
+        split->quality = (float)best_val;
+    }
+    return split;
+}
+
+
+CvDTreeSplit*
+CvBoostTree::find_split_cat_reg( CvDTreeNode* node, int vi, float init_quality, CvDTreeSplit* _split, uchar* _ext_buf )
+{
+    const double* weights = ensemble->get_subtree_weights()->data.db;
+    int ci = data->get_var_type(vi);
+    int n = node->sample_count;
+    int mi = data->cat_count->data.i[ci];
+    int base_size = (2*mi+3)*sizeof(double) + mi*sizeof(double*);
+    cv::AutoBuffer<uchar> inn_buf(base_size);
+    if( !_ext_buf )
+        inn_buf.allocate(base_size + n*(2*sizeof(int) + sizeof(float)));
+//SAB    uchar* base_buf = inn_buf.data();
+    uchar* base_buf = inn_buf;
+    uchar* ext_buf = _ext_buf ? _ext_buf : base_buf + base_size;
+
+    int* cat_labels_buf = (int*)ext_buf;
+    const int* cat_labels = data->get_cat_var_data(node, vi, cat_labels_buf);
+    float* responses_buf = (float*)(cat_labels_buf + n);
+    int* sample_indices_buf = (int*)(responses_buf + n);
+    const float* responses = data->get_ord_responses(node, responses_buf, sample_indices_buf);
+
+    double* sum = (double*)cv::alignPtr(base_buf,sizeof(double)) + 1;
+    double* counts = sum + mi + 1;
+    double** sum_ptr = (double**)(counts + mi);
+    double L = 0, R = 0, best_val = init_quality, lsum = 0, rsum = 0;
+    int i, best_subset = -1, subset_i;
+
+    for( i = -1; i < mi; i++ )
+        sum[i] = counts[i] = 0;
+
+    // calculate sum response and weight of each category of the input var
+    for( i = 0; i < n; i++ )
+    {
+        int idx = ((cat_labels[i] == 65535) && data->is_buf_16u) ? -1 : cat_labels[i];
+        double w = weights[i];
+        double s = sum[idx] + responses[i]*w;
+        double nc = counts[idx] + w;
+        sum[idx] = s;
+        counts[idx] = nc;
+    }
+
+    // calculate average response in each category
+    for( i = 0; i < mi; i++ )
+    {
+        R += counts[i];
+        rsum += sum[i];
+        sum[i] = fabs(counts[i]) > DBL_EPSILON ? sum[i]/counts[i] : 0;
+        sum_ptr[i] = sum + i;
+    }
+
+    std::sort(sum_ptr, sum_ptr + mi, LessThanPtr<double>());
+
+    // revert back to unnormalized sums
+    // (there should be a very little loss in accuracy)
+    for( i = 0; i < mi; i++ )
+        sum[i] *= counts[i];
+
+    for( subset_i = 0; subset_i < mi-1; subset_i++ )
+    {
+        int idx = (int)(sum_ptr[subset_i] - sum);
+        double ni = counts[idx];
+
+        if( ni > FLT_EPSILON )
+        {
+            double s = sum[idx];
+            lsum += s; L += ni;
+            rsum -= s; R -= ni;
+
+            if( L > FLT_EPSILON && R > FLT_EPSILON )
+            {
+                double val = (lsum*lsum*R + rsum*rsum*L)/(L*R);
+                if( best_val < val )
+                {
+                    best_val = val;
+                    best_subset = subset_i;
+                }
+            }
+        }
+    }
+
+    CvDTreeSplit* split = 0;
+    if( best_subset >= 0 )
+    {
+        split = _split ? _split : data->new_split_cat( 0, -1.0f);
+        split->var_idx = vi;
+        split->quality = (float)best_val;
+        memset( split->subset, 0, (data->max_c_count + 31)/32 * sizeof(int));
+        for( i = 0; i <= best_subset; i++ )
+        {
+            int idx = (int)(sum_ptr[i] - sum);
+            split->subset[idx >> 5] |= 1 << (idx & 31);
+        }
+    }
+    return split;
+}
+
+
+CvDTreeSplit*
+CvBoostTree::find_surrogate_split_ord( CvDTreeNode* node, int vi, uchar* _ext_buf )
+{
+    const float epsilon = FLT_EPSILON*2;
+    int n = node->sample_count;
+    cv::AutoBuffer<uchar> inn_buf;
+    if( !_ext_buf )
+        inn_buf.allocate(n*(2*sizeof(int)+sizeof(float)));
+//SAB    uchar* ext_buf = _ext_buf ? _ext_buf : inn_buf.data();
+    uchar* ext_buf = _ext_buf ? _ext_buf : inn_buf;
+    float* values_buf = (float*)ext_buf;
+    int* indices_buf = (int*)(values_buf + n);
+    int* sample_indices_buf = indices_buf + n;
+    const float* values = 0;
+    const int* indices = 0;
+    data->get_ord_var_data( node, vi, values_buf, indices_buf, &values, &indices, sample_indices_buf );
+
+    const double* weights = ensemble->get_subtree_weights()->data.db;
+    const char* dir = (char*)data->direction->data.ptr;
+    int n1 = node->get_num_valid(vi);
+    // LL - number of samples that both the primary and the surrogate splits send to the left
+    // LR - ... primary split sends to the left and the surrogate split sends to the right
+    // RL - ... primary split sends to the right and the surrogate split sends to the left
+    // RR - ... both send to the right
+    int i, best_i = -1, best_inversed = 0;
+    double best_val;
+    double LL = 0, RL = 0, LR, RR;
+    double worst_val = node->maxlr;
+    double sum = 0, sum_abs = 0;
+    best_val = worst_val;
+
+    for( i = 0; i < n1; i++ )
+    {
+        int idx = indices[i];
+        double w = weights[idx];
+        int d = dir[idx];
+        sum += d*w; sum_abs += (d & 1)*w;
+    }
+
+    // sum_abs = R + L; sum = R - L
+    RR = (sum_abs + sum)*0.5;
+    LR = (sum_abs - sum)*0.5;
+
+    // initially all the samples are sent to the right by the surrogate split,
+    // LR of them are sent to the left by primary split, and RR - to the right.
+    // now iteratively compute LL, LR, RL and RR for every possible surrogate split value.
+    for( i = 0; i < n1 - 1; i++ )
+    {
+        int idx = indices[i];
+        double w = weights[idx];
+        int d = dir[idx];
+
+        if( d < 0 )
+        {
+            LL += w; LR -= w;
+            if( LL + RR > best_val && values[i] + epsilon < values[i+1] )
+            {
+                best_val = LL + RR;
+                best_i = i; best_inversed = 0;
+            }
+        }
+        else if( d > 0 )
+        {
+            RL += w; RR -= w;
+            if( RL + LR > best_val && values[i] + epsilon < values[i+1] )
+            {
+                best_val = RL + LR;
+                best_i = i; best_inversed = 1;
+            }
+        }
+    }
+
+    return best_i >= 0 && best_val > node->maxlr ? data->new_split_ord( vi,
+        (values[best_i] + values[best_i+1])*0.5f, best_i,
+        best_inversed, (float)best_val ) : 0;
+}
+
+
+CvDTreeSplit*
+CvBoostTree::find_surrogate_split_cat( CvDTreeNode* node, int vi, uchar* _ext_buf )
+{
+    const char* dir = (char*)data->direction->data.ptr;
+    const double* weights = ensemble->get_subtree_weights()->data.db;
+    int n = node->sample_count;
+    int i, mi = data->cat_count->data.i[data->get_var_type(vi)];
+
+    int base_size = (2*mi+3)*sizeof(double);
+    cv::AutoBuffer<uchar> inn_buf(base_size);
+    if( !_ext_buf )
+        inn_buf.allocate(base_size + n*sizeof(int));
+//SAB    uchar* ext_buf = _ext_buf ? _ext_buf : inn_buf.data();
+    uchar* ext_buf = _ext_buf ? _ext_buf : inn_buf;
+    int* cat_labels_buf = (int*)ext_buf;
+    const int* cat_labels = data->get_cat_var_data(node, vi, cat_labels_buf);
+
+    // LL - number of samples that both the primary and the surrogate splits send to the left
+    // LR - ... primary split sends to the left and the surrogate split sends to the right
+    // RL - ... primary split sends to the right and the surrogate split sends to the left
+    // RR - ... both send to the right
+    CvDTreeSplit* split = data->new_split_cat( vi, 0 );
+    double best_val = 0;
+    double* lc = (double*)cv::alignPtr(cat_labels_buf + n, sizeof(double)) + 1;
+    double* rc = lc + mi + 1;
+
+    for( i = -1; i < mi; i++ )
+        lc[i] = rc[i] = 0;
+
+    // 1. for each category calculate the weight of samples
+    // sent to the left (lc) and to the right (rc) by the primary split
+    for( i = 0; i < n; i++ )
+    {
+        int idx = ((cat_labels[i] == 65535) && data->is_buf_16u) ? -1 : cat_labels[i];
+        double w = weights[i];
+        int d = dir[i];
+        double sum = lc[idx] + d*w;
+        double sum_abs = rc[idx] + (d & 1)*w;
+        lc[idx] = sum; rc[idx] = sum_abs;
+    }
+
+    for( i = 0; i < mi; i++ )
+    {
+        double sum = lc[i];
+        double sum_abs = rc[i];
+        lc[i] = (sum_abs - sum) * 0.5;
+        rc[i] = (sum_abs + sum) * 0.5;
+    }
+
+    // 2. now form the split.
+    // in each category send all the samples to the same direction as majority
+    for( i = 0; i < mi; i++ )
+    {
+        double lval = lc[i], rval = rc[i];
+        if( lval > rval )
+        {
+            split->subset[i >> 5] |= 1 << (i & 31);
+            best_val += lval;
+        }
+        else
+            best_val += rval;
+    }
+
+    split->quality = (float)best_val;
+    if( split->quality <= node->maxlr )
+        cvSetRemoveByPtr( data->split_heap, split ), split = 0;
+
+    return split;
+}
+
+
+void
+CvBoostTree::calc_node_value( CvDTreeNode* node )
+{
+    int i, n = node->sample_count;
+    const double* weights = ensemble->get_weights()->data.db;
+    cv::AutoBuffer<uchar> inn_buf(n*(sizeof(int) + ( data->is_classifier ? sizeof(int) : sizeof(int) + sizeof(float))));
+//SAB    int* labels_buf = (int*)inn_buf.data();
+    int* labels_buf = (int*)(uchar*)inn_buf;
+    const int* labels = data->get_cv_labels(node, labels_buf);
+    double* subtree_weights = ensemble->get_subtree_weights()->data.db;
+    double rcw[2] = {0,0};
+    int boost_type = ensemble->get_params().boost_type;
+
+    if( data->is_classifier )
+    {
+        int* _responses_buf = labels_buf + n;
+        const int* _responses = data->get_class_labels(node, _responses_buf);
+        int m = data->get_num_classes();
+        int* cls_count = data->counts->data.i;
+        for( int k = 0; k < m; k++ )
+            cls_count[k] = 0;
+
+        for( i = 0; i < n; i++ )
+        {
+            int idx = labels[i];
+            double w = weights[idx];
+            int r = _responses[i];
+            rcw[r] += w;
+            cls_count[r]++;
+            subtree_weights[i] = w;
+        }
+
+        node->class_idx = rcw[1] > rcw[0];
+
+        if( boost_type == CvBoost::DISCRETE )
+        {
+            // ignore cat_map for responses, and use {-1,1},
+            // as the whole ensemble response is computes as sign(sum_i(weak_response_i)
+            node->value = node->class_idx*2 - 1;
+        }
+        else
+        {
+            double p = rcw[1]/(rcw[0] + rcw[1]);
+            assert( boost_type == CvBoost::REAL );
+
+            // store log-ratio of the probability
+            node->value = 0.5*log_ratio(p);
+        }
+    }
+    else
+    {
+        // in case of regression tree:
+        //  * node value is 1/n*sum_i(Y_i), where Y_i is i-th response,
+        //    n is the number of samples in the node.
+        //  * node risk is the sum of squared errors: sum_i((Y_i - <node_value>)^2)
+        double sum = 0, sum2 = 0, iw;
+        float* values_buf = (float*)(labels_buf + n);
+        int* sample_indices_buf = (int*)(values_buf + n);
+        const float* values = data->get_ord_responses(node, values_buf, sample_indices_buf);
+
+        for( i = 0; i < n; i++ )
+        {
+            int idx = labels[i];
+            double w = weights[idx]/*priors[values[i] > 0]*/;
+            double t = values[i];
+            rcw[0] += w;
+            subtree_weights[i] = w;
+            sum += t*w;
+            sum2 += t*t*w;
+        }
+
+        iw = 1./rcw[0];
+        node->value = sum*iw;
+        node->node_risk = sum2 - (sum*iw)*sum;
+
+        // renormalize the risk, as in try_split_node the unweighted formula
+        // sqrt(risk)/n is used, rather than sqrt(risk)/sum(weights_i)
+        node->node_risk *= n*iw*n*iw;
+    }
+
+    // store summary weights
+    subtree_weights[n] = rcw[0];
+    subtree_weights[n+1] = rcw[1];
+}
+
+
+void CvBoostTree::read( CvFileStorage* fs, CvFileNode* fnode, CvBoost* _ensemble, CvDTreeTrainData* _data )
+{
+    CvDTree::read( fs, fnode, _data );
+    ensemble = _ensemble;
+}
+
+void CvBoostTree::read( CvFileStorage*, CvFileNode* )
+{
+    assert(0);
+}
+
+void CvBoostTree::read( CvFileStorage* _fs, CvFileNode* _node,
+                        CvDTreeTrainData* _data )
+{
+    CvDTree::read( _fs, _node, _data );
+}
+
+
+/////////////////////////////////// CvBoost /////////////////////////////////////
+
+CvBoost::CvBoost()
+{
+    data = 0;
+    weak = 0;
+    default_model_name = "my_boost_tree";
+
+    active_vars = active_vars_abs = orig_response = sum_response = weak_eval =
+        subsample_mask = weights = subtree_weights = 0;
+    have_active_cat_vars = have_subsample = false;
+
+    clear();
+}
+
+
+void CvBoost::prune( CvSlice slice )
+{
+    if( weak && weak->total > 0 )
+    {
+        CvSeqReader reader;
+        int i, count = cvSliceLength( slice, weak );
+
+        cvStartReadSeq( weak, &reader );
+        cvSetSeqReaderPos( &reader, slice.start_index );
+
+        for( i = 0; i < count; i++ )
+        {
+            CvBoostTree* w;
+            CV_READ_SEQ_ELEM( w, reader );
+            delete w;
+        }
+
+        cvSeqRemoveSlice( weak, slice );
+    }
+}
+
+
+void CvBoost::clear()
+{
+    if( weak )
+    {
+        prune( CV_WHOLE_SEQ );
+        cvReleaseMemStorage( &weak->storage );
+    }
+    if( data )
+        delete data;
+    weak = 0;
+    data = 0;
+    cvReleaseMat( &active_vars );
+    cvReleaseMat( &active_vars_abs );
+    cvReleaseMat( &orig_response );
+    cvReleaseMat( &sum_response );
+    cvReleaseMat( &weak_eval );
+    cvReleaseMat( &subsample_mask );
+    cvReleaseMat( &weights );
+    cvReleaseMat( &subtree_weights );
+
+    have_subsample = false;
+}
+
+
+CvBoost::~CvBoost()
+{
+    clear();
+}
+
+
+CvBoost::CvBoost( const CvMat* _train_data, int _tflag,
+                  const CvMat* _responses, const CvMat* _var_idx,
+                  const CvMat* _sample_idx, const CvMat* _var_type,
+                  const CvMat* _missing_mask, CvBoostParams _params )
+{
+    weak = 0;
+    data = 0;
+    default_model_name = "my_boost_tree";
+
+    active_vars = active_vars_abs = orig_response = sum_response = weak_eval =
+        subsample_mask = weights = subtree_weights = 0;
+
+    train( _train_data, _tflag, _responses, _var_idx, _sample_idx,
+           _var_type, _missing_mask, _params );
+}
+
+
+bool
+CvBoost::set_params( const CvBoostParams& _params )
+{
+    bool ok = false;
+
+    CV_FUNCNAME( "CvBoost::set_params" );
+
+    __BEGIN__;
+
+    params = _params;
+    if( params.boost_type != DISCRETE && params.boost_type != REAL &&
+        params.boost_type != LOGIT && params.boost_type != GENTLE )
+        CV_ERROR( CV_StsBadArg, "Unknown/unsupported boosting type" );
+
+    params.weak_count = MAX( params.weak_count, 1 );
+    params.weight_trim_rate = MAX( params.weight_trim_rate, 0. );
+    params.weight_trim_rate = MIN( params.weight_trim_rate, 1. );
+    if( params.weight_trim_rate < FLT_EPSILON )
+        params.weight_trim_rate = 1.f;
+
+    if( params.boost_type == DISCRETE &&
+        params.split_criteria != GINI && params.split_criteria != MISCLASS )
+        params.split_criteria = MISCLASS;
+    if( params.boost_type == REAL &&
+        params.split_criteria != GINI && params.split_criteria != MISCLASS )
+        params.split_criteria = GINI;
+    if( (params.boost_type == LOGIT || params.boost_type == GENTLE) &&
+        params.split_criteria != SQERR )
+        params.split_criteria = SQERR;
+
+    ok = true;
+
+    __END__;
+
+    return ok;
+}
+
+
+bool
+CvBoost::train( const CvMat* _train_data, int _tflag,
+              const CvMat* _responses, const CvMat* _var_idx,
+              const CvMat* _sample_idx, const CvMat* _var_type,
+              const CvMat* _missing_mask,
+              CvBoostParams _params, bool _update )
+{
+    bool ok = false;
+    CvMemStorage* storage = 0;
+
+    CV_FUNCNAME( "CvBoost::train" );
+
+    __BEGIN__;
+
+    int i;
+
+    set_params( _params );
+
+    cvReleaseMat( &active_vars );
+    cvReleaseMat( &active_vars_abs );
+
+    if( !_update || !data )
+    {
+        clear();
+        data = new CvDTreeTrainData( _train_data, _tflag, _responses, _var_idx,
+            _sample_idx, _var_type, _missing_mask, _params, true, true );
+
+        if( data->get_num_classes() != 2 )
+            CV_ERROR( CV_StsNotImplemented,
+            "Boosted trees can only be used for 2-class classification." );
+        CV_CALL( storage = cvCreateMemStorage() );
+        weak = cvCreateSeq( 0, sizeof(CvSeq), sizeof(CvBoostTree*), storage );
+        storage = 0;
+    }
+    else
+    {
+        data->set_data( _train_data, _tflag, _responses, _var_idx,
+            _sample_idx, _var_type, _missing_mask, _params, true, true, true );
+    }
+
+    if ( (_params.boost_type == LOGIT) || (_params.boost_type == GENTLE) )
+        data->do_responses_copy();
+
+    update_weights( 0 );
+
+    for( i = 0; i < params.weak_count; i++ )
+    {
+        CvBoostTree* tree = new CvBoostTree;
+        if( !tree->train( data, subsample_mask, this ) )
+        {
+            delete tree;
+            break;
+        }
+        //cvCheckArr( get_weak_response());
+        cvSeqPush( weak, &tree );
+        update_weights( tree );
+        trim_weights();
+        if( cvCountNonZero(subsample_mask) == 0 )
+            break;
+    }
+
+    if(weak->total > 0)
+    {
+        get_active_vars(); // recompute active_vars* maps and condensed_idx's in the splits.
+        data->is_classifier = true;
+        data->free_train_data();
+        ok = true;
+    }
+    else
+        clear();
+
+    __END__;
+
+    return ok;
+}
+
+bool CvBoost::train( CvMLData* _data,
+             CvBoostParams _params,
+             bool update )
+{
+    bool result = false;
+
+    CV_FUNCNAME( "CvBoost::train" );
+
+    __BEGIN__;
+
+    const CvMat* values = _data->get_values();
+    const CvMat* response = _data->get_responses();
+    const CvMat* missing = _data->get_missing();
+    const CvMat* var_types = _data->get_var_types();
+    const CvMat* train_sidx = _data->get_train_sample_idx();
+    const CvMat* var_idx = _data->get_var_idx();
+
+    CV_CALL( result = train( values, CV_ROW_SAMPLE, response, var_idx,
+        train_sidx, var_types, missing, _params, update ) );
+
+    __END__;
+
+    return result;
+}
+
+void CvBoost::initialize_weights(double (&p)[2])
+{
+    p[0] = 1.;
+    p[1] = 1.;
+}
+
+void
+CvBoost::update_weights( CvBoostTree* tree )
+{
+    CV_FUNCNAME( "CvBoost::update_weights" );
+
+    __BEGIN__;
+
+    int i, n = data->sample_count;
+    double sumw = 0.;
+    int step = 0;
+    float* fdata = 0;
+    int *sample_idx_buf;
+    const int* sample_idx = 0;
+    cv::AutoBuffer<uchar> inn_buf;
+    size_t _buf_size = (params.boost_type == LOGIT) || (params.boost_type == GENTLE) ? (size_t)(data->sample_count)*sizeof(int) : 0;
+    if( !tree )
+        _buf_size += n*sizeof(int);
+    else
+    {
+        if( have_subsample )
+            _buf_size += data->get_length_subbuf()*(sizeof(float)+sizeof(uchar));
+    }
+    inn_buf.allocate(_buf_size);
+//SAB    uchar* cur_buf_pos = inn_buf.data();
+    uchar* cur_buf_pos = inn_buf;
+
+    if ( (params.boost_type == LOGIT) || (params.boost_type == GENTLE) )
+    {
+        step = CV_IS_MAT_CONT(data->responses_copy->type) ?
+            1 : data->responses_copy->step / CV_ELEM_SIZE(data->responses_copy->type);
+        fdata = data->responses_copy->data.fl;
+        sample_idx_buf = (int*)cur_buf_pos;
+        cur_buf_pos = (uchar*)(sample_idx_buf + data->sample_count);
+        sample_idx = data->get_sample_indices( data->data_root, sample_idx_buf );
+    }
+    CvMat* dtree_data_buf = data->buf;
+    size_t length_buf_row = data->get_length_subbuf();
+    if( !tree ) // before training the first tree, initialize weights and other parameters
+    {
+        int* class_labels_buf = (int*)cur_buf_pos;
+        cur_buf_pos = (uchar*)(class_labels_buf + n);
+        const int* class_labels = data->get_class_labels(data->data_root, class_labels_buf);
+        // in case of logitboost and gentle adaboost each weak tree is a regression tree,
+        // so we need to convert class labels to floating-point values
+
+        double w0 = 1./ n;
+        double p[2] = { 1., 1. };
+        initialize_weights(p);
+
+        cvReleaseMat( &orig_response );
+        cvReleaseMat( &sum_response );
+        cvReleaseMat( &weak_eval );
+        cvReleaseMat( &subsample_mask );
+        cvReleaseMat( &weights );
+        cvReleaseMat( &subtree_weights );
+
+        CV_CALL( orig_response = cvCreateMat( 1, n, CV_32S ));
+        CV_CALL( weak_eval = cvCreateMat( 1, n, CV_64F ));
+        CV_CALL( subsample_mask = cvCreateMat( 1, n, CV_8U ));
+        CV_CALL( weights = cvCreateMat( 1, n, CV_64F ));
+        CV_CALL( subtree_weights = cvCreateMat( 1, n + 2, CV_64F ));
+
+        if( data->have_priors )
+        {
+            // compute weight scale for each class from their prior probabilities
+            int c1 = 0;
+            for( i = 0; i < n; i++ )
+                c1 += class_labels[i];
+            p[0] = data->priors->data.db[0]*(c1 < n ? 1./(n - c1) : 0.);
+            p[1] = data->priors->data.db[1]*(c1 > 0 ? 1./c1 : 0.);
+            p[0] /= p[0] + p[1];
+            p[1] = 1. - p[0];
+        }
+
+        if (data->is_buf_16u)
+        {
+            unsigned short* labels = (unsigned short*)(dtree_data_buf->data.s + data->data_root->buf_idx*length_buf_row +
+                data->data_root->offset + (size_t)(data->work_var_count-1)*data->sample_count);
+            for( i = 0; i < n; i++ )
+            {
+                // save original categorical responses {0,1}, convert them to {-1,1}
+                orig_response->data.i[i] = class_labels[i]*2 - 1;
+                // make all the samples active at start.
+                // later, in trim_weights() deactivate/reactive again some, if need
+                subsample_mask->data.ptr[i] = (uchar)1;
+                // make all the initial weights the same.
+                weights->data.db[i] = w0*p[class_labels[i]];
+                // set the labels to find (from within weak tree learning proc)
+                // the particular sample weight, and where to store the response.
+                labels[i] = (unsigned short)i;
+            }
+        }
+        else
+        {
+            int* labels = dtree_data_buf->data.i + data->data_root->buf_idx*length_buf_row +
+                data->data_root->offset + (size_t)(data->work_var_count-1)*data->sample_count;
+
+            for( i = 0; i < n; i++ )
+            {
+                // save original categorical responses {0,1}, convert them to {-1,1}
+                orig_response->data.i[i] = class_labels[i]*2 - 1;
+                // make all the samples active at start.
+                // later, in trim_weights() deactivate/reactive again some, if need
+                subsample_mask->data.ptr[i] = (uchar)1;
+                // make all the initial weights the same.
+                weights->data.db[i] = w0*p[class_labels[i]];
+                // set the labels to find (from within weak tree learning proc)
+                // the particular sample weight, and where to store the response.
+                labels[i] = i;
+            }
+        }
+
+        if( params.boost_type == LOGIT )
+        {
+            CV_CALL( sum_response = cvCreateMat( 1, n, CV_64F ));
+
+            for( i = 0; i < n; i++ )
+            {
+                sum_response->data.db[i] = 0;
+                fdata[sample_idx[i]*step] = orig_response->data.i[i] > 0 ? 2.f : -2.f;
+            }
+
+            // in case of logitboost each weak tree is a regression tree.
+            // the target function values are recalculated for each of the trees
+            data->is_classifier = false;
+        }
+        else if( params.boost_type == GENTLE )
+        {
+            for( i = 0; i < n; i++ )
+                fdata[sample_idx[i]*step] = (float)orig_response->data.i[i];
+
+            data->is_classifier = false;
+        }
+    }
+    else
+    {
+        // at this moment, for all the samples that participated in the training of the most
+        // recent weak classifier we know the responses. For other samples we need to compute them
+        if( have_subsample )
+        {
+            float* values = (float*)cur_buf_pos;
+            cur_buf_pos = (uchar*)(values + data->get_length_subbuf());
+            uchar* missing = cur_buf_pos;
+            cur_buf_pos = missing + data->get_length_subbuf() * (size_t)CV_ELEM_SIZE(data->buf->type);
+
+            CvMat _sample, _mask;
+
+            // invert the subsample mask
+            cvXorS( subsample_mask, cvScalar(1.), subsample_mask );
+            data->get_vectors( subsample_mask, values, missing, 0 );
+
+            _sample = cvMat( 1, data->var_count, CV_32F );
+            _mask = cvMat( 1, data->var_count, CV_8U );
+
+            // run tree through all the non-processed samples
+            for( i = 0; i < n; i++ )
+                if( subsample_mask->data.ptr[i] )
+                {
+                    _sample.data.fl = values;
+                    _mask.data.ptr = missing;
+                    values += _sample.cols;
+                    missing += _mask.cols;
+                    weak_eval->data.db[i] = tree->predict( &_sample, &_mask, true )->value;
+                }
+        }
+
+        // now update weights and other parameters for each type of boosting
+        if( params.boost_type == DISCRETE )
+        {
+            // Discrete AdaBoost:
+            //   weak_eval[i] (=f(x_i)) is in {-1,1}
+            //   err = sum(w_i*(f(x_i) != y_i))/sum(w_i)
+            //   C = log((1-err)/err)
+            //   w_i *= exp(C*(f(x_i) != y_i))
+
+            double C, err = 0.;
+            double scale[] = { 1., 0. };
+
+            for( i = 0; i < n; i++ )
+            {
+                double w = weights->data.db[i];
+                sumw += w;
+                err += w*(weak_eval->data.db[i] != orig_response->data.i[i]);
+            }
+
+            if( sumw != 0 )
+                err /= sumw;
+            C = err = -log_ratio( err );
+            scale[1] = exp(err);
+
+            sumw = 0;
+            for( i = 0; i < n; i++ )
+            {
+                double w = weights->data.db[i]*
+                    scale[weak_eval->data.db[i] != orig_response->data.i[i]];
+                sumw += w;
+                weights->data.db[i] = w;
+            }
+
+            tree->scale( C );
+        }
+        else if( params.boost_type == REAL )
+        {
+            // Real AdaBoost:
+            //   weak_eval[i] = f(x_i) = 0.5*log(p(x_i)/(1-p(x_i))), p(x_i)=P(y=1|x_i)
+            //   w_i *= exp(-y_i*f(x_i))
+
+            for( i = 0; i < n; i++ )
+                weak_eval->data.db[i] *= -orig_response->data.i[i];
+
+            cvExp( weak_eval, weak_eval );
+
+            for( i = 0; i < n; i++ )
+            {
+                double w = weights->data.db[i]*weak_eval->data.db[i];
+                sumw += w;
+                weights->data.db[i] = w;
+            }
+        }
+        else if( params.boost_type == LOGIT )
+        {
+            // LogitBoost:
+            //   weak_eval[i] = f(x_i) in [-z_max,z_max]
+            //   sum_response = F(x_i).
+            //   F(x_i) += 0.5*f(x_i)
+            //   p(x_i) = exp(F(x_i))/(exp(F(x_i)) + exp(-F(x_i))=1/(1+exp(-2*F(x_i)))
+            //   reuse weak_eval: weak_eval[i] <- p(x_i)
+            //   w_i = p(x_i)*1(1 - p(x_i))
+            //   z_i = ((y_i+1)/2 - p(x_i))/(p(x_i)*(1 - p(x_i)))
+            //   store z_i to the data->data_root as the new target responses
+
+            const double lb_weight_thresh = FLT_EPSILON;
+            const double lb_z_max = 10.;
+            /*float* responses_buf = data->get_resp_float_buf();
+            const float* responses = 0;
+            data->get_ord_responses(data->data_root, responses_buf, &responses);*/
+
+            /*if( weak->total == 7 )
+                putchar('*');*/
+
+            for( i = 0; i < n; i++ )
+            {
+                double s = sum_response->data.db[i] + 0.5*weak_eval->data.db[i];
+                sum_response->data.db[i] = s;
+                weak_eval->data.db[i] = -2*s;
+            }
+
+            cvExp( weak_eval, weak_eval );
+
+            for( i = 0; i < n; i++ )
+            {
+                double p = 1./(1. + weak_eval->data.db[i]);
+                double w = p*(1 - p), z;
+                w = MAX( w, lb_weight_thresh );
+                weights->data.db[i] = w;
+                sumw += w;
+                if( orig_response->data.i[i] > 0 )
+                {
+                    z = 1./p;
+                    fdata[sample_idx[i]*step] = (float)MIN(z, lb_z_max);
+                }
+                else
+                {
+                    z = 1./(1-p);
+                    fdata[sample_idx[i]*step] = (float)-MIN(z, lb_z_max);
+                }
+            }
+        }
+        else
+        {
+            // Gentle AdaBoost:
+            //   weak_eval[i] = f(x_i) in [-1,1]
+            //   w_i *= exp(-y_i*f(x_i))
+            assert( params.boost_type == GENTLE );
+
+            for( i = 0; i < n; i++ )
+                weak_eval->data.db[i] *= -orig_response->data.i[i];
+
+            cvExp( weak_eval, weak_eval );
+
+            for( i = 0; i < n; i++ )
+            {
+                double w = weights->data.db[i] * weak_eval->data.db[i];
+                weights->data.db[i] = w;
+                sumw += w;
+            }
+        }
+    }
+
+    // renormalize weights
+    if( sumw > FLT_EPSILON )
+    {
+        sumw = 1./sumw;
+        for( i = 0; i < n; ++i )
+            weights->data.db[i] *= sumw;
+    }
+
+    __END__;
+}
+
+
+void
+CvBoost::trim_weights()
+{
+    //CV_FUNCNAME( "CvBoost::trim_weights" );
+
+    __BEGIN__;
+
+    int i, count = data->sample_count, nz_count = 0;
+    double sum, threshold;
+
+    if( params.weight_trim_rate <= 0. || params.weight_trim_rate >= 1. )
+        EXIT;
+
+    // use weak_eval as temporary buffer for sorted weights
+    cvCopy( weights, weak_eval );
+
+    std::sort(weak_eval->data.db, weak_eval->data.db + count);
+
+    // as weight trimming occurs immediately after updating the weights,
+    // where they are renormalized, we assume that the weight sum = 1.
+    sum = 1. - params.weight_trim_rate;
+
+    for( i = 0; i < count; i++ )
+    {
+        double w = weak_eval->data.db[i];
+        if( sum <= 0 )
+            break;
+        sum -= w;
+    }
+
+    threshold = i < count ? weak_eval->data.db[i] : DBL_MAX;
+
+    for( i = 0; i < count; i++ )
+    {
+        double w = weights->data.db[i];
+        int f = w >= threshold;
+        subsample_mask->data.ptr[i] = (uchar)f;
+        nz_count += f;
+    }
+
+    have_subsample = nz_count < count;
+
+    __END__;
+}
+
+
+const CvMat*
+CvBoost::get_active_vars( bool absolute_idx )
+{
+    CvMat* mask = 0;
+    CvMat* inv_map = 0;
+    CvMat* result = 0;
+
+    CV_FUNCNAME( "CvBoost::get_active_vars" );
+
+    __BEGIN__;
+
+    if( !weak )
+        CV_ERROR( CV_StsError, "The boosted tree ensemble has not been trained yet" );
+
+    if( !active_vars || !active_vars_abs )
+    {
+        CvSeqReader reader;
+        int i, j, nactive_vars;
+        CvBoostTree* wtree;
+        const CvDTreeNode* node;
+
+        assert(!active_vars && !active_vars_abs);
+        mask = cvCreateMat( 1, data->var_count, CV_8U );
+        inv_map = cvCreateMat( 1, data->var_count, CV_32S );
+        cvZero( mask );
+        cvSet( inv_map, cvScalar(-1) );
+
+        // first pass: compute the mask of used variables
+        cvStartReadSeq( weak, &reader );
+        for( i = 0; i < weak->total; i++ )
+        {
+            CV_READ_SEQ_ELEM(wtree, reader);
+
+            node = wtree->get_root();
+            assert( node != 0 );
+            for(;;)
+            {
+                const CvDTreeNode* parent;
+                for(;;)
+                {
+                    CvDTreeSplit* split = node->split;
+                    for( ; split != 0; split = split->next )
+                        mask->data.ptr[split->var_idx] = 1;
+                    if( !node->left )
+                        break;
+                    node = node->left;
+                }
+
+                for( parent = node->parent; parent && parent->right == node;
+                    node = parent, parent = parent->parent )
+                    ;
+
+                if( !parent )
+                    break;
+
+                node = parent->right;
+            }
+        }
+
+        nactive_vars = cvCountNonZero(mask);
+
+        //if ( nactive_vars > 0 )
+        {
+            active_vars = cvCreateMat( 1, nactive_vars, CV_32S );
+            active_vars_abs = cvCreateMat( 1, nactive_vars, CV_32S );
+
+            have_active_cat_vars = false;
+
+            for( i = j = 0; i < data->var_count; i++ )
+            {
+                if( mask->data.ptr[i] )
+                {
+                    active_vars->data.i[j] = i;
+                    active_vars_abs->data.i[j] = data->var_idx ? data->var_idx->data.i[i] : i;
+                    inv_map->data.i[i] = j;
+                    if( data->var_type->data.i[i] >= 0 )
+                        have_active_cat_vars = true;
+                    j++;
+                }
+            }
+
+
+            // second pass: now compute the condensed indices
+            cvStartReadSeq( weak, &reader );
+            for( i = 0; i < weak->total; i++ )
+            {
+                CV_READ_SEQ_ELEM(wtree, reader);
+                node = wtree->get_root();
+                for(;;)
+                {
+                    const CvDTreeNode* parent;
+                    for(;;)
+                    {
+                        CvDTreeSplit* split = node->split;
+                        for( ; split != 0; split = split->next )
+                        {
+                            split->condensed_idx = inv_map->data.i[split->var_idx];
+                            assert( split->condensed_idx >= 0 );
+                        }
+
+                        if( !node->left )
+                            break;
+                        node = node->left;
+                    }
+
+                    for( parent = node->parent; parent && parent->right == node;
+                        node = parent, parent = parent->parent )
+                        ;
+
+                    if( !parent )
+                        break;
+
+                    node = parent->right;
+                }
+            }
+        }
+    }
+
+    result = absolute_idx ? active_vars_abs : active_vars;
+
+    __END__;
+
+    cvReleaseMat( &mask );
+    cvReleaseMat( &inv_map );
+
+    return result;
+}
+
+
+float
+CvBoost::predict( const CvMat* _sample, const CvMat* _missing,
+                  CvMat* weak_responses, CvSlice slice,
+                  bool raw_mode, bool return_sum ) const
+{
+    float value = -FLT_MAX;
+
+    CvSeqReader reader;
+    double sum = 0;
+    int wstep = 0;
+    const float* sample_data;
+
+    if( !weak )
+        CV_Error( CV_StsError, "The boosted tree ensemble has not been trained yet" );
+
+    if( !CV_IS_MAT(_sample) || CV_MAT_TYPE(_sample->type) != CV_32FC1 ||
+        (_sample->cols != 1 && _sample->rows != 1) ||
+        (_sample->cols + _sample->rows - 1 != data->var_all && !raw_mode) ||
+        (active_vars && _sample->cols + _sample->rows - 1 != active_vars->cols && raw_mode) )
+            CV_Error( CV_StsBadArg,
+        "the input sample must be 1d floating-point vector with the same "
+        "number of elements as the total number of variables or "
+        "as the number of variables used for training" );
+
+    if( _missing )
+    {
+        if( !CV_IS_MAT(_missing) || !CV_IS_MASK_ARR(_missing) ||
+            !CV_ARE_SIZES_EQ(_missing, _sample) )
+            CV_Error( CV_StsBadArg,
+            "the missing data mask must be 8-bit vector of the same size as input sample" );
+    }
+
+    int i, weak_count = cvSliceLength( slice, weak );
+    if( weak_count >= weak->total )
+    {
+        weak_count = weak->total;
+        slice.start_index = 0;
+    }
+
+    if( weak_responses )
+    {
+        if( !CV_IS_MAT(weak_responses) ||
+            CV_MAT_TYPE(weak_responses->type) != CV_32FC1 ||
+            (weak_responses->cols != 1 && weak_responses->rows != 1) ||
+            weak_responses->cols + weak_responses->rows - 1 != weak_count )
+            CV_Error( CV_StsBadArg,
+            "The output matrix of weak classifier responses must be valid "
+            "floating-point vector of the same number of components as the length of input slice" );
+        wstep = CV_IS_MAT_CONT(weak_responses->type) ? 1 : weak_responses->step/sizeof(float);
+    }
+
+    int var_count = active_vars->cols;
+    const int* vtype = data->var_type->data.i;
+    const int* cmap = data->cat_map->data.i;
+    const int* cofs = data->cat_ofs->data.i;
+
+    cv::Mat sample = cv::cvarrToMat(_sample);
+    cv::Mat missing;
+    if(!_missing)
+        missing = cv::cvarrToMat(_missing);
+
+    // if need, preprocess the input vector
+    if( !raw_mode )
+    {
+        int sstep, mstep = 0;
+        const float* src_sample;
+        const uchar* src_mask = 0;
+        float* dst_sample;
+        uchar* dst_mask;
+        const int* vidx = active_vars->data.i;
+        const int* vidx_abs = active_vars_abs->data.i;
+        bool have_mask = _missing != 0;
+
+        sample = cv::Mat(1, var_count, CV_32FC1);
+        missing = cv::Mat(1, var_count, CV_8UC1);
+
+        dst_sample = sample.ptr<float>();
+        dst_mask = missing.ptr<uchar>();
+
+        src_sample = _sample->data.fl;
+        sstep = CV_IS_MAT_CONT(_sample->type) ? 1 : _sample->step/sizeof(src_sample[0]);
+
+        if( _missing )
+        {
+            src_mask = _missing->data.ptr;
+            mstep = CV_IS_MAT_CONT(_missing->type) ? 1 : _missing->step;
+        }
+
+        for( i = 0; i < var_count; i++ )
+        {
+            int idx = vidx[i], idx_abs = vidx_abs[i];
+            float val = src_sample[idx_abs*sstep];
+            int ci = vtype[idx];
+            uchar m = src_mask ? src_mask[idx_abs*mstep] : (uchar)0;
+
+            if( ci >= 0 )
+            {
+                int a = cofs[ci], b = (ci+1 >= data->cat_ofs->cols) ? data->cat_map->cols : cofs[ci+1],
+                    c = a;
+                int ival = cvRound(val);
+                if ( (ival != val) && (!m) )
+                    CV_Error( CV_StsBadArg,
+                        "one of input categorical variable is not an integer" );
+
+                while( a < b )
+                {
+                    c = (a + b) >> 1;
+                    if( ival < cmap[c] )
+                        b = c;
+                    else if( ival > cmap[c] )
+                        a = c+1;
+                    else
+                        break;
+                }
+
+                if( c < 0 || ival != cmap[c] )
+                {
+                    m = 1;
+                    have_mask = true;
+                }
+                else
+                {
+                    val = (float)(c - cofs[ci]);
+                }
+            }
+
+            dst_sample[i] = val;
+            dst_mask[i] = m;
+        }
+
+        if( !have_mask )
+            missing.release();
+    }
+    else
+    {
+        if( !CV_IS_MAT_CONT(_sample->type & (_missing ? _missing->type : -1)) )
+            CV_Error( CV_StsBadArg, "In raw mode the input vectors must be continuous" );
+    }
+
+    cvStartReadSeq( weak, &reader );
+    cvSetSeqReaderPos( &reader, slice.start_index );
+
+    sample_data = sample.ptr<float>();
+
+    if( !have_active_cat_vars && missing.empty() && !weak_responses )
+    {
+        for( i = 0; i < weak_count; i++ )
+        {
+            CvBoostTree* wtree;
+            const CvDTreeNode* node;
+            CV_READ_SEQ_ELEM( wtree, reader );
+
+            node = wtree->get_root();
+            while( node->left )
+            {
+                CvDTreeSplit* split = node->split;
+                int vi = split->condensed_idx;
+                float val = sample_data[vi];
+                int dir = val <= split->ord.c ? -1 : 1;
+                if( split->inversed )
+                    dir = -dir;
+                node = dir < 0 ? node->left : node->right;
+            }
+            sum += node->value;
+        }
+    }
+    else
+    {
+        const int* avars = active_vars->data.i;
+        const uchar* m = !missing.empty() ? missing.ptr<uchar>() : 0;
+
+        // full-featured version
+        for( i = 0; i < weak_count; i++ )
+        {
+            CvBoostTree* wtree;
+            const CvDTreeNode* node;
+            CV_READ_SEQ_ELEM( wtree, reader );
+
+            node = wtree->get_root();
+            while( node->left )
+            {
+                const CvDTreeSplit* split = node->split;
+                int dir = 0;
+                for( ; !dir && split != 0; split = split->next )
+                {
+                    int vi = split->condensed_idx;
+                    int ci = vtype[avars[vi]];
+                    float val = sample_data[vi];
+                    if( m && m[vi] )
+                        continue;
+                    if( ci < 0 ) // ordered
+                        dir = val <= split->ord.c ? -1 : 1;
+                    else // categorical
+                    {
+                        int c = cvRound(val);
+                        dir = CV_DTREE_CAT_DIR(c, split->subset);
+                    }
+                    if( split->inversed )
+                        dir = -dir;
+                }
+
+                if( !dir )
+                {
+                    int diff = node->right->sample_count - node->left->sample_count;
+                    dir = diff < 0 ? -1 : 1;
+                }
+                node = dir < 0 ? node->left : node->right;
+            }
+            if( weak_responses )
+                weak_responses->data.fl[i*wstep] = (float)node->value;
+            sum += node->value;
+        }
+    }
+
+    if( return_sum )
+        value = (float)sum;
+    else
+    {
+        int cls_idx = sum >= 0;
+        if( raw_mode )
+            value = (float)cls_idx;
+        else
+            value = (float)cmap[cofs[vtype[data->var_count]] + cls_idx];
+    }
+
+    return value;
+}
+
+float CvBoost::calc_error( CvMLData* _data, int type, std::vector<float> *resp )
+{
+    float err = 0;
+    const CvMat* values = _data->get_values();
+    const CvMat* response = _data->get_responses();
+    const CvMat* missing = _data->get_missing();
+    const CvMat* sample_idx = (type == CV_TEST_ERROR) ? _data->get_test_sample_idx() : _data->get_train_sample_idx();
+    const CvMat* var_types = _data->get_var_types();
+    int* sidx = sample_idx ? sample_idx->data.i : 0;
+    int r_step = CV_IS_MAT_CONT(response->type) ?
+                1 : response->step / CV_ELEM_SIZE(response->type);
+    bool is_classifier = var_types->data.ptr[var_types->cols-1] == CV_VAR_CATEGORICAL;
+    int sample_count = sample_idx ? sample_idx->cols : 0;
+    sample_count = (type == CV_TRAIN_ERROR && sample_count == 0) ? values->rows : sample_count;
+    float* pred_resp = 0;
+    if( resp && (sample_count > 0) )
+    {
+        resp->resize( sample_count );
+        pred_resp = &((*resp)[0]);
+    }
+    if ( is_classifier )
+    {
+        for( int i = 0; i < sample_count; i++ )
+        {
+            CvMat sample, miss;
+            int si = sidx ? sidx[i] : i;
+            cvGetRow( values, &sample, si );
+            if( missing )
+                cvGetRow( missing, &miss, si );
+            float r = (float)predict( &sample, missing ? &miss : 0 );
+            if( pred_resp )
+                pred_resp[i] = r;
+            int d = fabs((double)r - response->data.fl[si*r_step]) <= FLT_EPSILON ? 0 : 1;
+            err += d;
+        }
+        err = sample_count ? err / (float)sample_count * 100 : -FLT_MAX;
+    }
+    else
+    {
+        for( int i = 0; i < sample_count; i++ )
+        {
+            CvMat sample, miss;
+            int si = sidx ? sidx[i] : i;
+            cvGetRow( values, &sample, si );
+            if( missing )
+                cvGetRow( missing, &miss, si );
+            float r = (float)predict( &sample, missing ? &miss : 0 );
+            if( pred_resp )
+                pred_resp[i] = r;
+            float d = r - response->data.fl[si*r_step];
+            err += d*d;
+        }
+        err = sample_count ? err / (float)sample_count : -FLT_MAX;
+    }
+    return err;
+}
+
+void CvBoost::write_params( CvFileStorage* fs ) const
+{
+    const char* boost_type_str =
+        params.boost_type == DISCRETE ? "DiscreteAdaboost" :
+        params.boost_type == REAL ? "RealAdaboost" :
+        params.boost_type == LOGIT ? "LogitBoost" :
+        params.boost_type == GENTLE ? "GentleAdaboost" : 0;
+
+    const char* split_crit_str =
+        params.split_criteria == DEFAULT ? "Default" :
+        params.split_criteria == GINI ? "Gini" :
+        params.boost_type == MISCLASS ? "Misclassification" :
+        params.boost_type == SQERR ? "SquaredErr" : 0;
+
+    if( boost_type_str )
+        cvWriteString( fs, "boosting_type", boost_type_str );
+    else
+        cvWriteInt( fs, "boosting_type", params.boost_type );
+
+    if( split_crit_str )
+        cvWriteString( fs, "splitting_criteria", split_crit_str );
+    else
+        cvWriteInt( fs, "splitting_criteria", params.split_criteria );
+
+    cvWriteInt( fs, "ntrees", weak->total );
+    cvWriteReal( fs, "weight_trimming_rate", params.weight_trim_rate );
+
+    data->write_params( fs );
+}
+
+
+void CvBoost::read_params( CvFileStorage* fs, CvFileNode* fnode )
+{
+    CV_FUNCNAME( "CvBoost::read_params" );
+
+    __BEGIN__;
+
+    CvFileNode* temp;
+
+    if( !fnode || !CV_NODE_IS_MAP(fnode->tag) )
+        return;
+
+    data = new CvDTreeTrainData();
+    CV_CALL( data->read_params(fs, fnode));
+    data->shared = true;
+
+    params.max_depth = data->params.max_depth;
+    params.min_sample_count = data->params.min_sample_count;
+    params.max_categories = data->params.max_categories;
+    params.priors = data->params.priors;
+    params.regression_accuracy = data->params.regression_accuracy;
+    params.use_surrogates = data->params.use_surrogates;
+
+    temp = cvGetFileNodeByName( fs, fnode, "boosting_type" );
+    if( !temp )
+        return;
+
+    if( temp && CV_NODE_IS_STRING(temp->tag) )
+    {
+        const char* boost_type_str = cvReadString( temp, "" );
+        params.boost_type = strcmp( boost_type_str, "DiscreteAdaboost" ) == 0 ? DISCRETE :
+                            strcmp( boost_type_str, "RealAdaboost" ) == 0 ? REAL :
+                            strcmp( boost_type_str, "LogitBoost" ) == 0 ? LOGIT :
+                            strcmp( boost_type_str, "GentleAdaboost" ) == 0 ? GENTLE : -1;
+    }
+    else
+        params.boost_type = cvReadInt( temp, -1 );
+
+    if( params.boost_type < DISCRETE || params.boost_type > GENTLE )
+        CV_ERROR( CV_StsBadArg, "Unknown boosting type" );
+
+    temp = cvGetFileNodeByName( fs, fnode, "splitting_criteria" );
+    if( temp && CV_NODE_IS_STRING(temp->tag) )
+    {
+        const char* split_crit_str = cvReadString( temp, "" );
+        params.split_criteria = strcmp( split_crit_str, "Default" ) == 0 ? DEFAULT :
+                                strcmp( split_crit_str, "Gini" ) == 0 ? GINI :
+                                strcmp( split_crit_str, "Misclassification" ) == 0 ? MISCLASS :
+                                strcmp( split_crit_str, "SquaredErr" ) == 0 ? SQERR : -1;
+    }
+    else
+        params.split_criteria = cvReadInt( temp, -1 );
+
+    if( params.split_criteria < DEFAULT || params.boost_type > SQERR )
+        CV_ERROR( CV_StsBadArg, "Unknown boosting type" );
+
+    params.weak_count = cvReadIntByName( fs, fnode, "ntrees" );
+    params.weight_trim_rate = cvReadRealByName( fs, fnode, "weight_trimming_rate", 0. );
+
+    __END__;
+}
+
+
+
+void
+CvBoost::read( CvFileStorage* fs, CvFileNode* node )
+{
+    CV_FUNCNAME( "CvBoost::read" );
+
+    __BEGIN__;
+
+    CvSeqReader reader;
+    CvFileNode* trees_fnode;
+    CvMemStorage* storage;
+    int i, ntrees;
+
+    clear();
+    read_params( fs, node );
+
+    if( !data )
+        EXIT;
+
+    trees_fnode = cvGetFileNodeByName( fs, node, "trees" );
+    if( !trees_fnode || !CV_NODE_IS_SEQ(trees_fnode->tag) )
+        CV_ERROR( CV_StsParseError, "<trees> tag is missing" );
+
+    cvStartReadSeq( trees_fnode->data.seq, &reader );
+    ntrees = trees_fnode->data.seq->total;
+
+    if( ntrees != params.weak_count )
+        CV_ERROR( CV_StsUnmatchedSizes,
+        "The number of trees stored does not match <ntrees> tag value" );
+
+    CV_CALL( storage = cvCreateMemStorage() );
+    weak = cvCreateSeq( 0, sizeof(CvSeq), sizeof(CvBoostTree*), storage );
+
+    for( i = 0; i < ntrees; i++ )
+    {
+        CvBoostTree* tree = new CvBoostTree();
+        CV_CALL(tree->read( fs, (CvFileNode*)reader.ptr, this, data ));
+        CV_NEXT_SEQ_ELEM( reader.seq->elem_size, reader );
+        cvSeqPush( weak, &tree );
+    }
+    get_active_vars();
+
+    __END__;
+}
+
+
+void
+CvBoost::write( CvFileStorage* fs, const char* name ) const
+{
+    CV_FUNCNAME( "CvBoost::write" );
+
+    __BEGIN__;
+
+    CvSeqReader reader;
+    int i;
+
+    cvStartWriteStruct( fs, name, CV_NODE_MAP, CV_TYPE_NAME_ML_BOOSTING );
+
+    if( !weak )
+        CV_ERROR( CV_StsBadArg, "The classifier has not been trained yet" );
+
+    write_params( fs );
+    cvStartWriteStruct( fs, "trees", CV_NODE_SEQ );
+
+    cvStartReadSeq( weak, &reader );
+
+    for( i = 0; i < weak->total; i++ )
+    {
+        CvBoostTree* tree;
+        CV_READ_SEQ_ELEM( tree, reader );
+        cvStartWriteStruct( fs, 0, CV_NODE_MAP );
+        tree->write( fs );
+        cvEndWriteStruct( fs );
+    }
+
+    cvEndWriteStruct( fs );
+    cvEndWriteStruct( fs );
+
+    __END__;
+}
+
+
+CvMat*
+CvBoost::get_weights()
+{
+    return weights;
+}
+
+
+CvMat*
+CvBoost::get_subtree_weights()
+{
+    return subtree_weights;
+}
+
+
+CvMat*
+CvBoost::get_weak_response()
+{
+    return weak_eval;
+}
+
+
+const CvBoostParams&
+CvBoost::get_params() const
+{
+    return params;
+}
+
+CvSeq* CvBoost::get_weak_predictors()
+{
+    return weak;
+}
+
+const CvDTreeTrainData* CvBoost::get_data() const
+{
+    return data;
+}
+
+using namespace cv;
+
+CvBoost::CvBoost( const Mat& _train_data, int _tflag,
+               const Mat& _responses, const Mat& _var_idx,
+               const Mat& _sample_idx, const Mat& _var_type,
+               const Mat& _missing_mask,
+               CvBoostParams _params )
+{
+    weak = 0;
+    data = 0;
+    default_model_name = "my_boost_tree";
+    active_vars = active_vars_abs = orig_response = sum_response = weak_eval =
+        subsample_mask = weights = subtree_weights = 0;
+
+    train( _train_data, _tflag, _responses, _var_idx, _sample_idx,
+          _var_type, _missing_mask, _params );
+}
+
+
+bool
+CvBoost::train( const Mat& _train_data, int _tflag,
+               const Mat& _responses, const Mat& _var_idx,
+               const Mat& _sample_idx, const Mat& _var_type,
+               const Mat& _missing_mask,
+               CvBoostParams _params, bool _update )
+{
+    train_data_hdr = _train_data;
+    train_data_mat = _train_data;
+    responses_hdr = _responses;
+    responses_mat = _responses;
+
+    CvMat vidx = _var_idx, sidx = _sample_idx, vtype = _var_type, mmask = _missing_mask;
+
+    return train(&train_data_hdr, _tflag, &responses_hdr, vidx.data.ptr ? &vidx : 0,
+          sidx.data.ptr ? &sidx : 0, vtype.data.ptr ? &vtype : 0,
+          mmask.data.ptr ? &mmask : 0, _params, _update);
+}
+
+float
+CvBoost::predict( const Mat& _sample, const Mat& _missing,
+                  const Range& slice, bool raw_mode, bool return_sum ) const
+{
+    CvMat sample = _sample, mmask = _missing;
+    /*if( weak_responses )
+    {
+        int weak_count = cvSliceLength( slice, weak );
+        if( weak_count >= weak->total )
+        {
+            weak_count = weak->total;
+            slice.start_index = 0;
+        }
+
+        if( !(weak_responses->data && weak_responses->type() == CV_32FC1 &&
+              (weak_responses->cols == 1 || weak_responses->rows == 1) &&
+              weak_responses->cols + weak_responses->rows - 1 == weak_count) )
+            weak_responses->create(weak_count, 1, CV_32FC1);
+        pwr = &(wr = *weak_responses);
+    }*/
+    return predict(&sample, _missing.empty() ? 0 : &mmask, 0,
+                   slice == Range::all() ? CV_WHOLE_SEQ : cvSlice(slice.start, slice.end),
+                   raw_mode, return_sum);
+}
+
+/* End of file. */
diff --git a/openbr/core/old_ml_data.cpp b/openbr/core/old_ml_data.cpp
new file mode 100644
index 0000000..d221dcb
--- /dev/null
+++ b/openbr/core/old_ml_data.cpp
@@ -0,0 +1,792 @@
+/*M///////////////////////////////////////////////////////////////////////////////////////
+//
+//  IMPORTANT: READ BEFORE DOWNLOADING, COPYING, INSTALLING OR USING.
+//
+//  By downloading, copying, installing or using the software you agree to this license.
+//  If you do not agree to this license, do not download, install,
+//  copy or use the software.
+//
+//
+//                        Intel License Agreement
+//
+// Copyright (C) 2000, Intel Corporation, all rights reserved.
+// Third party copyrights are property of their respective owners.
+//
+// Redistribution and use in source and binary forms, with or without modification,
+// are permitted provided that the following conditions are met:
+//
+//   * Redistribution's of source code must retain the above copyright notice,
+//     this list of conditions and the following disclaimer.
+//
+//   * Redistribution's in binary form must reproduce the above copyright notice,
+//     this list of conditions and the following disclaimer in the documentation
+//     and/or other materials provided with the distribution.
+//
+//   * The name of Intel Corporation may not be used to endorse or promote products
+//     derived from this software without specific prior written permission.
+//
+// This software is provided by the copyright holders and contributors "as is" and
+// any express or implied warranties, including, but not limited to, the implied
+// warranties of merchantability and fitness for a particular purpose are disclaimed.
+// In no event shall the Intel Corporation or contributors be liable for any direct,
+// indirect, incidental, special, exemplary, or consequential damages
+// (including, but not limited to, procurement of substitute goods or services;
+// loss of use, data, or profits; or business interruption) however caused
+// and on any theory of liability, whether in contract, strict liability,
+// or tort (including negligence or otherwise) arising in any way out of
+// the use of this software, even if advised of the possibility of such damage.
+//
+//M*/
+
+#include "old_ml_precomp.hpp"
+#include <ctype.h>
+
+#define MISS_VAL    FLT_MAX
+#define CV_VAR_MISS    0
+
+CvTrainTestSplit::CvTrainTestSplit()
+{
+    train_sample_part_mode = CV_COUNT;
+    train_sample_part.count = -1;
+    mix = false;
+}
+
+CvTrainTestSplit::CvTrainTestSplit( int _train_sample_count, bool _mix )
+{
+    train_sample_part_mode = CV_COUNT;
+    train_sample_part.count = _train_sample_count;
+    mix = _mix;
+}
+
+CvTrainTestSplit::CvTrainTestSplit( float _train_sample_portion, bool _mix )
+{
+    train_sample_part_mode = CV_PORTION;
+    train_sample_part.portion = _train_sample_portion;
+    mix = _mix;
+}
+
+////////////////
+
+CvMLData::CvMLData()
+{
+    values = missing = var_types = var_idx_mask = response_out = var_idx_out = var_types_out = 0;
+    train_sample_idx = test_sample_idx = 0;
+    header_lines_number = 0;
+    sample_idx = 0;
+    response_idx = -1;
+
+    train_sample_count = -1;
+
+    delimiter = ',';
+    miss_ch = '?';
+    //flt_separator = '.';
+
+    rng = &cv::theRNG();
+}
+
+CvMLData::~CvMLData()
+{
+    clear();
+}
+
+void CvMLData::free_train_test_idx()
+{
+    cvReleaseMat( &train_sample_idx );
+    cvReleaseMat( &test_sample_idx );
+    sample_idx = 0;
+}
+
+void CvMLData::clear()
+{
+    class_map.clear();
+
+    cvReleaseMat( &values );
+    cvReleaseMat( &missing );
+    cvReleaseMat( &var_types );
+    cvReleaseMat( &var_idx_mask );
+
+    cvReleaseMat( &response_out );
+    cvReleaseMat( &var_idx_out );
+    cvReleaseMat( &var_types_out );
+
+    free_train_test_idx();
+
+    total_class_count = 0;
+
+    response_idx = -1;
+
+    train_sample_count = -1;
+}
+
+
+void CvMLData::set_header_lines_number( int idx )
+{
+    header_lines_number = std::max(0, idx);
+}
+
+int CvMLData::get_header_lines_number() const
+{
+    return header_lines_number;
+}
+
+static char *fgets_chomp(char *str, int n, FILE *stream)
+{
+    char *head = fgets(str, n, stream);
+    if( head )
+    {
+        for(char *tail = head + strlen(head) - 1; tail >= head; --tail)
+        {
+            if( *tail != '\r'  && *tail != '\n' )
+                break;
+            *tail = '\0';
+        }
+    }
+    return head;
+}
+
+
+int CvMLData::read_csv(const char* filename)
+{
+    const int M = 1000000;
+    const char str_delimiter[3] = { ' ', delimiter, '\0' };
+    FILE* file = 0;
+    CvMemStorage* storage;
+    CvSeq* seq;
+    char *ptr;
+    float* el_ptr;
+    CvSeqReader reader;
+    int cols_count = 0;
+    uchar *var_types_ptr = 0;
+
+    clear();
+
+    file = fopen( filename, "rt" );
+
+    if( !file )
+        return -1;
+
+    std::vector<char> _buf(M);
+    char* buf = &_buf[0];
+
+    // skip header lines
+    for( int i = 0; i < header_lines_number; i++ )
+    {
+        if( fgets( buf, M, file ) == 0 )
+        {
+            fclose(file);
+            return -1;
+        }
+    }
+
+    // read the first data line and determine the number of variables
+    if( !fgets_chomp( buf, M, file ))
+    {
+        fclose(file);
+        return -1;
+    }
+
+    ptr = buf;
+    while( *ptr == ' ' )
+        ptr++;
+    for( ; *ptr != '\0'; )
+    {
+        if(*ptr == delimiter || *ptr == ' ')
+        {
+            cols_count++;
+            ptr++;
+            while( *ptr == ' ' ) ptr++;
+        }
+        else
+            ptr++;
+    }
+
+    cols_count++;
+
+    if ( cols_count == 0)
+    {
+        fclose(file);
+        return -1;
+    }
+
+    // create temporary memory storage to store the whole database
+    el_ptr = new float[cols_count];
+    storage = cvCreateMemStorage();
+    seq = cvCreateSeq( 0, sizeof(*seq), cols_count*sizeof(float), storage );
+
+    var_types = cvCreateMat( 1, cols_count, CV_8U );
+    cvZero( var_types );
+    var_types_ptr = var_types->data.ptr;
+
+    for(;;)
+    {
+        char *token = NULL;
+        int type;
+        token = strtok(buf, str_delimiter);
+        if (!token)
+            break;
+        for (int i = 0; i < cols_count-1; i++)
+        {
+            str_to_flt_elem( token, el_ptr[i], type);
+            var_types_ptr[i] |= type;
+            token = strtok(NULL, str_delimiter);
+            if (!token)
+            {
+                fclose(file);
+                delete [] el_ptr;
+                return -1;
+            }
+        }
+        str_to_flt_elem( token, el_ptr[cols_count-1], type);
+        var_types_ptr[cols_count-1] |= type;
+        cvSeqPush( seq, el_ptr );
+        if( !fgets_chomp( buf, M, file ) )
+            break;
+    }
+    fclose(file);
+
+    values = cvCreateMat( seq->total, cols_count, CV_32FC1 );
+    missing = cvCreateMat( seq->total, cols_count, CV_8U );
+    var_idx_mask = cvCreateMat( 1, values->cols, CV_8UC1 );
+    cvSet( var_idx_mask, cvRealScalar(1) );
+    train_sample_count = seq->total;
+
+    cvStartReadSeq( seq, &reader );
+    for(int i = 0; i < seq->total; i++ )
+    {
+        const float* sdata = (float*)reader.ptr;
+        float* ddata = values->data.fl + cols_count*i;
+        uchar* dm = missing->data.ptr + cols_count*i;
+
+        for( int j = 0; j < cols_count; j++ )
+        {
+            ddata[j] = sdata[j];
+            dm[j] = ( fabs( MISS_VAL - sdata[j] ) <= FLT_EPSILON );
+        }
+        CV_NEXT_SEQ_ELEM( seq->elem_size, reader );
+    }
+
+    if ( cvNorm( missing, 0, CV_L1 ) <= FLT_EPSILON )
+        cvReleaseMat( &missing );
+
+    cvReleaseMemStorage( &storage );
+    delete []el_ptr;
+    return 0;
+}
+
+const CvMat* CvMLData::get_values() const
+{
+    return values;
+}
+
+const CvMat* CvMLData::get_missing() const
+{
+    CV_FUNCNAME( "CvMLData::get_missing" );
+    __BEGIN__;
+
+    if ( !values )
+        CV_ERROR( CV_StsInternal, "data is empty" );
+
+    __END__;
+
+    return missing;
+}
+
+const std::map<cv::String, int>& CvMLData::get_class_labels_map() const
+{
+    return class_map;
+}
+
+void CvMLData::str_to_flt_elem( const char* token, float& flt_elem, int& type)
+{
+
+    char* stopstring = NULL;
+    flt_elem = (float)strtod( token, &stopstring );
+    assert( stopstring );
+    type = CV_VAR_ORDERED;
+    if ( *stopstring == miss_ch && strlen(stopstring) == 1 ) // missed value
+    {
+        flt_elem = MISS_VAL;
+        type = CV_VAR_MISS;
+    }
+    else
+    {
+        if ( (*stopstring != 0) && (*stopstring != '\n') && (strcmp(stopstring, "\r\n") != 0) ) // class label
+        {
+            int idx = class_map[token];
+            if ( idx == 0)
+            {
+                total_class_count++;
+                idx = total_class_count;
+                class_map[token] = idx;
+            }
+            flt_elem = (float)idx;
+            type = CV_VAR_CATEGORICAL;
+        }
+    }
+}
+
+void CvMLData::set_delimiter(char ch)
+{
+    CV_FUNCNAME( "CvMLData::set_delimited" );
+    __BEGIN__;
+
+    if (ch == miss_ch /*|| ch == flt_separator*/)
+        CV_ERROR(CV_StsBadArg, "delimited, miss_character and flt_separator must be different");
+
+    delimiter = ch;
+
+    __END__;
+}
+
+char CvMLData::get_delimiter() const
+{
+    return delimiter;
+}
+
+void CvMLData::set_miss_ch(char ch)
+{
+    CV_FUNCNAME( "CvMLData::set_miss_ch" );
+    __BEGIN__;
+
+    if (ch == delimiter/* || ch == flt_separator*/)
+        CV_ERROR(CV_StsBadArg, "delimited, miss_character and flt_separator must be different");
+
+    miss_ch = ch;
+
+    __END__;
+}
+
+char CvMLData::get_miss_ch() const
+{
+    return miss_ch;
+}
+
+void CvMLData::set_response_idx( int idx )
+{
+    CV_FUNCNAME( "CvMLData::set_response_idx" );
+    __BEGIN__;
+
+    if ( !values )
+        CV_ERROR( CV_StsInternal, "data is empty" );
+
+    if ( idx >= values->cols)
+        CV_ERROR( CV_StsBadArg, "idx value is not correct" );
+
+    if ( response_idx >= 0 )
+        chahge_var_idx( response_idx, true );
+    if ( idx >= 0 )
+        chahge_var_idx( idx, false );
+    response_idx = idx;
+
+    __END__;
+}
+
+int CvMLData::get_response_idx() const
+{
+    CV_FUNCNAME( "CvMLData::get_response_idx" );
+    __BEGIN__;
+
+    if ( !values )
+        CV_ERROR( CV_StsInternal, "data is empty" );
+     __END__;
+    return response_idx;
+}
+
+void CvMLData::change_var_type( int var_idx, int type )
+{
+    CV_FUNCNAME( "CvMLData::change_var_type" );
+    __BEGIN__;
+
+    int var_count = 0;
+
+    if ( !values )
+        CV_ERROR( CV_StsInternal, "data is empty" );
+
+     var_count = values->cols;
+
+    if ( var_idx < 0 || var_idx >= var_count)
+        CV_ERROR( CV_StsBadArg, "var_idx is not correct" );
+
+    if ( type != CV_VAR_ORDERED && type != CV_VAR_CATEGORICAL)
+         CV_ERROR( CV_StsBadArg, "type is not correct" );
+
+    assert( var_types );
+    if ( var_types->data.ptr[var_idx] == CV_VAR_CATEGORICAL && type == CV_VAR_ORDERED)
+        CV_ERROR( CV_StsBadArg, "it`s impossible to assign CV_VAR_ORDERED type to categorical variable" );
+    var_types->data.ptr[var_idx] = (uchar)type;
+
+    __END__;
+
+    return;
+}
+
+void CvMLData::set_var_types( const char* str )
+{
+    CV_FUNCNAME( "CvMLData::set_var_types" );
+    __BEGIN__;
+
+    const char* ord = 0, *cat = 0;
+    int var_count = 0, set_var_type_count = 0;
+    if ( !values )
+        CV_ERROR( CV_StsInternal, "data is empty" );
+
+    var_count = values->cols;
+
+    assert( var_types );
+
+    ord = strstr( str, "ord" );
+    cat = strstr( str, "cat" );
+    if ( !ord && !cat )
+        CV_ERROR( CV_StsBadArg, "types string is not correct" );
+
+    if ( !ord && strlen(cat) == 3 ) // str == "cat"
+    {
+        cvSet( var_types, cvScalarAll(CV_VAR_CATEGORICAL) );
+        return;
+    }
+
+    if ( !cat && strlen(ord) == 3 ) // str == "ord"
+    {
+        cvSet( var_types, cvScalarAll(CV_VAR_ORDERED) );
+        return;
+    }
+
+    if ( ord ) // parse ord str
+    {
+        char* stopstring = NULL;
+        if ( ord[3] != '[')
+            CV_ERROR( CV_StsBadArg, "types string is not correct" );
+
+        ord += 4; // pass "ord["
+        do
+        {
+            int b1 = (int)strtod( ord, &stopstring );
+            if ( *stopstring == 0 || (*stopstring != ',' && *stopstring != ']' && *stopstring != '-') )
+                CV_ERROR( CV_StsBadArg, "types string is not correct" );
+            ord = stopstring + 1;
+            if ( (stopstring[0] == ',') || (stopstring[0] == ']'))
+            {
+                if ( var_types->data.ptr[b1] == CV_VAR_CATEGORICAL)
+                    CV_ERROR( CV_StsBadArg, "it`s impossible to assign CV_VAR_ORDERED type to categorical variable" );
+                var_types->data.ptr[b1] = CV_VAR_ORDERED;
+                set_var_type_count++;
+            }
+            else
+            {
+                if ( stopstring[0] == '-')
+                {
+                    int b2 = (int)strtod( ord, &stopstring);
+                    if ( (*stopstring == 0) || (*stopstring != ',' && *stopstring != ']') )
+                        CV_ERROR( CV_StsBadArg, "types string is not correct" );
+                    ord = stopstring + 1;
+                    for (int i = b1; i <= b2; i++)
+                    {
+                        if ( var_types->data.ptr[i] == CV_VAR_CATEGORICAL)
+                            CV_ERROR( CV_StsBadArg, "it`s impossible to assign CV_VAR_ORDERED type to categorical variable" );
+                        var_types->data.ptr[i] = CV_VAR_ORDERED;
+                    }
+                    set_var_type_count += b2 - b1 + 1;
+                }
+                else
+                    CV_ERROR( CV_StsBadArg, "types string is not correct" );
+
+            }
+        }
+        while (*stopstring != ']');
+
+        if ( stopstring[1] != '\0' && stopstring[1] != ',')
+            CV_ERROR( CV_StsBadArg, "types string is not correct" );
+    }
+
+    if ( cat ) // parse cat str
+    {
+        char* stopstring = NULL;
+        if ( cat[3] != '[')
+            CV_ERROR( CV_StsBadArg, "types string is not correct" );
+
+        cat += 4; // pass "cat["
+        do
+        {
+            int b1 = (int)strtod( cat, &stopstring );
+            if ( *stopstring == 0 || (*stopstring != ',' && *stopstring != ']' && *stopstring != '-') )
+                CV_ERROR( CV_StsBadArg, "types string is not correct" );
+            cat = stopstring + 1;
+            if ( (stopstring[0] == ',') || (stopstring[0] == ']'))
+            {
+                var_types->data.ptr[b1] = CV_VAR_CATEGORICAL;
+                set_var_type_count++;
+            }
+            else
+            {
+                if ( stopstring[0] == '-')
+                {
+                    int b2 = (int)strtod( cat, &stopstring);
+                    if ( (*stopstring == 0) || (*stopstring != ',' && *stopstring != ']') )
+                        CV_ERROR( CV_StsBadArg, "types string is not correct" );
+                    cat = stopstring + 1;
+                    for (int i = b1; i <= b2; i++)
+                        var_types->data.ptr[i] = CV_VAR_CATEGORICAL;
+                    set_var_type_count += b2 - b1 + 1;
+                }
+                else
+                    CV_ERROR( CV_StsBadArg, "types string is not correct" );
+
+            }
+        }
+        while (*stopstring != ']');
+
+        if ( stopstring[1] != '\0' && stopstring[1] != ',')
+            CV_ERROR( CV_StsBadArg, "types string is not correct" );
+    }
+
+    if (set_var_type_count != var_count)
+        CV_ERROR( CV_StsBadArg, "types string is not correct" );
+
+     __END__;
+}
+
+const CvMat* CvMLData::get_var_types()
+{
+    CV_FUNCNAME( "CvMLData::get_var_types" );
+    __BEGIN__;
+
+    uchar *var_types_out_ptr = 0;
+    int avcount, vt_size;
+    if ( !values )
+        CV_ERROR( CV_StsInternal, "data is empty" );
+
+    assert( var_idx_mask );
+
+    avcount = cvFloor( cvNorm( var_idx_mask, 0, CV_L1 ) );
+    vt_size = avcount + (response_idx >= 0);
+
+    if ( avcount == values->cols || (avcount == values->cols-1 && response_idx == values->cols-1) )
+        return var_types;
+
+    if ( !var_types_out || ( var_types_out && var_types_out->cols != vt_size ) )
+    {
+        cvReleaseMat( &var_types_out );
+        var_types_out = cvCreateMat( 1, vt_size, CV_8UC1 );
+    }
+
+    var_types_out_ptr = var_types_out->data.ptr;
+    for( int i = 0; i < var_types->cols; i++)
+    {
+        if (i == response_idx || !var_idx_mask->data.ptr[i]) continue;
+        *var_types_out_ptr = var_types->data.ptr[i];
+        var_types_out_ptr++;
+    }
+    if ( response_idx >= 0 )
+        *var_types_out_ptr = var_types->data.ptr[response_idx];
+
+    __END__;
+
+    return var_types_out;
+}
+
+int CvMLData::get_var_type( int var_idx ) const
+{
+    return var_types->data.ptr[var_idx];
+}
+
+const CvMat* CvMLData::get_responses()
+{
+    CV_FUNCNAME( "CvMLData::get_responses_ptr" );
+    __BEGIN__;
+
+    int var_count = 0;
+
+    if ( !values )
+        CV_ERROR( CV_StsInternal, "data is empty" );
+    var_count = values->cols;
+
+    if ( response_idx < 0 || response_idx >= var_count )
+       return 0;
+    if ( !response_out )
+        response_out = cvCreateMatHeader( values->rows, 1, CV_32FC1 );
+    else
+        cvInitMatHeader( response_out, values->rows, 1, CV_32FC1);
+    cvGetCol( values, response_out, response_idx );
+
+    __END__;
+
+    return response_out;
+}
+
+void CvMLData::set_train_test_split( const CvTrainTestSplit * spl)
+{
+    CV_FUNCNAME( "CvMLData::set_division" );
+    __BEGIN__;
+
+    int sample_count = 0;
+
+    if ( !values )
+        CV_ERROR( CV_StsInternal, "data is empty" );
+
+    sample_count = values->rows;
+
+    float train_sample_portion;
+
+    if (spl->train_sample_part_mode == CV_COUNT)
+    {
+        train_sample_count = spl->train_sample_part.count;
+        if (train_sample_count > sample_count)
+            CV_ERROR( CV_StsBadArg, "train samples count is not correct" );
+        train_sample_count = train_sample_count<=0 ? sample_count : train_sample_count;
+    }
+    else // dtype.train_sample_part_mode == CV_PORTION
+    {
+        train_sample_portion = spl->train_sample_part.portion;
+        if ( train_sample_portion > 1)
+            CV_ERROR( CV_StsBadArg, "train samples count is not correct" );
+        train_sample_portion = train_sample_portion <= FLT_EPSILON ||
+            1 - train_sample_portion <= FLT_EPSILON ? 1 : train_sample_portion;
+        train_sample_count = std::max(1, cvFloor( train_sample_portion * sample_count ));
+    }
+
+    if ( train_sample_count == sample_count )
+    {
+        free_train_test_idx();
+        return;
+    }
+
+    if ( train_sample_idx && train_sample_idx->cols != train_sample_count )
+        free_train_test_idx();
+
+    if ( !sample_idx)
+    {
+        int test_sample_count = sample_count- train_sample_count;
+        sample_idx = (int*)cvAlloc( sample_count * sizeof(sample_idx[0]) );
+        for (int i = 0; i < sample_count; i++ )
+            sample_idx[i] = i;
+        train_sample_idx = cvCreateMatHeader( 1, train_sample_count, CV_32SC1 );
+        *train_sample_idx = cvMat( 1, train_sample_count, CV_32SC1, &sample_idx[0] );
+
+        CV_Assert(test_sample_count > 0);
+        test_sample_idx = cvCreateMatHeader( 1, test_sample_count, CV_32SC1 );
+        *test_sample_idx = cvMat( 1, test_sample_count, CV_32SC1, &sample_idx[train_sample_count] );
+    }
+
+    mix = spl->mix;
+    if ( mix )
+        mix_train_and_test_idx();
+
+    __END__;
+}
+
+const CvMat* CvMLData::get_train_sample_idx() const
+{
+    CV_FUNCNAME( "CvMLData::get_train_sample_idx" );
+    __BEGIN__;
+
+    if ( !values )
+        CV_ERROR( CV_StsInternal, "data is empty" );
+    __END__;
+
+    return train_sample_idx;
+}
+
+const CvMat* CvMLData::get_test_sample_idx() const
+{
+    CV_FUNCNAME( "CvMLData::get_test_sample_idx" );
+    __BEGIN__;
+
+    if ( !values )
+        CV_ERROR( CV_StsInternal, "data is empty" );
+    __END__;
+
+    return test_sample_idx;
+}
+
+void CvMLData::mix_train_and_test_idx()
+{
+    CV_FUNCNAME( "CvMLData::mix_train_and_test_idx" );
+    __BEGIN__;
+
+    if ( !values )
+        CV_ERROR( CV_StsInternal, "data is empty" );
+    __END__;
+
+    if ( !sample_idx)
+        return;
+
+    if ( train_sample_count > 0 && train_sample_count < values->rows )
+    {
+        int n = values->rows;
+        for (int i = 0; i < n; i++)
+        {
+            int a = (*rng)(n);
+            int b = (*rng)(n);
+            int t;
+            CV_SWAP( sample_idx[a], sample_idx[b], t );
+        }
+    }
+}
+
+const CvMat* CvMLData::get_var_idx()
+{
+     CV_FUNCNAME( "CvMLData::get_var_idx" );
+    __BEGIN__;
+
+    int avcount = 0;
+
+    if ( !values )
+        CV_ERROR( CV_StsInternal, "data is empty" );
+
+    assert( var_idx_mask );
+
+    avcount = cvFloor( cvNorm( var_idx_mask, 0, CV_L1 ) );
+    int* vidx;
+
+    if ( avcount == values->cols )
+        return 0;
+
+    if ( !var_idx_out || ( var_idx_out && var_idx_out->cols != avcount ) )
+    {
+        cvReleaseMat( &var_idx_out );
+        var_idx_out = cvCreateMat( 1, avcount, CV_32SC1);
+        if ( response_idx >=0 )
+            var_idx_mask->data.ptr[response_idx] = 0;
+    }
+
+    vidx = var_idx_out->data.i;
+
+    for(int i = 0; i < var_idx_mask->cols; i++)
+        if ( var_idx_mask->data.ptr[i] )
+        {
+            *vidx = i;
+            vidx++;
+        }
+
+    __END__;
+
+    return var_idx_out;
+}
+
+void CvMLData::chahge_var_idx( int vi, bool state )
+{
+    change_var_idx( vi, state );
+}
+
+void CvMLData::change_var_idx( int vi, bool state )
+{
+     CV_FUNCNAME( "CvMLData::change_var_idx" );
+    __BEGIN__;
+
+    int var_count = 0;
+
+    if ( !values )
+        CV_ERROR( CV_StsInternal, "data is empty" );
+
+    var_count = values->cols;
+
+    if ( vi < 0 || vi >= var_count)
+        CV_ERROR( CV_StsBadArg, "variable index is not correct" );
+
+    assert( var_idx_mask );
+    var_idx_mask->data.ptr[vi] = state;
+
+    __END__;
+}
+
+/* End of file. */
diff --git a/openbr/core/old_ml_inner_functions.cpp b/openbr/core/old_ml_inner_functions.cpp
new file mode 100644
index 0000000..5858a40
--- /dev/null
+++ b/openbr/core/old_ml_inner_functions.cpp
@@ -0,0 +1,1688 @@
+/*M///////////////////////////////////////////////////////////////////////////////////////
+//
+//  IMPORTANT: READ BEFORE DOWNLOADING, COPYING, INSTALLING OR USING.
+//
+//  By downloading, copying, installing or using the software you agree to this license.
+//  If you do not agree to this license, do not download, install,
+//  copy or use the software.
+//
+//
+//                        Intel License Agreement
+//
+// Copyright (C) 2000, Intel Corporation, all rights reserved.
+// Third party copyrights are property of their respective owners.
+//
+// Redistribution and use in source and binary forms, with or without modification,
+// are permitted provided that the following conditions are met:
+//
+//   * Redistribution's of source code must retain the above copyright notice,
+//     this list of conditions and the following disclaimer.
+//
+//   * Redistribution's in binary form must reproduce the above copyright notice,
+//     this list of conditions and the following disclaimer in the documentation
+//     and/or other materials provided with the distribution.
+//
+//   * The name of Intel Corporation may not be used to endorse or promote products
+//     derived from this software without specific prior written permission.
+//
+// This software is provided by the copyright holders and contributors "as is" and
+// any express or implied warranties, including, but not limited to, the implied
+// warranties of merchantability and fitness for a particular purpose are disclaimed.
+// In no event shall the Intel Corporation or contributors be liable for any direct,
+// indirect, incidental, special, exemplary, or consequential damages
+// (including, but not limited to, procurement of substitute goods or services;
+// loss of use, data, or profits; or business interruption) however caused
+// and on any theory of liability, whether in contract, strict liability,
+// or tort (including negligence or otherwise) arising in any way out of
+// the use of this software, even if advised of the possibility of such damage.
+//
+//M*/
+
+#include "old_ml_precomp.hpp"
+
+
+CvStatModel::CvStatModel()
+{
+    default_model_name = "my_stat_model";
+}
+
+
+CvStatModel::~CvStatModel()
+{
+    clear();
+}
+
+
+void CvStatModel::clear()
+{
+}
+
+
+void CvStatModel::save( const char* filename, const char* name ) const
+{
+    CvFileStorage* fs = 0;
+
+    CV_FUNCNAME( "CvStatModel::save" );
+
+    __BEGIN__;
+
+    CV_CALL( fs = cvOpenFileStorage( filename, 0, CV_STORAGE_WRITE ));
+    if( !fs )
+        CV_ERROR( CV_StsError, "Could not open the file storage. Check the path and permissions" );
+
+    write( fs, name ? name : default_model_name );
+
+    __END__;
+
+    cvReleaseFileStorage( &fs );
+}
+
+
+void CvStatModel::load( const char* filename, const char* name )
+{
+    CvFileStorage* fs = 0;
+
+    CV_FUNCNAME( "CvAlgorithm::load" );
+
+    __BEGIN__;
+
+    CvFileNode* model_node = 0;
+
+    CV_CALL( fs = cvOpenFileStorage( filename, 0, CV_STORAGE_READ ));
+    if( !fs )
+        EXIT;
+
+    if( name )
+        model_node = cvGetFileNodeByName( fs, 0, name );
+    else
+    {
+        CvFileNode* root = cvGetRootFileNode( fs );
+        if( root->data.seq->total > 0 )
+            model_node = (CvFileNode*)cvGetSeqElem( root->data.seq, 0 );
+    }
+
+    read( fs, model_node );
+
+    __END__;
+
+    cvReleaseFileStorage( &fs );
+}
+
+
+void CvStatModel::write( CvFileStorage*, const char* ) const
+{
+    OPENCV_ERROR( CV_StsNotImplemented, "CvStatModel::write", "" );
+}
+
+void CvStatModel::read( CvFileStorage*, CvFileNode* )
+{
+    OPENCV_ERROR( CV_StsNotImplemented, "CvStatModel::read", "" );
+}
+
+CvMat* icvGenerateRandomClusterCenters ( int seed, const CvMat* data,
+                                         int num_of_clusters, CvMat* _centers )
+{
+    CvMat* centers = _centers;
+
+    CV_FUNCNAME("icvGenerateRandomClusterCenters");
+    __BEGIN__;
+
+    CvRNG rng;
+    CvMat data_comp, centers_comp;
+    CvPoint minLoc, maxLoc; // Not used, just for function "cvMinMaxLoc"
+    double minVal, maxVal;
+    int i;
+    int dim = data ? data->cols : 0;
+
+    if( ICV_IS_MAT_OF_TYPE(data, CV_32FC1) )
+    {
+        if( _centers && !ICV_IS_MAT_OF_TYPE (_centers, CV_32FC1) )
+        {
+            CV_ERROR(CV_StsBadArg,"");
+        }
+        else if( !_centers )
+            CV_CALL(centers = cvCreateMat (num_of_clusters, dim, CV_32FC1));
+    }
+    else if( ICV_IS_MAT_OF_TYPE(data, CV_64FC1) )
+    {
+        if( _centers && !ICV_IS_MAT_OF_TYPE (_centers, CV_64FC1) )
+        {
+            CV_ERROR(CV_StsBadArg,"");
+        }
+        else if( !_centers )
+            CV_CALL(centers = cvCreateMat (num_of_clusters, dim, CV_64FC1));
+    }
+    else
+        CV_ERROR (CV_StsBadArg,"");
+
+    if( num_of_clusters < 1 )
+        CV_ERROR (CV_StsBadArg,"");
+
+    rng = cvRNG(seed);
+    for (i = 0; i < dim; i++)
+    {
+        CV_CALL(cvGetCol (data, &data_comp, i));
+        CV_CALL(cvMinMaxLoc (&data_comp, &minVal, &maxVal, &minLoc, &maxLoc));
+        CV_CALL(cvGetCol (centers, &centers_comp, i));
+        CV_CALL(cvRandArr (&rng, &centers_comp, CV_RAND_UNI, cvScalarAll(minVal), cvScalarAll(maxVal)));
+    }
+
+    __END__;
+
+    if( (cvGetErrStatus () < 0) || (centers != _centers) )
+        cvReleaseMat (&centers);
+
+    return _centers ? _centers : centers;
+} // end of icvGenerateRandomClusterCenters
+
+static int CV_CDECL
+icvCmpIntegers( const void* a, const void* b )
+{
+    return *(const int*)a - *(const int*)b;
+}
+
+
+static int CV_CDECL
+icvCmpIntegersPtr( const void* _a, const void* _b )
+{
+    int a = **(const int**)_a;
+    int b = **(const int**)_b;
+    return (a < b ? -1 : 0)|(a > b);
+}
+
+
+static int icvCmpSparseVecElems( const void* a, const void* b )
+{
+    return ((CvSparseVecElem32f*)a)->idx - ((CvSparseVecElem32f*)b)->idx;
+}
+
+
+CvMat*
+cvPreprocessIndexArray( const CvMat* idx_arr, int data_arr_size, bool check_for_duplicates )
+{
+    CvMat* idx = 0;
+
+    CV_FUNCNAME( "cvPreprocessIndexArray" );
+
+    __BEGIN__;
+
+    int i, idx_total, idx_selected = 0, step, type, prev = INT_MIN, is_sorted = 1;
+    uchar* srcb = 0;
+    int* srci = 0;
+    int* dsti;
+
+    if( !CV_IS_MAT(idx_arr) )
+        CV_ERROR( CV_StsBadArg, "Invalid index array" );
+
+    if( idx_arr->rows != 1 && idx_arr->cols != 1 )
+        CV_ERROR( CV_StsBadSize, "the index array must be 1-dimensional" );
+
+    idx_total = idx_arr->rows + idx_arr->cols - 1;
+    srcb = idx_arr->data.ptr;
+    srci = idx_arr->data.i;
+
+    type = CV_MAT_TYPE(idx_arr->type);
+    step = CV_IS_MAT_CONT(idx_arr->type) ? 1 : idx_arr->step/CV_ELEM_SIZE(type);
+
+    switch( type )
+    {
+    case CV_8UC1:
+    case CV_8SC1:
+        // idx_arr is array of 1's and 0's -
+        // i.e. it is a mask of the selected components
+        if( idx_total != data_arr_size )
+            CV_ERROR( CV_StsUnmatchedSizes,
+            "Component mask should contain as many elements as the total number of input variables" );
+
+        for( i = 0; i < idx_total; i++ )
+            idx_selected += srcb[i*step] != 0;
+
+        if( idx_selected == 0 )
+            CV_ERROR( CV_StsOutOfRange, "No components/input_variables is selected!" );
+
+        break;
+    case CV_32SC1:
+        // idx_arr is array of integer indices of selected components
+        if( idx_total > data_arr_size )
+            CV_ERROR( CV_StsOutOfRange,
+            "index array may not contain more elements than the total number of input variables" );
+        idx_selected = idx_total;
+        // check if sorted already
+        for( i = 0; i < idx_total; i++ )
+        {
+            int val = srci[i*step];
+            if( val >= prev )
+            {
+                is_sorted = 0;
+                break;
+            }
+            prev = val;
+        }
+        break;
+    default:
+        CV_ERROR( CV_StsUnsupportedFormat, "Unsupported index array data type "
+                                           "(it should be 8uC1, 8sC1 or 32sC1)" );
+    }
+
+    CV_CALL( idx = cvCreateMat( 1, idx_selected, CV_32SC1 ));
+    dsti = idx->data.i;
+
+    if( type < CV_32SC1 )
+    {
+        for( i = 0; i < idx_total; i++ )
+            if( srcb[i*step] )
+                *dsti++ = i;
+    }
+    else
+    {
+        for( i = 0; i < idx_total; i++ )
+            dsti[i] = srci[i*step];
+
+        if( !is_sorted )
+            qsort( dsti, idx_total, sizeof(dsti[0]), icvCmpIntegers );
+
+        if( dsti[0] < 0 || dsti[idx_total-1] >= data_arr_size )
+            CV_ERROR( CV_StsOutOfRange, "the index array elements are out of range" );
+
+        if( check_for_duplicates )
+        {
+            for( i = 1; i < idx_total; i++ )
+                if( dsti[i] <= dsti[i-1] )
+                    CV_ERROR( CV_StsBadArg, "There are duplicated index array elements" );
+        }
+    }
+
+    __END__;
+
+    if( cvGetErrStatus() < 0 )
+        cvReleaseMat( &idx );
+
+    return idx;
+}
+
+
+CvMat*
+cvPreprocessVarType( const CvMat* var_type, const CvMat* var_idx,
+                     int var_count, int* response_type )
+{
+    CvMat* out_var_type = 0;
+    CV_FUNCNAME( "cvPreprocessVarType" );
+
+    if( response_type )
+        *response_type = -1;
+
+    __BEGIN__;
+
+    int i, tm_size, tm_step;
+    //int* map = 0;
+    const uchar* src;
+    uchar* dst;
+
+    if( !CV_IS_MAT(var_type) )
+        CV_ERROR( var_type ? CV_StsBadArg : CV_StsNullPtr, "Invalid or absent var_type array" );
+
+    if( var_type->rows != 1 && var_type->cols != 1 )
+        CV_ERROR( CV_StsBadSize, "var_type array must be 1-dimensional" );
+
+    if( !CV_IS_MASK_ARR(var_type))
+        CV_ERROR( CV_StsUnsupportedFormat, "type mask must be 8uC1 or 8sC1 array" );
+
+    tm_size = var_type->rows + var_type->cols - 1;
+    tm_step = var_type->rows == 1 ? 1 : var_type->step/CV_ELEM_SIZE(var_type->type);
+
+    if( /*tm_size != var_count &&*/ tm_size != var_count + 1 )
+        CV_ERROR( CV_StsBadArg,
+        "type mask must be of <input var count> + 1 size" );
+
+    if( response_type && tm_size > var_count )
+        *response_type = var_type->data.ptr[var_count*tm_step] != 0;
+
+    if( var_idx )
+    {
+        if( !CV_IS_MAT(var_idx) || CV_MAT_TYPE(var_idx->type) != CV_32SC1 ||
+            (var_idx->rows != 1 && var_idx->cols != 1) || !CV_IS_MAT_CONT(var_idx->type) )
+            CV_ERROR( CV_StsBadArg, "var index array should be continuous 1-dimensional integer vector" );
+        if( var_idx->rows + var_idx->cols - 1 > var_count )
+            CV_ERROR( CV_StsBadSize, "var index array is too large" );
+        //map = var_idx->data.i;
+        var_count = var_idx->rows + var_idx->cols - 1;
+    }
+
+    CV_CALL( out_var_type = cvCreateMat( 1, var_count, CV_8UC1 ));
+    src = var_type->data.ptr;
+    dst = out_var_type->data.ptr;
+
+    for( i = 0; i < var_count; i++ )
+    {
+        //int idx = map ? map[i] : i;
+        assert( (unsigned)/*idx*/i < (unsigned)tm_size );
+        dst[i] = (uchar)(src[/*idx*/i*tm_step] != 0);
+    }
+
+    __END__;
+
+    return out_var_type;
+}
+
+
+CvMat*
+cvPreprocessOrderedResponses( const CvMat* responses, const CvMat* sample_idx, int sample_all )
+{
+    CvMat* out_responses = 0;
+
+    CV_FUNCNAME( "cvPreprocessOrderedResponses" );
+
+    __BEGIN__;
+
+    int i, r_type, r_step;
+    const int* map = 0;
+    float* dst;
+    int sample_count = sample_all;
+
+    if( !CV_IS_MAT(responses) )
+        CV_ERROR( CV_StsBadArg, "Invalid response array" );
+
+    if( responses->rows != 1 && responses->cols != 1 )
+        CV_ERROR( CV_StsBadSize, "Response array must be 1-dimensional" );
+
+    if( responses->rows + responses->cols - 1 != sample_count )
+        CV_ERROR( CV_StsUnmatchedSizes,
+        "Response array must contain as many elements as the total number of samples" );
+
+    r_type = CV_MAT_TYPE(responses->type);
+    if( r_type != CV_32FC1 && r_type != CV_32SC1 )
+        CV_ERROR( CV_StsUnsupportedFormat, "Unsupported response type" );
+
+    r_step = responses->step ? responses->step / CV_ELEM_SIZE(responses->type) : 1;
+
+    if( r_type == CV_32FC1 && CV_IS_MAT_CONT(responses->type) && !sample_idx )
+    {
+        out_responses = cvCloneMat( responses );
+        EXIT;
+    }
+
+    if( sample_idx )
+    {
+        if( !CV_IS_MAT(sample_idx) || CV_MAT_TYPE(sample_idx->type) != CV_32SC1 ||
+            (sample_idx->rows != 1 && sample_idx->cols != 1) || !CV_IS_MAT_CONT(sample_idx->type) )
+            CV_ERROR( CV_StsBadArg, "sample index array should be continuous 1-dimensional integer vector" );
+        if( sample_idx->rows + sample_idx->cols - 1 > sample_count )
+            CV_ERROR( CV_StsBadSize, "sample index array is too large" );
+        map = sample_idx->data.i;
+        sample_count = sample_idx->rows + sample_idx->cols - 1;
+    }
+
+    CV_CALL( out_responses = cvCreateMat( 1, sample_count, CV_32FC1 ));
+
+    dst = out_responses->data.fl;
+    if( r_type == CV_32FC1 )
+    {
+        const float* src = responses->data.fl;
+        for( i = 0; i < sample_count; i++ )
+        {
+            int idx = map ? map[i] : i;
+            assert( (unsigned)idx < (unsigned)sample_all );
+            dst[i] = src[idx*r_step];
+        }
+    }
+    else
+    {
+        const int* src = responses->data.i;
+        for( i = 0; i < sample_count; i++ )
+        {
+            int idx = map ? map[i] : i;
+            assert( (unsigned)idx < (unsigned)sample_all );
+            dst[i] = (float)src[idx*r_step];
+        }
+    }
+
+    __END__;
+
+    return out_responses;
+}
+
+CvMat*
+cvPreprocessCategoricalResponses( const CvMat* responses,
+    const CvMat* sample_idx, int sample_all,
+    CvMat** out_response_map, CvMat** class_counts )
+{
+    CvMat* out_responses = 0;
+    int** response_ptr = 0;
+
+    CV_FUNCNAME( "cvPreprocessCategoricalResponses" );
+
+    if( out_response_map )
+        *out_response_map = 0;
+
+    if( class_counts )
+        *class_counts = 0;
+
+    __BEGIN__;
+
+    int i, r_type, r_step;
+    int cls_count = 1, prev_cls, prev_i;
+    const int* map = 0;
+    const int* srci;
+    const float* srcfl;
+    int* dst;
+    int* cls_map;
+    int* cls_counts = 0;
+    int sample_count = sample_all;
+
+    if( !CV_IS_MAT(responses) )
+        CV_ERROR( CV_StsBadArg, "Invalid response array" );
+
+    if( responses->rows != 1 && responses->cols != 1 )
+        CV_ERROR( CV_StsBadSize, "Response array must be 1-dimensional" );
+
+    if( responses->rows + responses->cols - 1 != sample_count )
+        CV_ERROR( CV_StsUnmatchedSizes,
+        "Response array must contain as many elements as the total number of samples" );
+
+    r_type = CV_MAT_TYPE(responses->type);
+    if( r_type != CV_32FC1 && r_type != CV_32SC1 )
+        CV_ERROR( CV_StsUnsupportedFormat, "Unsupported response type" );
+
+    r_step = responses->rows == 1 ? 1 : responses->step / CV_ELEM_SIZE(responses->type);
+
+    if( sample_idx )
+    {
+        if( !CV_IS_MAT(sample_idx) || CV_MAT_TYPE(sample_idx->type) != CV_32SC1 ||
+            (sample_idx->rows != 1 && sample_idx->cols != 1) || !CV_IS_MAT_CONT(sample_idx->type) )
+            CV_ERROR( CV_StsBadArg, "sample index array should be continuous 1-dimensional integer vector" );
+        if( sample_idx->rows + sample_idx->cols - 1 > sample_count )
+            CV_ERROR( CV_StsBadSize, "sample index array is too large" );
+        map = sample_idx->data.i;
+        sample_count = sample_idx->rows + sample_idx->cols - 1;
+    }
+
+    CV_CALL( out_responses = cvCreateMat( 1, sample_count, CV_32SC1 ));
+
+    if( !out_response_map )
+        CV_ERROR( CV_StsNullPtr, "out_response_map pointer is NULL" );
+
+    CV_CALL( response_ptr = (int**)cvAlloc( sample_count*sizeof(response_ptr[0])));
+
+    srci = responses->data.i;
+    srcfl = responses->data.fl;
+    dst = out_responses->data.i;
+
+    for( i = 0; i < sample_count; i++ )
+    {
+        int idx = map ? map[i] : i;
+        assert( (unsigned)idx < (unsigned)sample_all );
+        if( r_type == CV_32SC1 )
+            dst[i] = srci[idx*r_step];
+        else
+        {
+            float rf = srcfl[idx*r_step];
+            int ri = cvRound(rf);
+            if( ri != rf )
+            {
+                char buf[100];
+                sprintf( buf, "response #%d is not integral", idx );
+                CV_ERROR( CV_StsBadArg, buf );
+            }
+            dst[i] = ri;
+        }
+        response_ptr[i] = dst + i;
+    }
+
+    qsort( response_ptr, sample_count, sizeof(int*), icvCmpIntegersPtr );
+
+    // count the classes
+    for( i = 1; i < sample_count; i++ )
+        cls_count += *response_ptr[i] != *response_ptr[i-1];
+
+    if( cls_count < 2 )
+        CV_ERROR( CV_StsBadArg, "There is only a single class" );
+
+    CV_CALL( *out_response_map = cvCreateMat( 1, cls_count, CV_32SC1 ));
+
+    if( class_counts )
+    {
+        CV_CALL( *class_counts = cvCreateMat( 1, cls_count, CV_32SC1 ));
+        cls_counts = (*class_counts)->data.i;
+    }
+
+    // compact the class indices and build the map
+    prev_cls = ~*response_ptr[0];
+    cls_count = -1;
+    cls_map = (*out_response_map)->data.i;
+
+    for( i = 0, prev_i = -1; i < sample_count; i++ )
+    {
+        int cur_cls = *response_ptr[i];
+        if( cur_cls != prev_cls )
+        {
+            if( cls_counts && cls_count >= 0 )
+                cls_counts[cls_count] = i - prev_i;
+            cls_map[++cls_count] = prev_cls = cur_cls;
+            prev_i = i;
+        }
+        *response_ptr[i] = cls_count;
+    }
+
+    if( cls_counts )
+        cls_counts[cls_count] = i - prev_i;
+
+    __END__;
+
+    cvFree( &response_ptr );
+
+    return out_responses;
+}
+
+
+const float**
+cvGetTrainSamples( const CvMat* train_data, int tflag,
+                   const CvMat* var_idx, const CvMat* sample_idx,
+                   int* _var_count, int* _sample_count,
+                   bool always_copy_data )
+{
+    float** samples = 0;
+
+    CV_FUNCNAME( "cvGetTrainSamples" );
+
+    __BEGIN__;
+
+    int i, j, var_count, sample_count, s_step, v_step;
+    bool copy_data;
+    const float* data;
+    const int *s_idx, *v_idx;
+
+    if( !CV_IS_MAT(train_data) )
+        CV_ERROR( CV_StsBadArg, "Invalid or NULL training data matrix" );
+
+    var_count = var_idx ? var_idx->cols + var_idx->rows - 1 :
+                tflag == CV_ROW_SAMPLE ? train_data->cols : train_data->rows;
+    sample_count = sample_idx ? sample_idx->cols + sample_idx->rows - 1 :
+                   tflag == CV_ROW_SAMPLE ? train_data->rows : train_data->cols;
+
+    if( _var_count )
+        *_var_count = var_count;
+
+    if( _sample_count )
+        *_sample_count = sample_count;
+
+    copy_data = tflag != CV_ROW_SAMPLE || var_idx || always_copy_data;
+
+    CV_CALL( samples = (float**)cvAlloc(sample_count*sizeof(samples[0]) +
+                (copy_data ? 1 : 0)*var_count*sample_count*sizeof(samples[0][0])) );
+    data = train_data->data.fl;
+    s_step = train_data->step / sizeof(samples[0][0]);
+    v_step = 1;
+    s_idx = sample_idx ? sample_idx->data.i : 0;
+    v_idx = var_idx ? var_idx->data.i : 0;
+
+    if( !copy_data )
+    {
+        for( i = 0; i < sample_count; i++ )
+            samples[i] = (float*)(data + (s_idx ? s_idx[i] : i)*s_step);
+    }
+    else
+    {
+        samples[0] = (float*)(samples + sample_count);
+        if( tflag != CV_ROW_SAMPLE )
+            CV_SWAP( s_step, v_step, i );
+
+        for( i = 0; i < sample_count; i++ )
+        {
+            float* dst = samples[i] = samples[0] + i*var_count;
+            const float* src = data + (s_idx ? s_idx[i] : i)*s_step;
+
+            if( !v_idx )
+                for( j = 0; j < var_count; j++ )
+                    dst[j] = src[j*v_step];
+            else
+                for( j = 0; j < var_count; j++ )
+                    dst[j] = src[v_idx[j]*v_step];
+        }
+    }
+
+    __END__;
+
+    return (const float**)samples;
+}
+
+
+void
+cvCheckTrainData( const CvMat* train_data, int tflag,
+                  const CvMat* missing_mask,
+                  int* var_all, int* sample_all )
+{
+    CV_FUNCNAME( "cvCheckTrainData" );
+
+    if( var_all )
+        *var_all = 0;
+
+    if( sample_all )
+        *sample_all = 0;
+
+    __BEGIN__;
+
+    // check parameter types and sizes
+    if( !CV_IS_MAT(train_data) || CV_MAT_TYPE(train_data->type) != CV_32FC1 )
+        CV_ERROR( CV_StsBadArg, "train data must be floating-point matrix" );
+
+    if( missing_mask )
+    {
+        if( !CV_IS_MAT(missing_mask) || !CV_IS_MASK_ARR(missing_mask) ||
+            !CV_ARE_SIZES_EQ(train_data, missing_mask) )
+            CV_ERROR( CV_StsBadArg,
+            "missing value mask must be 8-bit matrix of the same size as training data" );
+    }
+
+    if( tflag != CV_ROW_SAMPLE && tflag != CV_COL_SAMPLE )
+        CV_ERROR( CV_StsBadArg,
+        "Unknown training data layout (must be CV_ROW_SAMPLE or CV_COL_SAMPLE)" );
+
+    if( var_all )
+        *var_all = tflag == CV_ROW_SAMPLE ? train_data->cols : train_data->rows;
+
+    if( sample_all )
+        *sample_all = tflag == CV_ROW_SAMPLE ? train_data->rows : train_data->cols;
+
+    __END__;
+}
+
+
+int
+cvPrepareTrainData( const char* /*funcname*/,
+                    const CvMat* train_data, int tflag,
+                    const CvMat* responses, int response_type,
+                    const CvMat* var_idx,
+                    const CvMat* sample_idx,
+                    bool always_copy_data,
+                    const float*** out_train_samples,
+                    int* _sample_count,
+                    int* _var_count,
+                    int* _var_all,
+                    CvMat** out_responses,
+                    CvMat** out_response_map,
+                    CvMat** out_var_idx,
+                    CvMat** out_sample_idx )
+{
+    int ok = 0;
+    CvMat* _var_idx = 0;
+    CvMat* _sample_idx = 0;
+    CvMat* _responses = 0;
+    int sample_all = 0, sample_count = 0, var_all = 0, var_count = 0;
+
+    CV_FUNCNAME( "cvPrepareTrainData" );
+
+    // step 0. clear all the output pointers to ensure we do not try
+    // to call free() with uninitialized pointers
+    if( out_responses )
+        *out_responses = 0;
+
+    if( out_response_map )
+        *out_response_map = 0;
+
+    if( out_var_idx )
+        *out_var_idx = 0;
+
+    if( out_sample_idx )
+        *out_sample_idx = 0;
+
+    if( out_train_samples )
+        *out_train_samples = 0;
+
+    if( _sample_count )
+        *_sample_count = 0;
+
+    if( _var_count )
+        *_var_count = 0;
+
+    if( _var_all )
+        *_var_all = 0;
+
+    __BEGIN__;
+
+    if( !out_train_samples )
+        CV_ERROR( CV_StsBadArg, "output pointer to train samples is NULL" );
+
+    CV_CALL( cvCheckTrainData( train_data, tflag, 0, &var_all, &sample_all ));
+
+    if( sample_idx )
+        CV_CALL( _sample_idx = cvPreprocessIndexArray( sample_idx, sample_all ));
+    if( var_idx )
+        CV_CALL( _var_idx = cvPreprocessIndexArray( var_idx, var_all ));
+
+    if( responses )
+    {
+        if( !out_responses )
+            CV_ERROR( CV_StsNullPtr, "output response pointer is NULL" );
+
+        if( response_type == CV_VAR_NUMERICAL )
+        {
+            CV_CALL( _responses = cvPreprocessOrderedResponses( responses,
+                                                _sample_idx, sample_all ));
+        }
+        else
+        {
+            CV_CALL( _responses = cvPreprocessCategoricalResponses( responses,
+                                _sample_idx, sample_all, out_response_map, 0 ));
+        }
+    }
+
+    CV_CALL( *out_train_samples =
+                cvGetTrainSamples( train_data, tflag, _var_idx, _sample_idx,
+                                   &var_count, &sample_count, always_copy_data ));
+
+    ok = 1;
+
+    __END__;
+
+    if( ok )
+    {
+        if( out_responses )
+            *out_responses = _responses, _responses = 0;
+
+        if( out_var_idx )
+            *out_var_idx = _var_idx, _var_idx = 0;
+
+        if( out_sample_idx )
+            *out_sample_idx = _sample_idx, _sample_idx = 0;
+
+        if( _sample_count )
+            *_sample_count = sample_count;
+
+        if( _var_count )
+            *_var_count = var_count;
+
+        if( _var_all )
+            *_var_all = var_all;
+    }
+    else
+    {
+        if( out_response_map )
+            cvReleaseMat( out_response_map );
+        cvFree( out_train_samples );
+    }
+
+    if( _responses != responses )
+        cvReleaseMat( &_responses );
+    cvReleaseMat( &_var_idx );
+    cvReleaseMat( &_sample_idx );
+
+    return ok;
+}
+
+
+typedef struct CvSampleResponsePair
+{
+    const float* sample;
+    const uchar* mask;
+    int response;
+    int index;
+}
+CvSampleResponsePair;
+
+
+static int
+CV_CDECL icvCmpSampleResponsePairs( const void* a, const void* b )
+{
+    int ra = ((const CvSampleResponsePair*)a)->response;
+    int rb = ((const CvSampleResponsePair*)b)->response;
+    int ia = ((const CvSampleResponsePair*)a)->index;
+    int ib = ((const CvSampleResponsePair*)b)->index;
+
+    return ra < rb ? -1 : ra > rb ? 1 : ia - ib;
+    //return (ra > rb ? -1 : 0)|(ra < rb);
+}
+
+
+void
+cvSortSamplesByClasses( const float** samples, const CvMat* classes,
+                        int* class_ranges, const uchar** mask )
+{
+    CvSampleResponsePair* pairs = 0;
+    CV_FUNCNAME( "cvSortSamplesByClasses" );
+
+    __BEGIN__;
+
+    int i, k = 0, sample_count;
+
+    if( !samples || !classes || !class_ranges )
+        CV_ERROR( CV_StsNullPtr, "INTERNAL ERROR: some of the args are NULL pointers" );
+
+    if( classes->rows != 1 || CV_MAT_TYPE(classes->type) != CV_32SC1 )
+        CV_ERROR( CV_StsBadArg, "classes array must be a single row of integers" );
+
+    sample_count = classes->cols;
+    CV_CALL( pairs = (CvSampleResponsePair*)cvAlloc( (sample_count+1)*sizeof(pairs[0])));
+
+    for( i = 0; i < sample_count; i++ )
+    {
+        pairs[i].sample = samples[i];
+        pairs[i].mask = (mask) ? (mask[i]) : 0;
+        pairs[i].response = classes->data.i[i];
+        pairs[i].index = i;
+        assert( classes->data.i[i] >= 0 );
+    }
+
+    qsort( pairs, sample_count, sizeof(pairs[0]), icvCmpSampleResponsePairs );
+    pairs[sample_count].response = -1;
+    class_ranges[0] = 0;
+
+    for( i = 0; i < sample_count; i++ )
+    {
+        samples[i] = pairs[i].sample;
+        if (mask)
+            mask[i] = pairs[i].mask;
+        classes->data.i[i] = pairs[i].response;
+
+        if( pairs[i].response != pairs[i+1].response )
+            class_ranges[++k] = i+1;
+    }
+
+    __END__;
+
+    cvFree( &pairs );
+}
+
+
+void
+cvPreparePredictData( const CvArr* _sample, int dims_all,
+                      const CvMat* comp_idx, int class_count,
+                      const CvMat* prob, float** _row_sample,
+                      int as_sparse )
+{
+    float* row_sample = 0;
+    int* inverse_comp_idx = 0;
+
+    CV_FUNCNAME( "cvPreparePredictData" );
+
+    __BEGIN__;
+
+    const CvMat* sample = (const CvMat*)_sample;
+    float* sample_data;
+    int sample_step;
+    int is_sparse = CV_IS_SPARSE_MAT(sample);
+    int d, sizes[CV_MAX_DIM];
+    int i, dims_selected;
+    int vec_size;
+
+    if( !is_sparse && !CV_IS_MAT(sample) )
+        CV_ERROR( !sample ? CV_StsNullPtr : CV_StsBadArg, "The sample is not a valid vector" );
+
+    if( cvGetElemType( sample ) != CV_32FC1 )
+        CV_ERROR( CV_StsUnsupportedFormat, "Input sample must have 32fC1 type" );
+
+    CV_CALL( d = cvGetDims( sample, sizes ));
+
+    if( !((is_sparse && d == 1) || (!is_sparse && d == 2 && (sample->rows == 1 || sample->cols == 1))) )
+        CV_ERROR( CV_StsBadSize, "Input sample must be 1-dimensional vector" );
+
+    if( d == 1 )
+        sizes[1] = 1;
+
+    if( sizes[0] + sizes[1] - 1 != dims_all )
+        CV_ERROR( CV_StsUnmatchedSizes,
+        "The sample size is different from what has been used for training" );
+
+    if( !_row_sample )
+        CV_ERROR( CV_StsNullPtr, "INTERNAL ERROR: The row_sample pointer is NULL" );
+
+    if( comp_idx && (!CV_IS_MAT(comp_idx) || comp_idx->rows != 1 ||
+        CV_MAT_TYPE(comp_idx->type) != CV_32SC1) )
+        CV_ERROR( CV_StsBadArg, "INTERNAL ERROR: invalid comp_idx" );
+
+    dims_selected = comp_idx ? comp_idx->cols : dims_all;
+
+    if( prob )
+    {
+        if( !CV_IS_MAT(prob) )
+            CV_ERROR( CV_StsBadArg, "The output matrix of probabilities is invalid" );
+
+        if( (prob->rows != 1 && prob->cols != 1) ||
+            (CV_MAT_TYPE(prob->type) != CV_32FC1 &&
+            CV_MAT_TYPE(prob->type) != CV_64FC1) )
+            CV_ERROR( CV_StsBadSize,
+            "The matrix of probabilities must be 1-dimensional vector of 32fC1 type" );
+
+        if( prob->rows + prob->cols - 1 != class_count )
+            CV_ERROR( CV_StsUnmatchedSizes,
+            "The vector of probabilities must contain as many elements as "
+            "the number of classes in the training set" );
+    }
+
+    vec_size = !as_sparse ? dims_selected*sizeof(row_sample[0]) :
+                (dims_selected + 1)*sizeof(CvSparseVecElem32f);
+
+    if( CV_IS_MAT(sample) )
+    {
+        sample_data = sample->data.fl;
+        sample_step = CV_IS_MAT_CONT(sample->type) ? 1 : sample->step/sizeof(row_sample[0]);
+
+        if( !comp_idx && CV_IS_MAT_CONT(sample->type) && !as_sparse )
+            *_row_sample = sample_data;
+        else
+        {
+            CV_CALL( row_sample = (float*)cvAlloc( vec_size ));
+
+            if( !comp_idx )
+                for( i = 0; i < dims_selected; i++ )
+                    row_sample[i] = sample_data[sample_step*i];
+            else
+            {
+                int* comp = comp_idx->data.i;
+                for( i = 0; i < dims_selected; i++ )
+                    row_sample[i] = sample_data[sample_step*comp[i]];
+            }
+
+            *_row_sample = row_sample;
+        }
+
+        if( as_sparse )
+        {
+            const float* src = (const float*)row_sample;
+            CvSparseVecElem32f* dst = (CvSparseVecElem32f*)row_sample;
+
+            dst[dims_selected].idx = -1;
+            for( i = dims_selected - 1; i >= 0; i-- )
+            {
+                dst[i].idx = i;
+                dst[i].val = src[i];
+            }
+        }
+    }
+    else
+    {
+        CvSparseNode* node;
+        CvSparseMatIterator mat_iterator;
+        const CvSparseMat* sparse = (const CvSparseMat*)sample;
+        assert( is_sparse );
+
+        node = cvInitSparseMatIterator( sparse, &mat_iterator );
+        CV_CALL( row_sample = (float*)cvAlloc( vec_size ));
+
+        if( comp_idx )
+        {
+            CV_CALL( inverse_comp_idx = (int*)cvAlloc( dims_all*sizeof(int) ));
+            memset( inverse_comp_idx, -1, dims_all*sizeof(int) );
+            for( i = 0; i < dims_selected; i++ )
+                inverse_comp_idx[comp_idx->data.i[i]] = i;
+        }
+
+        if( !as_sparse )
+        {
+            memset( row_sample, 0, vec_size );
+
+            for( ; node != 0; node = cvGetNextSparseNode(&mat_iterator) )
+            {
+                int idx = *CV_NODE_IDX( sparse, node );
+                if( inverse_comp_idx )
+                {
+                    idx = inverse_comp_idx[idx];
+                    if( idx < 0 )
+                        continue;
+                }
+                row_sample[idx] = *(float*)CV_NODE_VAL( sparse, node );
+            }
+        }
+        else
+        {
+            CvSparseVecElem32f* ptr = (CvSparseVecElem32f*)row_sample;
+
+            for( ; node != 0; node = cvGetNextSparseNode(&mat_iterator) )
+            {
+                int idx = *CV_NODE_IDX( sparse, node );
+                if( inverse_comp_idx )
+                {
+                    idx = inverse_comp_idx[idx];
+                    if( idx < 0 )
+                        continue;
+                }
+                ptr->idx = idx;
+                ptr->val = *(float*)CV_NODE_VAL( sparse, node );
+                ptr++;
+            }
+
+            qsort( row_sample, ptr - (CvSparseVecElem32f*)row_sample,
+                   sizeof(ptr[0]), icvCmpSparseVecElems );
+            ptr->idx = -1;
+        }
+
+        *_row_sample = row_sample;
+    }
+
+    __END__;
+
+    if( inverse_comp_idx )
+        cvFree( &inverse_comp_idx );
+
+    if( cvGetErrStatus() < 0 && _row_sample )
+    {
+        cvFree( &row_sample );
+        *_row_sample = 0;
+    }
+}
+
+
+static void
+icvConvertDataToSparse( const uchar* src, int src_step, int src_type,
+                        uchar* dst, int dst_step, int dst_type,
+                        CvSize size, int* idx )
+{
+    CV_FUNCNAME( "icvConvertDataToSparse" );
+
+    __BEGIN__;
+
+    int i, j;
+    src_type = CV_MAT_TYPE(src_type);
+    dst_type = CV_MAT_TYPE(dst_type);
+
+    if( CV_MAT_CN(src_type) != 1 || CV_MAT_CN(dst_type) != 1 )
+        CV_ERROR( CV_StsUnsupportedFormat, "The function supports only single-channel arrays" );
+
+    if( src_step == 0 )
+        src_step = CV_ELEM_SIZE(src_type);
+
+    if( dst_step == 0 )
+        dst_step = CV_ELEM_SIZE(dst_type);
+
+    // if there is no "idx" and if both arrays are continuous,
+    // do the whole processing (copying or conversion) in a single loop
+    if( !idx && CV_ELEM_SIZE(src_type)*size.width == src_step &&
+        CV_ELEM_SIZE(dst_type)*size.width == dst_step )
+    {
+        size.width *= size.height;
+        size.height = 1;
+    }
+
+    if( src_type == dst_type )
+    {
+        int full_width = CV_ELEM_SIZE(dst_type)*size.width;
+
+        if( full_width == sizeof(int) ) // another common case: copy int's or float's
+            for( i = 0; i < size.height; i++, src += src_step )
+                *(int*)(dst + dst_step*(idx ? idx[i] : i)) = *(int*)src;
+        else
+            for( i = 0; i < size.height; i++, src += src_step )
+                memcpy( dst + dst_step*(idx ? idx[i] : i), src, full_width );
+    }
+    else if( src_type == CV_32SC1 && (dst_type == CV_32FC1 || dst_type == CV_64FC1) )
+        for( i = 0; i < size.height; i++, src += src_step )
+        {
+            uchar* _dst = dst + dst_step*(idx ? idx[i] : i);
+            if( dst_type == CV_32FC1 )
+                for( j = 0; j < size.width; j++ )
+                    ((float*)_dst)[j] = (float)((int*)src)[j];
+            else
+                for( j = 0; j < size.width; j++ )
+                    ((double*)_dst)[j] = ((int*)src)[j];
+        }
+    else if( (src_type == CV_32FC1 || src_type == CV_64FC1) && dst_type == CV_32SC1 )
+        for( i = 0; i < size.height; i++, src += src_step )
+        {
+            uchar* _dst = dst + dst_step*(idx ? idx[i] : i);
+            if( src_type == CV_32FC1 )
+                for( j = 0; j < size.width; j++ )
+                    ((int*)_dst)[j] = cvRound(((float*)src)[j]);
+            else
+                for( j = 0; j < size.width; j++ )
+                    ((int*)_dst)[j] = cvRound(((double*)src)[j]);
+        }
+    else if( (src_type == CV_32FC1 && dst_type == CV_64FC1) ||
+             (src_type == CV_64FC1 && dst_type == CV_32FC1) )
+        for( i = 0; i < size.height; i++, src += src_step )
+        {
+            uchar* _dst = dst + dst_step*(idx ? idx[i] : i);
+            if( src_type == CV_32FC1 )
+                for( j = 0; j < size.width; j++ )
+                    ((double*)_dst)[j] = ((float*)src)[j];
+            else
+                for( j = 0; j < size.width; j++ )
+                    ((float*)_dst)[j] = (float)((double*)src)[j];
+        }
+    else
+        CV_ERROR( CV_StsUnsupportedFormat, "Unsupported combination of input and output vectors" );
+
+    __END__;
+}
+
+
+void
+cvWritebackLabels( const CvMat* labels, CvMat* dst_labels,
+                   const CvMat* centers, CvMat* dst_centers,
+                   const CvMat* probs, CvMat* dst_probs,
+                   const CvMat* sample_idx, int samples_all,
+                   const CvMat* comp_idx, int dims_all )
+{
+    CV_FUNCNAME( "cvWritebackLabels" );
+
+    __BEGIN__;
+
+    int samples_selected = samples_all, dims_selected = dims_all;
+
+    if( dst_labels && !CV_IS_MAT(dst_labels) )
+        CV_ERROR( CV_StsBadArg, "Array of output labels is not a valid matrix" );
+
+    if( dst_centers )
+        if( !ICV_IS_MAT_OF_TYPE(dst_centers, CV_32FC1) &&
+            !ICV_IS_MAT_OF_TYPE(dst_centers, CV_64FC1) )
+            CV_ERROR( CV_StsBadArg, "Array of cluster centers is not a valid matrix" );
+
+    if( dst_probs && !CV_IS_MAT(dst_probs) )
+        CV_ERROR( CV_StsBadArg, "Probability matrix is not valid" );
+
+    if( sample_idx )
+    {
+        CV_ASSERT( sample_idx->rows == 1 && CV_MAT_TYPE(sample_idx->type) == CV_32SC1 );
+        samples_selected = sample_idx->cols;
+    }
+
+    if( comp_idx )
+    {
+        CV_ASSERT( comp_idx->rows == 1 && CV_MAT_TYPE(comp_idx->type) == CV_32SC1 );
+        dims_selected = comp_idx->cols;
+    }
+
+    if( dst_labels && (!labels || labels->data.ptr != dst_labels->data.ptr) )
+    {
+        if( !labels )
+            CV_ERROR( CV_StsNullPtr, "NULL labels" );
+
+        CV_ASSERT( labels->rows == 1 );
+
+        if( dst_labels->rows != 1 && dst_labels->cols != 1 )
+            CV_ERROR( CV_StsBadSize, "Array of output labels should be 1d vector" );
+
+        if( dst_labels->rows + dst_labels->cols - 1 != samples_all )
+            CV_ERROR( CV_StsUnmatchedSizes,
+            "Size of vector of output labels is not equal to the total number of input samples" );
+
+        CV_ASSERT( labels->cols == samples_selected );
+
+        CV_CALL( icvConvertDataToSparse( labels->data.ptr, labels->step, labels->type,
+                        dst_labels->data.ptr, dst_labels->step, dst_labels->type,
+                        cvSize( 1, samples_selected ), sample_idx ? sample_idx->data.i : 0 ));
+    }
+
+    if( dst_centers && (!centers || centers->data.ptr != dst_centers->data.ptr) )
+    {
+        int i;
+
+        if( !centers )
+            CV_ERROR( CV_StsNullPtr, "NULL centers" );
+
+        if( centers->rows != dst_centers->rows )
+            CV_ERROR( CV_StsUnmatchedSizes, "Invalid number of rows in matrix of output centers" );
+
+        if( dst_centers->cols != dims_all )
+            CV_ERROR( CV_StsUnmatchedSizes,
+            "Number of columns in matrix of output centers is "
+            "not equal to the total number of components in the input samples" );
+
+        CV_ASSERT( centers->cols == dims_selected );
+
+        for( i = 0; i < centers->rows; i++ )
+            CV_CALL( icvConvertDataToSparse( centers->data.ptr + i*centers->step, 0, centers->type,
+                        dst_centers->data.ptr + i*dst_centers->step, 0, dst_centers->type,
+                        cvSize( 1, dims_selected ), comp_idx ? comp_idx->data.i : 0 ));
+    }
+
+    if( dst_probs && (!probs || probs->data.ptr != dst_probs->data.ptr) )
+    {
+        if( !probs )
+            CV_ERROR( CV_StsNullPtr, "NULL probs" );
+
+        if( probs->cols != dst_probs->cols )
+            CV_ERROR( CV_StsUnmatchedSizes, "Invalid number of columns in output probability matrix" );
+
+        if( dst_probs->rows != samples_all )
+            CV_ERROR( CV_StsUnmatchedSizes,
+            "Number of rows in output probability matrix is "
+            "not equal to the total number of input samples" );
+
+        CV_ASSERT( probs->rows == samples_selected );
+
+        CV_CALL( icvConvertDataToSparse( probs->data.ptr, probs->step, probs->type,
+                        dst_probs->data.ptr, dst_probs->step, dst_probs->type,
+                        cvSize( probs->cols, samples_selected ),
+                        sample_idx ? sample_idx->data.i : 0 ));
+    }
+
+    __END__;
+}
+
+#if 0
+CV_IMPL void
+cvStatModelMultiPredict( const CvStatModel* stat_model,
+                         const CvArr* predict_input,
+                         int flags, CvMat* predict_output,
+                         CvMat* probs, const CvMat* sample_idx )
+{
+    CvMemStorage* storage = 0;
+    CvMat* sample_idx_buffer = 0;
+    CvSparseMat** sparse_rows = 0;
+    int samples_selected = 0;
+
+    CV_FUNCNAME( "cvStatModelMultiPredict" );
+
+    __BEGIN__;
+
+    int i;
+    int predict_output_step = 1, sample_idx_step = 1;
+    int type;
+    int d, sizes[CV_MAX_DIM];
+    int tflag = flags == CV_COL_SAMPLE;
+    int samples_all, dims_all;
+    int is_sparse = CV_IS_SPARSE_MAT(predict_input);
+    CvMat predict_input_part;
+    CvArr* sample = &predict_input_part;
+    CvMat probs_part;
+    CvMat* probs1 = probs ? &probs_part : 0;
+
+    if( !CV_IS_STAT_MODEL(stat_model) )
+        CV_ERROR( !stat_model ? CV_StsNullPtr : CV_StsBadArg, "Invalid statistical model" );
+
+    if( !stat_model->predict )
+        CV_ERROR( CV_StsNotImplemented, "There is no \"predict\" method" );
+
+    if( !predict_input || !predict_output )
+        CV_ERROR( CV_StsNullPtr, "NULL input or output matrices" );
+
+    if( !is_sparse && !CV_IS_MAT(predict_input) )
+        CV_ERROR( CV_StsBadArg, "predict_input should be a matrix or a sparse matrix" );
+
+    if( !CV_IS_MAT(predict_output) )
+        CV_ERROR( CV_StsBadArg, "predict_output should be a matrix" );
+
+    type = cvGetElemType( predict_input );
+    if( type != CV_32FC1 ||
+        (CV_MAT_TYPE(predict_output->type) != CV_32FC1 &&
+         CV_MAT_TYPE(predict_output->type) != CV_32SC1 ))
+         CV_ERROR( CV_StsUnsupportedFormat, "The input or output matrix has unsupported format" );
+
+    CV_CALL( d = cvGetDims( predict_input, sizes ));
+    if( d > 2 )
+        CV_ERROR( CV_StsBadSize, "The input matrix should be 1- or 2-dimensional" );
+
+    if( !tflag )
+    {
+        samples_all = samples_selected = sizes[0];
+        dims_all = sizes[1];
+    }
+    else
+    {
+        samples_all = samples_selected = sizes[1];
+        dims_all = sizes[0];
+    }
+
+    if( sample_idx )
+    {
+        if( !CV_IS_MAT(sample_idx) )
+            CV_ERROR( CV_StsBadArg, "Invalid sample_idx matrix" );
+
+        if( sample_idx->cols != 1 && sample_idx->rows != 1 )
+            CV_ERROR( CV_StsBadSize, "sample_idx must be 1-dimensional matrix" );
+
+        samples_selected = sample_idx->rows + sample_idx->cols - 1;
+
+        if( CV_MAT_TYPE(sample_idx->type) == CV_32SC1 )
+        {
+            if( samples_selected > samples_all )
+                CV_ERROR( CV_StsBadSize, "sample_idx is too large vector" );
+        }
+        else if( samples_selected != samples_all )
+            CV_ERROR( CV_StsUnmatchedSizes, "sample_idx has incorrect size" );
+
+        sample_idx_step = sample_idx->step ?
+            sample_idx->step / CV_ELEM_SIZE(sample_idx->type) : 1;
+    }
+
+    if( predict_output->rows != 1 && predict_output->cols != 1 )
+        CV_ERROR( CV_StsBadSize, "predict_output should be a 1-dimensional matrix" );
+
+    if( predict_output->rows + predict_output->cols - 1 != samples_all )
+        CV_ERROR( CV_StsUnmatchedSizes, "predict_output and predict_input have uncoordinated sizes" );
+
+    predict_output_step = predict_output->step ?
+        predict_output->step / CV_ELEM_SIZE(predict_output->type) : 1;
+
+    if( probs )
+    {
+        if( !CV_IS_MAT(probs) )
+            CV_ERROR( CV_StsBadArg, "Invalid matrix of probabilities" );
+
+        if( probs->rows != samples_all )
+            CV_ERROR( CV_StsUnmatchedSizes,
+            "matrix of probabilities must have as many rows as the total number of samples" );
+
+        if( CV_MAT_TYPE(probs->type) != CV_32FC1 )
+            CV_ERROR( CV_StsUnsupportedFormat, "matrix of probabilities must have 32fC1 type" );
+    }
+
+    if( is_sparse )
+    {
+        CvSparseNode* node;
+        CvSparseMatIterator mat_iterator;
+        CvSparseMat* sparse = (CvSparseMat*)predict_input;
+
+        if( sample_idx && CV_MAT_TYPE(sample_idx->type) == CV_32SC1 )
+        {
+            CV_CALL( sample_idx_buffer = cvCreateMat( 1, samples_all, CV_8UC1 ));
+            cvZero( sample_idx_buffer );
+            for( i = 0; i < samples_selected; i++ )
+                sample_idx_buffer->data.ptr[sample_idx->data.i[i*sample_idx_step]] = 1;
+            samples_selected = samples_all;
+            sample_idx = sample_idx_buffer;
+            sample_idx_step = 1;
+        }
+
+        CV_CALL( sparse_rows = (CvSparseMat**)cvAlloc( samples_selected*sizeof(sparse_rows[0])));
+        for( i = 0; i < samples_selected; i++ )
+        {
+            if( sample_idx && sample_idx->data.ptr[i*sample_idx_step] == 0 )
+                continue;
+            CV_CALL( sparse_rows[i] = cvCreateSparseMat( 1, &dims_all, type ));
+            if( !storage )
+                storage = sparse_rows[i]->heap->storage;
+            else
+            {
+                // hack: to decrease memory footprint, make all the sparse matrices
+                // reside in the same storage
+                int elem_size = sparse_rows[i]->heap->elem_size;
+                cvReleaseMemStorage( &sparse_rows[i]->heap->storage );
+                sparse_rows[i]->heap = cvCreateSet( 0, sizeof(CvSet), elem_size, storage );
+            }
+        }
+
+        // put each row (or column) of predict_input into separate sparse matrix.
+        node = cvInitSparseMatIterator( sparse, &mat_iterator );
+        for( ; node != 0; node = cvGetNextSparseNode( &mat_iterator ))
+        {
+            int* idx = CV_NODE_IDX( sparse, node );
+            int idx0 = idx[tflag ^ 1];
+            int idx1 = idx[tflag];
+
+            if( sample_idx && sample_idx->data.ptr[idx0*sample_idx_step] == 0 )
+                continue;
+
+            assert( sparse_rows[idx0] != 0 );
+            *(float*)cvPtrND( sparse, &idx1, 0, 1, 0 ) = *(float*)CV_NODE_VAL( sparse, node );
+        }
+    }
+
+    for( i = 0; i < samples_selected; i++ )
+    {
+        int idx = i;
+        float response;
+
+        if( sample_idx )
+        {
+            if( CV_MAT_TYPE(sample_idx->type) == CV_32SC1 )
+            {
+                idx = sample_idx->data.i[i*sample_idx_step];
+                if( (unsigned)idx >= (unsigned)samples_all )
+                    CV_ERROR( CV_StsOutOfRange, "Some of sample_idx elements are out of range" );
+            }
+            else if( CV_MAT_TYPE(sample_idx->type) == CV_8UC1 &&
+                     sample_idx->data.ptr[i*sample_idx_step] == 0 )
+                continue;
+        }
+
+        if( !is_sparse )
+        {
+            if( !tflag )
+                cvGetRow( predict_input, &predict_input_part, idx );
+            else
+            {
+                cvGetCol( predict_input, &predict_input_part, idx );
+            }
+        }
+        else
+            sample = sparse_rows[idx];
+
+        if( probs )
+            cvGetRow( probs, probs1, idx );
+
+        CV_CALL( response = stat_model->predict( stat_model, (CvMat*)sample, probs1 ));
+
+        if( CV_MAT_TYPE(predict_output->type) == CV_32FC1 )
+            predict_output->data.fl[idx*predict_output_step] = response;
+        else
+        {
+            CV_ASSERT( cvRound(response) == response );
+            predict_output->data.i[idx*predict_output_step] = cvRound(response);
+        }
+    }
+
+    __END__;
+
+    if( sparse_rows )
+    {
+        int i;
+        for( i = 0; i < samples_selected; i++ )
+            if( sparse_rows[i] )
+            {
+                sparse_rows[i]->heap->storage = 0;
+                cvReleaseSparseMat( &sparse_rows[i] );
+            }
+        cvFree( &sparse_rows );
+    }
+
+    cvReleaseMat( &sample_idx_buffer );
+    cvReleaseMemStorage( &storage );
+}
+#endif
+
+// By P. Yarykin - begin -
+
+void cvCombineResponseMaps (CvMat*  _responses,
+                      const CvMat*  old_response_map,
+                            CvMat*  new_response_map,
+                            CvMat** out_response_map)
+{
+    int** old_data = NULL;
+    int** new_data = NULL;
+
+        CV_FUNCNAME ("cvCombineResponseMaps");
+        __BEGIN__
+
+    int i,j;
+    int old_n, new_n, out_n;
+    int samples, free_response;
+    int* first;
+    int* responses;
+    int* out_data;
+
+    if( out_response_map )
+        *out_response_map = 0;
+
+// Check input data.
+    if ((!ICV_IS_MAT_OF_TYPE (_responses, CV_32SC1)) ||
+        (!ICV_IS_MAT_OF_TYPE (old_response_map, CV_32SC1)) ||
+        (!ICV_IS_MAT_OF_TYPE (new_response_map, CV_32SC1)))
+    {
+        CV_ERROR (CV_StsBadArg, "Some of input arguments is not the CvMat")
+    }
+
+// Prepare sorted responses.
+    first = new_response_map->data.i;
+    new_n = new_response_map->cols;
+    CV_CALL (new_data = (int**)cvAlloc (new_n * sizeof (new_data[0])));
+    for (i = 0; i < new_n; i++)
+        new_data[i] = first + i;
+    qsort (new_data, new_n, sizeof(int*), icvCmpIntegersPtr);
+
+    first = old_response_map->data.i;
+    old_n = old_response_map->cols;
+    CV_CALL (old_data = (int**)cvAlloc (old_n * sizeof (old_data[0])));
+    for (i = 0; i < old_n; i++)
+        old_data[i] = first + i;
+    qsort (old_data, old_n, sizeof(int*), icvCmpIntegersPtr);
+
+// Count the number of different responses.
+    for (i = 0, j = 0, out_n = 0; i < old_n && j < new_n; out_n++)
+    {
+        if (*old_data[i] == *new_data[j])
+        {
+            i++;
+            j++;
+        }
+        else if (*old_data[i] < *new_data[j])
+            i++;
+        else
+            j++;
+    }
+    out_n += old_n - i + new_n - j;
+
+// Create and fill the result response maps.
+    CV_CALL (*out_response_map = cvCreateMat (1, out_n, CV_32SC1));
+    out_data = (*out_response_map)->data.i;
+    memcpy (out_data, first, old_n * sizeof (int));
+
+    free_response = old_n;
+    for (i = 0, j = 0; i < old_n && j < new_n; )
+    {
+        if (*old_data[i] == *new_data[j])
+        {
+            *new_data[j] = (int)(old_data[i] - first);
+            i++;
+            j++;
+        }
+        else if (*old_data[i] < *new_data[j])
+            i++;
+        else
+        {
+            out_data[free_response] = *new_data[j];
+            *new_data[j] = free_response++;
+            j++;
+        }
+    }
+    for (; j < new_n; j++)
+    {
+        out_data[free_response] = *new_data[j];
+        *new_data[j] = free_response++;
+    }
+    CV_ASSERT (free_response == out_n);
+
+// Change <responses> according to out response map.
+    samples = _responses->cols + _responses->rows - 1;
+    responses = _responses->data.i;
+    first = new_response_map->data.i;
+    for (i = 0; i < samples; i++)
+    {
+        responses[i] = first[responses[i]];
+    }
+
+        __END__
+
+    cvFree(&old_data);
+    cvFree(&new_data);
+
+}
+
+
+static int icvGetNumberOfCluster( double* prob_vector, int num_of_clusters, float r,
+                           float outlier_thresh, int normalize_probs )
+{
+    int max_prob_loc = 0;
+
+    CV_FUNCNAME("icvGetNumberOfCluster");
+    __BEGIN__;
+
+    double prob, maxprob, sum;
+    int i;
+
+    CV_ASSERT(prob_vector);
+    CV_ASSERT(num_of_clusters >= 0);
+
+    maxprob = prob_vector[0];
+    max_prob_loc = 0;
+    sum = maxprob;
+    for( i = 1; i < num_of_clusters; i++ )
+    {
+        prob = prob_vector[i];
+        sum += prob;
+        if( prob > maxprob )
+        {
+            max_prob_loc = i;
+            maxprob = prob;
+        }
+    }
+    if( normalize_probs && fabs(sum - 1.) > FLT_EPSILON )
+    {
+        for( i = 0; i < num_of_clusters; i++ )
+            prob_vector[i] /= sum;
+    }
+    if( fabs(r - 1.) > FLT_EPSILON && fabs(sum - 1.) < outlier_thresh )
+        max_prob_loc = -1;
+
+    __END__;
+
+    return max_prob_loc;
+
+} // End of icvGetNumberOfCluster
+
+
+void icvFindClusterLabels( const CvMat* probs, float outlier_thresh, float r,
+                          const CvMat* labels )
+{
+    CvMat* counts = 0;
+
+    CV_FUNCNAME("icvFindClusterLabels");
+    __BEGIN__;
+
+    int nclusters, nsamples;
+    int i, j;
+    double* probs_data;
+
+    CV_ASSERT( ICV_IS_MAT_OF_TYPE(probs, CV_64FC1) );
+    CV_ASSERT( ICV_IS_MAT_OF_TYPE(labels, CV_32SC1) );
+
+    nclusters = probs->cols;
+    nsamples  = probs->rows;
+    CV_ASSERT( nsamples == labels->cols );
+
+    CV_CALL( counts = cvCreateMat( 1, nclusters + 1, CV_32SC1 ) );
+    CV_CALL( cvSetZero( counts ));
+    for( i = 0; i < nsamples; i++ )
+    {
+        labels->data.i[i] = icvGetNumberOfCluster( probs->data.db + i*probs->cols,
+            nclusters, r, outlier_thresh, 1 );
+        counts->data.i[labels->data.i[i] + 1]++;
+    }
+    CV_ASSERT((int)cvSum(counts).val[0] == nsamples);
+    // Filling empty clusters with the vector, that has the maximal probability
+    for( j = 0; j < nclusters; j++ ) // outliers are ignored
+    {
+        int maxprob_loc = -1;
+        double maxprob = 0;
+
+        if( counts->data.i[j+1] ) // j-th class is not empty
+            continue;
+        // look for the presentative, which is not lonely in it's cluster
+        // and that has a maximal probability among all these vectors
+        probs_data = probs->data.db;
+        for( i = 0; i < nsamples; i++, probs_data++ )
+        {
+            int label = labels->data.i[i];
+            double prob;
+            if( counts->data.i[label+1] == 0 ||
+                (counts->data.i[label+1] <= 1 && label != -1) )
+                continue;
+            prob = *probs_data;
+            if( prob >= maxprob )
+            {
+                maxprob = prob;
+                maxprob_loc = i;
+            }
+        }
+        // maxprob_loc == 0 <=> number of vectors less then number of clusters
+        CV_ASSERT( maxprob_loc >= 0 );
+        counts->data.i[labels->data.i[maxprob_loc] + 1]--;
+        labels->data.i[maxprob_loc] = j;
+        counts->data.i[j + 1]++;
+    }
+
+    __END__;
+
+    cvReleaseMat( &counts );
+} // End of icvFindClusterLabels
+
+/* End of file */
diff --git a/openbr/core/old_ml_precomp.hpp b/openbr/core/old_ml_precomp.hpp
new file mode 100644
index 0000000..8409e11
--- /dev/null
+++ b/openbr/core/old_ml_precomp.hpp
@@ -0,0 +1,406 @@
+/*M///////////////////////////////////////////////////////////////////////////////////////
+//
+//  IMPORTANT: READ BEFORE DOWNLOADING, COPYING, INSTALLING OR USING.
+//
+//  By downloading, copying, installing or using the software you agree to this license.
+//  If you do not agree to this license, do not download, install,
+//  copy or use the software.
+//
+//
+//                        Intel License Agreement
+//
+// Copyright (C) 2000, Intel Corporation, all rights reserved.
+// Third party copyrights are property of their respective owners.
+//
+// Redistribution and use in source and binary forms, with or without modification,
+// are permitted provided that the following conditions are met:
+//
+//   * Redistribution's of source code must retain the above copyright notice,
+//     this list of conditions and the following disclaimer.
+//
+//   * Redistribution's in binary form must reproduce the above copyright notice,
+//     this list of conditions and the following disclaimer in the documentation
+//     and/or other materials provided with the distribution.
+//
+//   * The name of Intel Corporation may not be used to endorse or promote products
+//     derived from this software without specific prior written permission.
+//
+// This software is provided by the copyright holders and contributors "as is" and
+// any express or implied warranties, including, but not limited to, the implied
+// warranties of merchantability and fitness for a particular purpose are disclaimed.
+// In no event shall the Intel Corporation or contributors be liable for any direct,
+// indirect, incidental, special, exemplary, or consequential damages
+// (including, but not limited to, procurement of substitute goods or services;
+// loss of use, data, or profits; or business interruption) however caused
+// and on any theory of liability, whether in contract, strict liability,
+// or tort (including negligence or otherwise) arising in any way out of
+// the use of this software, even if advised of the possibility of such damage.
+//
+//M*/
+
+#ifndef OPENCV_PRECOMP_H
+#define OPENCV_PRECOMP_H
+
+#include "opencv2/core.hpp"
+#include "old_ml.hpp"
+#include "opencv2/core/core_c.h"
+#include "opencv2/core/utility.hpp"
+
+//SAB #include "opencv2/core/private.hpp"
+
+#include <assert.h>
+#include <float.h>
+#include <limits.h>
+#include <math.h>
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <time.h>
+
+#define ML_IMPL CV_IMPL
+#define __BEGIN__ __CV_BEGIN__
+#define __END__ __CV_END__
+#define EXIT __CV_EXIT__
+
+#define CV_MAT_ELEM_FLAG( mat, type, comp, vect, tflag )    \
+    (( tflag == CV_ROW_SAMPLE )                             \
+    ? (CV_MAT_ELEM( mat, type, comp, vect ))                \
+    : (CV_MAT_ELEM( mat, type, vect, comp )))
+
+/* Convert matrix to vector */
+#define ICV_MAT2VEC( mat, vdata, vstep, num )      \
+    if( MIN( (mat).rows, (mat).cols ) != 1 )       \
+        CV_ERROR( CV_StsBadArg, "" );              \
+    (vdata) = ((mat).data.ptr);                    \
+    if( (mat).rows == 1 )                          \
+    {                                              \
+        (vstep) = CV_ELEM_SIZE( (mat).type );      \
+        (num) = (mat).cols;                        \
+    }                                              \
+    else                                           \
+    {                                              \
+        (vstep) = (mat).step;                      \
+        (num) = (mat).rows;                        \
+    }
+
+/* get raw data */
+#define ICV_RAWDATA( mat, flags, rdata, sstep, cstep, m, n )         \
+    (rdata) = (mat).data.ptr;                                        \
+    if( CV_IS_ROW_SAMPLE( flags ) )                                  \
+    {                                                                \
+        (sstep) = (mat).step;                                        \
+        (cstep) = CV_ELEM_SIZE( (mat).type );                        \
+        (m) = (mat).rows;                                            \
+        (n) = (mat).cols;                                            \
+    }                                                                \
+    else                                                             \
+    {                                                                \
+        (cstep) = (mat).step;                                        \
+        (sstep) = CV_ELEM_SIZE( (mat).type );                        \
+        (n) = (mat).rows;                                            \
+        (m) = (mat).cols;                                            \
+    }
+
+#define ICV_IS_MAT_OF_TYPE( mat, mat_type) \
+    (CV_IS_MAT( mat ) && CV_MAT_TYPE( mat->type ) == (mat_type) &&   \
+    (mat)->cols > 0 && (mat)->rows > 0)
+
+/*
+    uchar* data; int sstep, cstep;      - trainData->data
+    uchar* classes; int clstep; int ncl;- trainClasses
+    uchar* tmask; int tmstep; int ntm;  - typeMask
+    uchar* missed;int msstep, mcstep;   -missedMeasurements...
+    int mm, mn;                         == m,n == size,dim
+    uchar* sidx;int sistep;             - sampleIdx
+    uchar* cidx;int cistep;             - compIdx
+    int k, l;                           == n,m == dim,size (length of cidx, sidx)
+    int m, n;                           == size,dim
+*/
+#define ICV_DECLARE_TRAIN_ARGS()                                                    \
+    uchar* data;                                                                    \
+    int sstep, cstep;                                                               \
+    uchar* classes;                                                                 \
+    int clstep;                                                                     \
+    int ncl;                                                                        \
+    uchar* tmask;                                                                   \
+    int tmstep;                                                                     \
+    int ntm;                                                                        \
+    uchar* missed;                                                                  \
+    int msstep, mcstep;                                                             \
+    int mm, mn;                                                                     \
+    uchar* sidx;                                                                    \
+    int sistep;                                                                     \
+    uchar* cidx;                                                                    \
+    int cistep;                                                                     \
+    int k, l;                                                                       \
+    int m, n;                                                                       \
+                                                                                    \
+    data = classes = tmask = missed = sidx = cidx = NULL;                           \
+    sstep = cstep = clstep = ncl = tmstep = ntm = msstep = mcstep = mm = mn = 0;    \
+    sistep = cistep = k = l = m = n = 0;
+
+#define ICV_TRAIN_DATA_REQUIRED( param, flags )                                     \
+    if( !ICV_IS_MAT_OF_TYPE( (param), CV_32FC1 ) )                                  \
+    {                                                                               \
+        CV_ERROR( CV_StsBadArg, "Invalid " #param " parameter" );                   \
+    }                                                                               \
+    else                                                                            \
+    {                                                                               \
+        ICV_RAWDATA( *(param), (flags), data, sstep, cstep, m, n );                 \
+        k = n;                                                                      \
+        l = m;                                                                      \
+    }
+
+#define ICV_TRAIN_CLASSES_REQUIRED( param )                                         \
+    if( !ICV_IS_MAT_OF_TYPE( (param), CV_32FC1 ) )                                  \
+    {                                                                               \
+        CV_ERROR( CV_StsBadArg, "Invalid " #param " parameter" );                   \
+    }                                                                               \
+    else                                                                            \
+    {                                                                               \
+        ICV_MAT2VEC( *(param), classes, clstep, ncl );                              \
+        if( m != ncl )                                                              \
+        {                                                                           \
+            CV_ERROR( CV_StsBadArg, "Unmatched sizes" );                            \
+        }                                                                           \
+    }
+
+#define ICV_ARG_NULL( param )                                                       \
+    if( (param) != NULL )                                                           \
+    {                                                                               \
+        CV_ERROR( CV_StsBadArg, #param " parameter must be NULL" );                 \
+    }
+
+#define ICV_MISSED_MEASUREMENTS_OPTIONAL( param, flags )                            \
+    if( param )                                                                     \
+    {                                                                               \
+        if( !ICV_IS_MAT_OF_TYPE( param, CV_8UC1 ) )                                 \
+        {                                                                           \
+            CV_ERROR( CV_StsBadArg, "Invalid " #param " parameter" );               \
+        }                                                                           \
+        else                                                                        \
+        {                                                                           \
+            ICV_RAWDATA( *(param), (flags), missed, msstep, mcstep, mm, mn );       \
+            if( mm != m || mn != n )                                                \
+            {                                                                       \
+                CV_ERROR( CV_StsBadArg, "Unmatched sizes" );                        \
+            }                                                                       \
+        }                                                                           \
+    }
+
+#define ICV_COMP_IDX_OPTIONAL( param )                                              \
+    if( param )                                                                     \
+    {                                                                               \
+        if( !ICV_IS_MAT_OF_TYPE( param, CV_32SC1 ) )                                \
+        {                                                                           \
+            CV_ERROR( CV_StsBadArg, "Invalid " #param " parameter" );               \
+        }                                                                           \
+        else                                                                        \
+        {                                                                           \
+            ICV_MAT2VEC( *(param), cidx, cistep, k );                               \
+            if( k > n )                                                             \
+                CV_ERROR( CV_StsBadArg, "Invalid " #param " parameter" );           \
+        }                                                                           \
+    }
+
+#define ICV_SAMPLE_IDX_OPTIONAL( param )                                            \
+    if( param )                                                                     \
+    {                                                                               \
+        if( !ICV_IS_MAT_OF_TYPE( param, CV_32SC1 ) )                                \
+        {                                                                           \
+            CV_ERROR( CV_StsBadArg, "Invalid " #param " parameter" );               \
+        }                                                                           \
+        else                                                                        \
+        {                                                                           \
+            ICV_MAT2VEC( *sampleIdx, sidx, sistep, l );                             \
+            if( l > m )                                                             \
+                CV_ERROR( CV_StsBadArg, "Invalid " #param " parameter" );           \
+        }                                                                           \
+    }
+
+/****************************************************************************************/
+#define ICV_CONVERT_FLOAT_ARRAY_TO_MATRICE( array, matrice )        \
+{                                                                   \
+    CvMat a, b;                                                     \
+    int dims = (matrice)->cols;                                     \
+    int nsamples = (matrice)->rows;                                 \
+    int type = CV_MAT_TYPE((matrice)->type);                        \
+    int i, offset = dims;                                           \
+                                                                    \
+    CV_ASSERT( type == CV_32FC1 || type == CV_64FC1 );              \
+    offset *= ((type == CV_32FC1) ? sizeof(float) : sizeof(double));\
+                                                                    \
+    b = cvMat( 1, dims, CV_32FC1 );                                 \
+    cvGetRow( matrice, &a, 0 );                                     \
+    for( i = 0; i < nsamples; i++, a.data.ptr += offset )           \
+    {                                                               \
+        b.data.fl = (float*)array[i];                               \
+        CV_CALL( cvConvert( &b, &a ) );                             \
+    }                                                               \
+}
+
+/****************************************************************************************\
+*                       Auxiliary functions declarations                                 *
+\****************************************************************************************/
+
+/* Generates a set of classes centers in quantity <num_of_clusters> that are generated as
+   uniform random vectors in parallelepiped, where <data> is concentrated. Vectors in
+   <data> should have horizontal orientation. If <centers> != NULL, the function doesn't
+   allocate any memory and stores generated centers in <centers>, returns <centers>.
+   If <centers> == NULL, the function allocates memory and creates the matrice. Centers
+   are supposed to be oriented horizontally. */
+CvMat* icvGenerateRandomClusterCenters( int seed,
+                                        const CvMat* data,
+                                        int num_of_clusters,
+                                        CvMat* centers CV_DEFAULT(0));
+
+/* Fills the <labels> using <probs> by choosing the maximal probability. Outliers are
+   fixed by <oulier_tresh> and have cluster label (-1). Function also controls that there
+   weren't "empty" clusters by filling empty clusters with the maximal probability vector.
+   If probs_sums != NULL, fills it with the sums of probabilities for each sample (it is
+   useful for normalizing probabilities' matrice of FCM) */
+void icvFindClusterLabels( const CvMat* probs, float outlier_thresh, float r,
+                           const CvMat* labels );
+
+typedef struct CvSparseVecElem32f
+{
+    int idx;
+    float val;
+}
+CvSparseVecElem32f;
+
+/* Prepare training data and related parameters */
+#define CV_TRAIN_STATMODEL_DEFRAGMENT_TRAIN_DATA    1
+#define CV_TRAIN_STATMODEL_SAMPLES_AS_ROWS          2
+#define CV_TRAIN_STATMODEL_SAMPLES_AS_COLUMNS       4
+#define CV_TRAIN_STATMODEL_CATEGORICAL_RESPONSE     8
+#define CV_TRAIN_STATMODEL_ORDERED_RESPONSE         16
+#define CV_TRAIN_STATMODEL_RESPONSES_ON_OUTPUT      32
+#define CV_TRAIN_STATMODEL_ALWAYS_COPY_TRAIN_DATA   64
+#define CV_TRAIN_STATMODEL_SPARSE_AS_SPARSE         128
+
+int
+cvPrepareTrainData( const char* /*funcname*/,
+                    const CvMat* train_data, int tflag,
+                    const CvMat* responses, int response_type,
+                    const CvMat* var_idx,
+                    const CvMat* sample_idx,
+                    bool always_copy_data,
+                    const float*** out_train_samples,
+                    int* _sample_count,
+                    int* _var_count,
+                    int* _var_all,
+                    CvMat** out_responses,
+                    CvMat** out_response_map,
+                    CvMat** out_var_idx,
+                    CvMat** out_sample_idx=0 );
+
+void
+cvSortSamplesByClasses( const float** samples, const CvMat* classes,
+                        int* class_ranges, const uchar** mask CV_DEFAULT(0) );
+
+void
+cvCombineResponseMaps (CvMat*  _responses,
+                 const CvMat*  old_response_map,
+                       CvMat*  new_response_map,
+                       CvMat** out_response_map);
+
+void
+cvPreparePredictData( const CvArr* sample, int dims_all, const CvMat* comp_idx,
+                      int class_count, const CvMat* prob, float** row_sample,
+                      int as_sparse CV_DEFAULT(0) );
+
+/* copies clustering [or batch "predict"] results
+   (labels and/or centers and/or probs) back to the output arrays */
+void
+cvWritebackLabels( const CvMat* labels, CvMat* dst_labels,
+                   const CvMat* centers, CvMat* dst_centers,
+                   const CvMat* probs, CvMat* dst_probs,
+                   const CvMat* sample_idx, int samples_all,
+                   const CvMat* comp_idx, int dims_all );
+#define cvWritebackResponses cvWritebackLabels
+
+#define XML_FIELD_NAME "_name"
+CvFileNode* icvFileNodeGetChild(CvFileNode* father, const char* name);
+CvFileNode* icvFileNodeGetChildArrayElem(CvFileNode* father, const char* name,int index);
+CvFileNode* icvFileNodeGetNext(CvFileNode* n, const char* name);
+
+
+void cvCheckTrainData( const CvMat* train_data, int tflag,
+                       const CvMat* missing_mask,
+                       int* var_all, int* sample_all );
+
+CvMat* cvPreprocessIndexArray( const CvMat* idx_arr, int data_arr_size, bool check_for_duplicates=false );
+
+CvMat* cvPreprocessVarType( const CvMat* type_mask, const CvMat* var_idx,
+                            int var_all, int* response_type );
+
+CvMat* cvPreprocessOrderedResponses( const CvMat* responses,
+                const CvMat* sample_idx, int sample_all );
+
+CvMat* cvPreprocessCategoricalResponses( const CvMat* responses,
+                const CvMat* sample_idx, int sample_all,
+                CvMat** out_response_map, CvMat** class_counts=0 );
+
+const float** cvGetTrainSamples( const CvMat* train_data, int tflag,
+                   const CvMat* var_idx, const CvMat* sample_idx,
+                   int* _var_count, int* _sample_count,
+                   bool always_copy_data=false );
+
+namespace cv
+{
+//SAB begin
+    class BlockedRange
+    {
+    public:
+        BlockedRange() : _begin(0), _end(0), _grainsize(0) {}
+        BlockedRange(int b, int e, int g=1) : _begin(b), _end(e), _grainsize(g) {}
+        int begin() const { return _begin; }
+        int end() const { return _end; }
+        int grainsize() const { return _grainsize; }
+
+    protected:
+        int _begin, _end, _grainsize;
+    };
+
+    template<typename Body> static inline
+    void parallel_for( const BlockedRange& range, const Body& body )
+    {
+        body(range);
+    }
+//    typedef std::vector<Rect> ConcurrentRectVector;
+
+    class Split {};
+
+    template<typename Body> static inline
+    void parallel_reduce( const BlockedRange& range, Body& body )
+    {
+        body(range);
+    }
+//SAB end
+
+    struct DTreeBestSplitFinder
+    {
+        DTreeBestSplitFinder(){ splitSize = 0, tree = 0; node = 0; }
+        DTreeBestSplitFinder( CvDTree* _tree, CvDTreeNode* _node);
+        DTreeBestSplitFinder( const DTreeBestSplitFinder& finder, Split );
+        virtual ~DTreeBestSplitFinder() {}
+        virtual void operator()(const BlockedRange& range);
+        void join( DTreeBestSplitFinder& rhs );
+        Ptr<CvDTreeSplit> bestSplit;
+        Ptr<CvDTreeSplit> split;
+        int splitSize;
+        CvDTree* tree;
+        CvDTreeNode* node;
+    };
+
+    struct ForestTreeBestSplitFinder : DTreeBestSplitFinder
+    {
+        ForestTreeBestSplitFinder() : DTreeBestSplitFinder() {}
+        ForestTreeBestSplitFinder( CvForestTree* _tree, CvDTreeNode* _node );
+        ForestTreeBestSplitFinder( const ForestTreeBestSplitFinder& finder, Split );
+        virtual void operator()(const BlockedRange& range);
+    };
+}
+
+#endif /* __ML_H__ */
diff --git a/openbr/core/old_ml_rtrees.cpp b/openbr/core/old_ml_rtrees.cpp
new file mode 100644
index 0000000..d6005d4
--- /dev/null
+++ b/openbr/core/old_ml_rtrees.cpp
@@ -0,0 +1,868 @@
+/*M///////////////////////////////////////////////////////////////////////////////////////
+//
+//  IMPORTANT: READ BEFORE DOWNLOADING, COPYING, INSTALLING OR USING.
+//
+//  By downloading, copying, installing or using the software you agree to this license.
+//  If you do not agree to this license, do not download, install,
+//  copy or use the software.
+//
+//
+//                        Intel License Agreement
+//
+// Copyright (C) 2000, Intel Corporation, all rights reserved.
+// Third party copyrights are property of their respective owners.
+//
+// Redistribution and use in source and binary forms, with or without modification,
+// are permitted provided that the following conditions are met:
+//
+//   * Redistribution's of source code must retain the above copyright notice,
+//     this list of conditions and the following disclaimer.
+//
+//   * Redistribution's in binary form must reproduce the above copyright notice,
+//     this list of conditions and the following disclaimer in the documentation
+//     and/or other materials provided with the distribution.
+//
+//   * The name of Intel Corporation may not be used to endorse or promote products
+//     derived from this software without specific prior written permission.
+//
+// This software is provided by the copyright holders and contributors "as is" and
+// any express or implied warranties, including, but not limited to, the implied
+// warranties of merchantability and fitness for a particular purpose are disclaimed.
+// In no event shall the Intel Corporation or contributors be liable for any direct,
+// indirect, incidental, special, exemplary, or consequential damages
+// (including, but not limited to, procurement of substitute goods or services;
+// loss of use, data, or profits; or business interruption) however caused
+// and on any theory of liability, whether in contract, strict liability,
+// or tort (including negligence or otherwise) arising in any way out of
+// the use of this software, even if advised of the possibility of such damage.
+//
+//M*/
+
+#include "old_ml_precomp.hpp"
+
+CvForestTree::CvForestTree()
+{
+    forest = NULL;
+}
+
+
+CvForestTree::~CvForestTree()
+{
+    clear();
+}
+
+
+bool CvForestTree::train( CvDTreeTrainData* _data,
+                          const CvMat* _subsample_idx,
+                          CvRTrees* _forest )
+{
+    clear();
+    forest = _forest;
+
+    data = _data;
+    data->shared = true;
+    return do_train(_subsample_idx);
+}
+
+
+bool
+CvForestTree::train( const CvMat*, int, const CvMat*, const CvMat*,
+                    const CvMat*, const CvMat*, const CvMat*, CvDTreeParams )
+{
+    assert(0);
+    return false;
+}
+
+
+bool
+CvForestTree::train( CvDTreeTrainData*, const CvMat* )
+{
+    assert(0);
+    return false;
+}
+
+
+
+namespace cv
+{
+
+ForestTreeBestSplitFinder::ForestTreeBestSplitFinder( CvForestTree* _tree, CvDTreeNode* _node ) :
+    DTreeBestSplitFinder(_tree, _node) {}
+
+ForestTreeBestSplitFinder::ForestTreeBestSplitFinder( const ForestTreeBestSplitFinder& finder, Split spl ) :
+    DTreeBestSplitFinder( finder, spl ) {}
+
+void ForestTreeBestSplitFinder::operator()(const BlockedRange& range)
+{
+    int vi, vi1 = range.begin(), vi2 = range.end();
+    int n = node->sample_count;
+    CvDTreeTrainData* data = tree->get_data();
+    AutoBuffer<uchar> inn_buf(2*n*(sizeof(int) + sizeof(float)));
+
+    CvForestTree* ftree = (CvForestTree*)tree;
+    const CvMat* active_var_mask = ftree->forest->get_active_var_mask();
+
+    for( vi = vi1; vi < vi2; vi++ )
+    {
+        CvDTreeSplit *res;
+        int ci = data->var_type->data.i[vi];
+        if( node->num_valid[vi] <= 1
+            || (active_var_mask && !active_var_mask->data.ptr[vi]) )
+            continue;
+
+        if( data->is_classifier )
+        {
+            if( ci >= 0 )
+                res = ftree->find_split_cat_class( node, vi, bestSplit->quality, split, (uchar*)inn_buf );
+            else
+                res = ftree->find_split_ord_class( node, vi, bestSplit->quality, split, (uchar*)inn_buf );
+        }
+        else
+        {
+            if( ci >= 0 )
+                res = ftree->find_split_cat_reg( node, vi, bestSplit->quality, split, (uchar*)inn_buf );
+            else
+                res = ftree->find_split_ord_reg( node, vi, bestSplit->quality, split, (uchar*)inn_buf );
+        }
+
+        if( res && bestSplit->quality < split->quality )
+                memcpy( (CvDTreeSplit*)bestSplit, (CvDTreeSplit*)split, splitSize );
+    }
+}
+}
+
+CvDTreeSplit* CvForestTree::find_best_split( CvDTreeNode* node )
+{
+    CvMat* active_var_mask = 0;
+    if( forest )
+    {
+        int var_count;
+        CvRNG* rng = forest->get_rng();
+
+        active_var_mask = forest->get_active_var_mask();
+        var_count = active_var_mask->cols;
+
+        CV_Assert( var_count == data->var_count );
+
+        for( int vi = 0; vi < var_count; vi++ )
+        {
+            uchar temp;
+            int i1 = cvRandInt(rng) % var_count;
+            int i2 = cvRandInt(rng) % var_count;
+            CV_SWAP( active_var_mask->data.ptr[i1],
+                active_var_mask->data.ptr[i2], temp );
+        }
+    }
+
+    cv::ForestTreeBestSplitFinder finder( this, node );
+
+    cv::parallel_reduce(cv::BlockedRange(0, data->var_count), finder);
+
+    CvDTreeSplit *bestSplit = 0;
+    if( finder.bestSplit->quality > 0 )
+    {
+        bestSplit = data->new_split_cat( 0, -1.0f );
+        memcpy( bestSplit, finder.bestSplit, finder.splitSize );
+    }
+
+    return bestSplit;
+}
+
+void CvForestTree::read( CvFileStorage* fs, CvFileNode* fnode, CvRTrees* _forest, CvDTreeTrainData* _data )
+{
+    CvDTree::read( fs, fnode, _data );
+    forest = _forest;
+}
+
+
+void CvForestTree::read( CvFileStorage*, CvFileNode* )
+{
+    assert(0);
+}
+
+void CvForestTree::read( CvFileStorage* _fs, CvFileNode* _node,
+                         CvDTreeTrainData* _data )
+{
+    CvDTree::read( _fs, _node, _data );
+}
+
+
+//////////////////////////////////////////////////////////////////////////////////////////
+//                                  Random trees                                        //
+//////////////////////////////////////////////////////////////////////////////////////////
+CvRTParams::CvRTParams() : CvDTreeParams( 5, 10, 0, false, 10, 0, false, false, 0 ),
+    calc_var_importance(false), nactive_vars(0)
+{
+    term_crit = cvTermCriteria( CV_TERMCRIT_ITER+CV_TERMCRIT_EPS, 50, 0.1 );
+}
+
+CvRTParams::CvRTParams( int _max_depth, int _min_sample_count,
+                        float _regression_accuracy, bool _use_surrogates,
+                        int _max_categories, const float* _priors, bool _calc_var_importance,
+                        int _nactive_vars, int max_num_of_trees_in_the_forest,
+                        float forest_accuracy, int termcrit_type ) :
+    CvDTreeParams( _max_depth, _min_sample_count, _regression_accuracy,
+                   _use_surrogates, _max_categories, 0,
+                   false, false, _priors ),
+    calc_var_importance(_calc_var_importance),
+    nactive_vars(_nactive_vars)
+{
+    term_crit = cvTermCriteria(termcrit_type,
+        max_num_of_trees_in_the_forest, forest_accuracy);
+}
+
+CvRTrees::CvRTrees()
+{
+    nclasses         = 0;
+    oob_error        = 0;
+    ntrees           = 0;
+    trees            = NULL;
+    data             = NULL;
+    active_var_mask  = NULL;
+    var_importance   = NULL;
+    rng = &cv::theRNG();
+    default_model_name = "my_random_trees";
+}
+
+
+void CvRTrees::clear()
+{
+    int k;
+    for( k = 0; k < ntrees; k++ )
+        delete trees[k];
+    cvFree( &trees );
+
+    delete data;
+    data = 0;
+
+    cvReleaseMat( &active_var_mask );
+    cvReleaseMat( &var_importance );
+    ntrees = 0;
+}
+
+
+CvRTrees::~CvRTrees()
+{
+    clear();
+}
+
+//SAB std::string CvRTrees::getName() const
+cv::String CvRTrees::getName() const
+{
+    return CV_TYPE_NAME_ML_RTREES;
+}
+
+CvMat* CvRTrees::get_active_var_mask()
+{
+    return active_var_mask;
+}
+
+
+CvRNG* CvRTrees::get_rng()
+{
+    return &rng->state;
+}
+
+bool CvRTrees::train( const CvMat* _train_data, int _tflag,
+                        const CvMat* _responses, const CvMat* _var_idx,
+                        const CvMat* _sample_idx, const CvMat* _var_type,
+                        const CvMat* _missing_mask, CvRTParams params )
+{
+    clear();
+
+    CvDTreeParams tree_params( params.max_depth, params.min_sample_count,
+        params.regression_accuracy, params.use_surrogates, params.max_categories,
+        params.cv_folds, params.use_1se_rule, false, params.priors );
+
+    data = new CvDTreeTrainData();
+    data->set_data( _train_data, _tflag, _responses, _var_idx,
+        _sample_idx, _var_type, _missing_mask, tree_params, true);
+
+    int var_count = data->var_count;
+    if( params.nactive_vars > var_count )
+        params.nactive_vars = var_count;
+    else if( params.nactive_vars == 0 )
+        params.nactive_vars = (int)sqrt((double)var_count);
+    else if( params.nactive_vars < 0 )
+        CV_Error( CV_StsBadArg, "<nactive_vars> must be non-negative" );
+
+    // Create mask of active variables at the tree nodes
+    active_var_mask = cvCreateMat( 1, var_count, CV_8UC1 );
+    if( params.calc_var_importance )
+    {
+        var_importance  = cvCreateMat( 1, var_count, CV_32FC1 );
+        cvZero(var_importance);
+    }
+    { // initialize active variables mask
+        CvMat submask1, submask2;
+        CV_Assert( (active_var_mask->cols >= 1) && (params.nactive_vars > 0) && (params.nactive_vars <= active_var_mask->cols) );
+        cvGetCols( active_var_mask, &submask1, 0, params.nactive_vars );
+        cvSet( &submask1, cvScalar(1) );
+        if( params.nactive_vars < active_var_mask->cols )
+        {
+            cvGetCols( active_var_mask, &submask2, params.nactive_vars, var_count );
+            cvZero( &submask2 );
+        }
+    }
+
+    return grow_forest( params.term_crit );
+}
+
+bool CvRTrees::train( CvMLData* _data, CvRTParams params )
+{
+    const CvMat* values = _data->get_values();
+    const CvMat* response = _data->get_responses();
+    const CvMat* missing = _data->get_missing();
+    const CvMat* var_types = _data->get_var_types();
+    const CvMat* train_sidx = _data->get_train_sample_idx();
+    const CvMat* var_idx = _data->get_var_idx();
+
+    return train( values, CV_ROW_SAMPLE, response, var_idx,
+                  train_sidx, var_types, missing, params );
+}
+
+bool CvRTrees::grow_forest( const CvTermCriteria term_crit )
+{
+    CvMat* sample_idx_mask_for_tree = 0;
+    CvMat* sample_idx_for_tree      = 0;
+
+    const int max_ntrees = term_crit.max_iter;
+    const double max_oob_err = term_crit.epsilon;
+
+    const int dims = data->var_count;
+    float maximal_response = 0;
+
+    CvMat* oob_sample_votes    = 0;
+    CvMat* oob_responses       = 0;
+
+    float* oob_samples_perm_ptr= 0;
+
+    float* samples_ptr     = 0;
+    uchar* missing_ptr     = 0;
+    float* true_resp_ptr   = 0;
+    bool is_oob_or_vimportance = (max_oob_err > 0 && term_crit.type != CV_TERMCRIT_ITER) || var_importance;
+
+    // oob_predictions_sum[i] = sum of predicted values for the i-th sample
+    // oob_num_of_predictions[i] = number of summands
+    //                            (number of predictions for the i-th sample)
+    // initialize these variable to avoid warning C4701
+    CvMat oob_predictions_sum = cvMat( 1, 1, CV_32FC1 );
+    CvMat oob_num_of_predictions = cvMat( 1, 1, CV_32FC1 );
+
+    nsamples = data->sample_count;
+    nclasses = data->get_num_classes();
+
+    if ( is_oob_or_vimportance )
+    {
+        if( data->is_classifier )
+        {
+            oob_sample_votes = cvCreateMat( nsamples, nclasses, CV_32SC1 );
+            cvZero(oob_sample_votes);
+        }
+        else
+        {
+            // oob_responses[0,i] = oob_predictions_sum[i]
+            //    = sum of predicted values for the i-th sample
+            // oob_responses[1,i] = oob_num_of_predictions[i]
+            //    = number of summands (number of predictions for the i-th sample)
+            oob_responses = cvCreateMat( 2, nsamples, CV_32FC1 );
+            cvZero(oob_responses);
+            cvGetRow( oob_responses, &oob_predictions_sum, 0 );
+            cvGetRow( oob_responses, &oob_num_of_predictions, 1 );
+        }
+
+        oob_samples_perm_ptr     = (float*)cvAlloc( sizeof(float)*nsamples*dims );
+        samples_ptr              = (float*)cvAlloc( sizeof(float)*nsamples*dims );
+        missing_ptr              = (uchar*)cvAlloc( sizeof(uchar)*nsamples*dims );
+        true_resp_ptr            = (float*)cvAlloc( sizeof(float)*nsamples );
+
+        data->get_vectors( 0, samples_ptr, missing_ptr, true_resp_ptr );
+
+        double minval, maxval;
+        CvMat responses = cvMat(1, nsamples, CV_32FC1, true_resp_ptr);
+        cvMinMaxLoc( &responses, &minval, &maxval );
+        maximal_response = (float)MAX( MAX( fabs(minval), fabs(maxval) ), 0 );
+    }
+
+    trees = (CvForestTree**)cvAlloc( sizeof(trees[0])*max_ntrees );
+    memset( trees, 0, sizeof(trees[0])*max_ntrees );
+
+    sample_idx_mask_for_tree = cvCreateMat( 1, nsamples, CV_8UC1 );
+    sample_idx_for_tree      = cvCreateMat( 1, nsamples, CV_32SC1 );
+
+    ntrees = 0;
+    while( ntrees < max_ntrees )
+    {
+        int i, oob_samples_count = 0;
+        double ncorrect_responses = 0; // used for estimation of variable importance
+        CvForestTree* tree = 0;
+
+        cvZero( sample_idx_mask_for_tree );
+        for(i = 0; i < nsamples; i++ ) //form sample for creation one tree
+        {
+            int idx = (*rng)(nsamples);
+            sample_idx_for_tree->data.i[i] = idx;
+            sample_idx_mask_for_tree->data.ptr[idx] = 0xFF;
+        }
+
+        trees[ntrees] = new CvForestTree();
+        tree = trees[ntrees];
+        tree->train( data, sample_idx_for_tree, this );
+
+        if ( is_oob_or_vimportance )
+        {
+            CvMat sample, missing;
+            // form array of OOB samples indices and get these samples
+            sample   = cvMat( 1, dims, CV_32FC1, samples_ptr );
+            missing  = cvMat( 1, dims, CV_8UC1,  missing_ptr );
+
+            oob_error = 0;
+            for( i = 0; i < nsamples; i++,
+                sample.data.fl += dims, missing.data.ptr += dims )
+            {
+                CvDTreeNode* predicted_node = 0;
+                // check if the sample is OOB
+                if( sample_idx_mask_for_tree->data.ptr[i] )
+                    continue;
+
+                // predict oob samples
+                if( !predicted_node )
+                    predicted_node = tree->predict(&sample, &missing, true);
+
+                if( !data->is_classifier ) //regression
+                {
+                    double avg_resp, resp = predicted_node->value;
+                    oob_predictions_sum.data.fl[i] += (float)resp;
+                    oob_num_of_predictions.data.fl[i] += 1;
+
+                    // compute oob error
+                    avg_resp = oob_predictions_sum.data.fl[i]/oob_num_of_predictions.data.fl[i];
+                    avg_resp -= true_resp_ptr[i];
+                    oob_error += avg_resp*avg_resp;
+                    resp = (resp - true_resp_ptr[i])/maximal_response;
+                    ncorrect_responses += exp( -resp*resp );
+                }
+                else //classification
+                {
+                    double prdct_resp;
+                    CvPoint max_loc;
+                    CvMat votes;
+
+                    cvGetRow(oob_sample_votes, &votes, i);
+                    votes.data.i[predicted_node->class_idx]++;
+
+                    // compute oob error
+                    cvMinMaxLoc( &votes, 0, 0, 0, &max_loc );
+
+                    prdct_resp = data->cat_map->data.i[max_loc.x];
+                    oob_error += (fabs(prdct_resp - true_resp_ptr[i]) < FLT_EPSILON) ? 0 : 1;
+
+                    ncorrect_responses += cvRound(predicted_node->value - true_resp_ptr[i]) == 0;
+                }
+                oob_samples_count++;
+            }
+            if( oob_samples_count > 0 )
+                oob_error /= (double)oob_samples_count;
+
+            // estimate variable importance
+            if( var_importance && oob_samples_count > 0 )
+            {
+                int m;
+
+                memcpy( oob_samples_perm_ptr, samples_ptr, dims*nsamples*sizeof(float));
+                for( m = 0; m < dims; m++ )
+                {
+                    double ncorrect_responses_permuted = 0;
+                    // randomly permute values of the m-th variable in the oob samples
+                    float* mth_var_ptr = oob_samples_perm_ptr + m;
+
+                    for( i = 0; i < nsamples; i++ )
+                    {
+                        int i1, i2;
+                        float temp;
+
+                        if( sample_idx_mask_for_tree->data.ptr[i] ) //the sample is not OOB
+                            continue;
+                        i1 = (*rng)(nsamples);
+                        i2 = (*rng)(nsamples);
+                        CV_SWAP( mth_var_ptr[i1*dims], mth_var_ptr[i2*dims], temp );
+
+                        // turn values of (m-1)-th variable, that were permuted
+                        // at the previous iteration, untouched
+                        if( m > 1 )
+                            oob_samples_perm_ptr[i*dims+m-1] = samples_ptr[i*dims+m-1];
+                    }
+
+                    // predict "permuted" cases and calculate the number of votes for the
+                    // correct class in the variable-m-permuted oob data
+                    sample  = cvMat( 1, dims, CV_32FC1, oob_samples_perm_ptr );
+                    missing = cvMat( 1, dims, CV_8UC1, missing_ptr );
+                    for( i = 0; i < nsamples; i++,
+                        sample.data.fl += dims, missing.data.ptr += dims )
+                    {
+                        double predct_resp, true_resp;
+
+                        if( sample_idx_mask_for_tree->data.ptr[i] ) //the sample is not OOB
+                            continue;
+
+                        predct_resp = tree->predict(&sample, &missing, true)->value;
+                        true_resp   = true_resp_ptr[i];
+                        if( data->is_classifier )
+                            ncorrect_responses_permuted += cvRound(true_resp - predct_resp) == 0;
+                        else
+                        {
+                            true_resp = (true_resp - predct_resp)/maximal_response;
+                            ncorrect_responses_permuted += exp( -true_resp*true_resp );
+                        }
+                    }
+                    var_importance->data.fl[m] += (float)(ncorrect_responses
+                        - ncorrect_responses_permuted);
+                }
+            }
+        }
+        ntrees++;
+        if( term_crit.type != CV_TERMCRIT_ITER && oob_error < max_oob_err )
+            break;
+    }
+
+    if( var_importance )
+    {
+        for ( int vi = 0; vi < var_importance->cols; vi++ )
+                var_importance->data.fl[vi] = ( var_importance->data.fl[vi] > 0 ) ?
+                    var_importance->data.fl[vi] : 0;
+        cvNormalize( var_importance, var_importance, 1., 0, CV_L1 );
+    }
+
+    cvFree( &oob_samples_perm_ptr );
+    cvFree( &samples_ptr );
+    cvFree( &missing_ptr );
+    cvFree( &true_resp_ptr );
+
+    cvReleaseMat( &sample_idx_mask_for_tree );
+    cvReleaseMat( &sample_idx_for_tree );
+
+    cvReleaseMat( &oob_sample_votes );
+    cvReleaseMat( &oob_responses );
+
+    return true;
+}
+
+
+const CvMat* CvRTrees::get_var_importance()
+{
+    return var_importance;
+}
+
+
+float CvRTrees::get_proximity( const CvMat* sample1, const CvMat* sample2,
+                              const CvMat* missing1, const CvMat* missing2 ) const
+{
+    float result = 0;
+
+    for( int i = 0; i < ntrees; i++ )
+        result += trees[i]->predict( sample1, missing1 ) ==
+        trees[i]->predict( sample2, missing2 ) ?  1 : 0;
+    result = result/(float)ntrees;
+
+    return result;
+}
+
+float CvRTrees::calc_error( CvMLData* _data, int type , std::vector<float> *resp )
+{
+    float err = 0;
+    const CvMat* values = _data->get_values();
+    const CvMat* response = _data->get_responses();
+    const CvMat* missing = _data->get_missing();
+    const CvMat* sample_idx = (type == CV_TEST_ERROR) ? _data->get_test_sample_idx() : _data->get_train_sample_idx();
+    const CvMat* var_types = _data->get_var_types();
+    int* sidx = sample_idx ? sample_idx->data.i : 0;
+    int r_step = CV_IS_MAT_CONT(response->type) ?
+                1 : response->step / CV_ELEM_SIZE(response->type);
+    bool is_classifier = var_types->data.ptr[var_types->cols-1] == CV_VAR_CATEGORICAL;
+    int sample_count = sample_idx ? sample_idx->cols : 0;
+    sample_count = (type == CV_TRAIN_ERROR && sample_count == 0) ? values->rows : sample_count;
+    float* pred_resp = 0;
+    if( resp && (sample_count > 0) )
+    {
+        resp->resize( sample_count );
+        pred_resp = &((*resp)[0]);
+    }
+    if ( is_classifier )
+    {
+        for( int i = 0; i < sample_count; i++ )
+        {
+            CvMat sample, miss;
+            int si = sidx ? sidx[i] : i;
+            cvGetRow( values, &sample, si );
+            if( missing )
+                cvGetRow( missing, &miss, si );
+            float r = (float)predict( &sample, missing ? &miss : 0 );
+            if( pred_resp )
+                pred_resp[i] = r;
+            int d = fabs((double)r - response->data.fl[si*r_step]) <= FLT_EPSILON ? 0 : 1;
+            err += d;
+        }
+        err = sample_count ? err / (float)sample_count * 100 : -FLT_MAX;
+    }
+    else
+    {
+        for( int i = 0; i < sample_count; i++ )
+        {
+            CvMat sample, miss;
+            int si = sidx ? sidx[i] : i;
+            cvGetRow( values, &sample, si );
+            if( missing )
+                cvGetRow( missing, &miss, si );
+            float r = (float)predict( &sample, missing ? &miss : 0 );
+            if( pred_resp )
+                pred_resp[i] = r;
+            float d = r - response->data.fl[si*r_step];
+            err += d*d;
+        }
+        err = sample_count ? err / (float)sample_count : -FLT_MAX;
+    }
+    return err;
+}
+
+float CvRTrees::get_train_error()
+{
+    float err = -1;
+
+    int sample_count = data->sample_count;
+    int var_count = data->var_count;
+
+    float *values_ptr = (float*)cvAlloc( sizeof(float)*sample_count*var_count );
+    uchar *missing_ptr = (uchar*)cvAlloc( sizeof(uchar)*sample_count*var_count );
+    float *responses_ptr = (float*)cvAlloc( sizeof(float)*sample_count );
+
+    data->get_vectors( 0, values_ptr, missing_ptr, responses_ptr);
+
+    if (data->is_classifier)
+    {
+        int err_count = 0;
+        float *vp = values_ptr;
+        uchar *mp = missing_ptr;
+        for (int si = 0; si < sample_count; si++, vp += var_count, mp += var_count)
+        {
+            CvMat sample = cvMat( 1, var_count, CV_32FC1, vp );
+            CvMat missing = cvMat( 1, var_count, CV_8UC1,  mp );
+            float r = predict( &sample, &missing );
+            if (fabs(r - responses_ptr[si]) >= FLT_EPSILON)
+                err_count++;
+        }
+        err = (float)err_count / (float)sample_count;
+    }
+    else
+        CV_Error( CV_StsBadArg, "This method is not supported for regression problems" );
+
+    cvFree( &values_ptr );
+    cvFree( &missing_ptr );
+    cvFree( &responses_ptr );
+
+    return err;
+}
+
+
+float CvRTrees::predict( const CvMat* sample, const CvMat* missing ) const
+{
+    double result = -1;
+    int k;
+
+    if( nclasses > 0 ) //classification
+    {
+        int max_nvotes = 0;
+        cv::AutoBuffer<int> _votes(nclasses);
+        int* votes = _votes;
+        memset( votes, 0, sizeof(*votes)*nclasses );
+        for( k = 0; k < ntrees; k++ )
+        {
+            CvDTreeNode* predicted_node = trees[k]->predict( sample, missing );
+            int nvotes;
+            int class_idx = predicted_node->class_idx;
+            CV_Assert( 0 <= class_idx && class_idx < nclasses );
+
+            nvotes = ++votes[class_idx];
+            if( nvotes > max_nvotes )
+            {
+                max_nvotes = nvotes;
+                result = predicted_node->value;
+            }
+        }
+    }
+    else // regression
+    {
+        result = 0;
+        for( k = 0; k < ntrees; k++ )
+            result += trees[k]->predict( sample, missing )->value;
+        result /= (double)ntrees;
+    }
+
+    return (float)result;
+}
+
+float CvRTrees::predict_prob( const CvMat* sample, const CvMat* missing) const
+{
+    if( nclasses == 2 ) //classification
+    {
+        cv::AutoBuffer<int> _votes(nclasses);
+        int* votes = _votes;
+        memset( votes, 0, sizeof(*votes)*nclasses );
+        for( int k = 0; k < ntrees; k++ )
+        {
+            CvDTreeNode* predicted_node = trees[k]->predict( sample, missing );
+            int class_idx = predicted_node->class_idx;
+            CV_Assert( 0 <= class_idx && class_idx < nclasses );
+
+            ++votes[class_idx];
+        }
+
+        return float(votes[1])/ntrees;
+    }
+    else // regression
+        CV_Error(CV_StsBadArg, "This function works for binary classification problems only...");
+
+    return -1;
+}
+
+void CvRTrees::write( CvFileStorage* fs, const char* name ) const
+{
+    int k;
+
+    if( ntrees < 1 || !trees || nsamples < 1 )
+        CV_Error( CV_StsBadArg, "Invalid CvRTrees object" );
+
+    std::string modelNodeName = this->getName();
+    cvStartWriteStruct( fs, name, CV_NODE_MAP, modelNodeName.c_str() );
+
+    cvWriteInt( fs, "nclasses", nclasses );
+    cvWriteInt( fs, "nsamples", nsamples );
+    cvWriteInt( fs, "nactive_vars", (int)cvSum(active_var_mask).val[0] );
+    cvWriteReal( fs, "oob_error", oob_error );
+
+    if( var_importance )
+        cvWrite( fs, "var_importance", var_importance );
+
+    cvWriteInt( fs, "ntrees", ntrees );
+
+    data->write_params( fs );
+
+    cvStartWriteStruct( fs, "trees", CV_NODE_SEQ );
+
+    for( k = 0; k < ntrees; k++ )
+    {
+        cvStartWriteStruct( fs, 0, CV_NODE_MAP );
+        trees[k]->write( fs );
+        cvEndWriteStruct( fs );
+    }
+
+    cvEndWriteStruct( fs ); //trees
+    cvEndWriteStruct( fs ); //CV_TYPE_NAME_ML_RTREES
+}
+
+
+void CvRTrees::read( CvFileStorage* fs, CvFileNode* fnode )
+{
+    int nactive_vars, var_count, k;
+    CvSeqReader reader;
+    CvFileNode* trees_fnode = 0;
+
+    clear();
+
+    nclasses     = cvReadIntByName( fs, fnode, "nclasses", -1 );
+    nsamples     = cvReadIntByName( fs, fnode, "nsamples" );
+    nactive_vars = cvReadIntByName( fs, fnode, "nactive_vars", -1 );
+    oob_error    = cvReadRealByName(fs, fnode, "oob_error", -1 );
+    ntrees       = cvReadIntByName( fs, fnode, "ntrees", -1 );
+
+    var_importance = (CvMat*)cvReadByName( fs, fnode, "var_importance" );
+
+    if( nclasses < 0 || nsamples <= 0 || nactive_vars < 0 || oob_error < 0 || ntrees <= 0)
+        CV_Error( CV_StsParseError, "Some <nclasses>, <nsamples>, <var_count>, "
+        "<nactive_vars>, <oob_error>, <ntrees> of tags are missing" );
+
+    rng = &cv::theRNG();
+
+    trees = (CvForestTree**)cvAlloc( sizeof(trees[0])*ntrees );
+    memset( trees, 0, sizeof(trees[0])*ntrees );
+
+    data = new CvDTreeTrainData();
+    data->read_params( fs, fnode );
+    data->shared = true;
+
+    trees_fnode = cvGetFileNodeByName( fs, fnode, "trees" );
+    if( !trees_fnode || !CV_NODE_IS_SEQ(trees_fnode->tag) )
+        CV_Error( CV_StsParseError, "<trees> tag is missing" );
+
+    cvStartReadSeq( trees_fnode->data.seq, &reader );
+    if( reader.seq->total != ntrees )
+        CV_Error( CV_StsParseError,
+        "<ntrees> is not equal to the number of trees saved in file" );
+
+    for( k = 0; k < ntrees; k++ )
+    {
+        trees[k] = new CvForestTree();
+        trees[k]->read( fs, (CvFileNode*)reader.ptr, this, data );
+        CV_NEXT_SEQ_ELEM( reader.seq->elem_size, reader );
+    }
+
+    var_count = data->var_count;
+    active_var_mask = cvCreateMat( 1, var_count, CV_8UC1 );
+    {
+        // initialize active variables mask
+        CvMat submask1;
+        cvGetCols( active_var_mask, &submask1, 0, nactive_vars );
+        cvSet( &submask1, cvScalar(1) );
+
+        if( nactive_vars < var_count )
+        {
+            CvMat submask2;
+            cvGetCols( active_var_mask, &submask2, nactive_vars, var_count );
+            cvZero( &submask2 );
+        }
+    }
+}
+
+
+int CvRTrees::get_tree_count() const
+{
+    return ntrees;
+}
+
+CvForestTree* CvRTrees::get_tree(int i) const
+{
+    return (unsigned)i < (unsigned)ntrees ? trees[i] : 0;
+}
+
+using namespace cv;
+
+bool CvRTrees::train( const Mat& _train_data, int _tflag,
+                     const Mat& _responses, const Mat& _var_idx,
+                     const Mat& _sample_idx, const Mat& _var_type,
+                     const Mat& _missing_mask, CvRTParams _params )
+{
+    CvMat tdata = _train_data, responses = _responses, vidx = _var_idx,
+    sidx = _sample_idx, vtype = _var_type, mmask = _missing_mask;
+    return train(&tdata, _tflag, &responses, vidx.data.ptr ? &vidx : 0,
+                 sidx.data.ptr ? &sidx : 0, vtype.data.ptr ? &vtype : 0,
+                 mmask.data.ptr ? &mmask : 0, _params);
+}
+
+
+float CvRTrees::predict( const Mat& _sample, const Mat& _missing ) const
+{
+    CvMat sample = _sample, mmask = _missing;
+    return predict(&sample, mmask.data.ptr ? &mmask : 0);
+}
+
+float CvRTrees::predict_prob( const Mat& _sample, const Mat& _missing) const
+{
+    CvMat sample = _sample, mmask = _missing;
+    return predict_prob(&sample, mmask.data.ptr ? &mmask : 0);
+}
+
+Mat CvRTrees::getVarImportance()
+{
+    return cvarrToMat(get_var_importance());
+}
+
+// End of file.
diff --git a/openbr/core/old_ml_svm.cpp b/openbr/core/old_ml_svm.cpp
new file mode 100644
index 0000000..1a7b186
--- /dev/null
+++ b/openbr/core/old_ml_svm.cpp
@@ -0,0 +1,2964 @@
+/*M///////////////////////////////////////////////////////////////////////////////////////
+//
+//  IMPORTANT: READ BEFORE DOWNLOADING, COPYING, INSTALLING OR USING.
+//
+//  By downloading, copying, installing or using the software you agree to this license.
+//  If you do not agree to this license, do not download, install,
+//  copy or use the software.
+//
+//
+//                        Intel License Agreement
+//
+// Copyright (C) 2000, Intel Corporation, all rights reserved.
+// Third party copyrights are property of their respective owners.
+//
+// Redistribution and use in source and binary forms, with or without modification,
+// are permitted provided that the following conditions are met:
+//
+//   * Redistribution's of source code must retain the above copyright notice,
+//     this list of conditions and the following disclaimer.
+//
+//   * Redistribution's in binary form must reproduce the above copyright notice,
+//     this list of conditions and the following disclaimer in the documentation
+//     and/or other materials provided with the distribution.
+//
+//   * The name of Intel Corporation may not be used to endorse or promote products
+//     derived from this software without specific prior written permission.
+//
+// This software is provided by the copyright holders and contributors "as is" and
+// any express or implied warranties, including, but not limited to, the implied
+// warranties of merchantability and fitness for a particular purpose are disclaimed.
+// In no event shall the Intel Corporation or contributors be liable for any direct,
+// indirect, incidental, special, exemplary, or consequential damages
+// (including, but not limited to, procurement of substitute goods or services;
+// loss of use, data, or profits; or business interruption) however caused
+// and on any theory of liability, whether in contract, strict liability,
+// or tort (including negligence or otherwise) arising in any way out of
+// the use of this software, even if advised of the possibility of such damage.
+//
+//M*/
+
+#include "old_ml_precomp.hpp"
+
+/****************************************************************************************\
+                                COPYRIGHT NOTICE
+                                ----------------
+
+  The code has been derived from libsvm library (version 2.6)
+  (http://www.csie.ntu.edu.tw/~cjlin/libsvm).
+
+  Here is the orignal copyright:
+------------------------------------------------------------------------------------------
+    Copyright (c) 2000-2003 Chih-Chung Chang and Chih-Jen Lin
+    All rights reserved.
+
+    Redistribution and use in source and binary forms, with or without
+    modification, are permitted provided that the following conditions
+    are met:
+
+    1. Redistributions of source code must retain the above copyright
+    notice, this list of conditions and the following disclaimer.
+
+    2. Redistributions in binary form must reproduce the above copyright
+    notice, this list of conditions and the following disclaimer in the
+    documentation and/or other materials provided with the distribution.
+
+    3. Neither name of copyright holders nor the names of its contributors
+    may be used to endorse or promote products derived from this software
+    without specific prior written permission.
+
+
+    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+    ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+    LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+    A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR
+    CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+    EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+    PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+    PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+    LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+    NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+    SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+\****************************************************************************************/
+
+using namespace cv;
+
+#define CV_SVM_MIN_CACHE_SIZE  (40 << 20)  /* 40Mb */
+
+#include <stdarg.h>
+#include <ctype.h>
+
+#if 1
+typedef float Qfloat;
+#define QFLOAT_TYPE CV_32F
+#else
+typedef double Qfloat;
+#define QFLOAT_TYPE CV_64F
+#endif
+
+// Param Grid
+bool CvParamGrid::check() const
+{
+    bool ok = false;
+
+    CV_FUNCNAME( "CvParamGrid::check" );
+    __BEGIN__;
+
+    if( min_val > max_val )
+        CV_ERROR( CV_StsBadArg, "Lower bound of the grid must be less then the upper one" );
+    if( min_val < DBL_EPSILON )
+        CV_ERROR( CV_StsBadArg, "Lower bound of the grid must be positive" );
+    if( step < 1. + FLT_EPSILON )
+        CV_ERROR( CV_StsBadArg, "Grid step must greater then 1" );
+
+    ok = true;
+
+    __END__;
+
+    return ok;
+}
+
+CvParamGrid CvSVM::get_default_grid( int param_id )
+{
+    CvParamGrid grid;
+    if( param_id == CvSVM::C )
+    {
+        grid.min_val = 0.1;
+        grid.max_val = 500;
+        grid.step = 5; // total iterations = 5
+    }
+    else if( param_id == CvSVM::GAMMA )
+    {
+        grid.min_val = 1e-5;
+        grid.max_val = 0.6;
+        grid.step = 15; // total iterations = 4
+    }
+    else if( param_id == CvSVM::P )
+    {
+        grid.min_val = 0.01;
+        grid.max_val = 100;
+        grid.step = 7; // total iterations = 4
+    }
+    else if( param_id == CvSVM::NU )
+    {
+        grid.min_val = 0.01;
+        grid.max_val = 0.2;
+        grid.step = 3; // total iterations = 3
+    }
+    else if( param_id == CvSVM::COEF )
+    {
+        grid.min_val = 0.1;
+        grid.max_val = 300;
+        grid.step = 14; // total iterations = 3
+    }
+    else if( param_id == CvSVM::DEGREE )
+    {
+        grid.min_val = 0.01;
+        grid.max_val = 4;
+        grid.step = 7; // total iterations = 3
+    }
+    else
+        cvError( CV_StsBadArg, "CvSVM::get_default_grid", "Invalid type of parameter "
+            "(use one of CvSVM::C, CvSVM::GAMMA et al.)", __FILE__, __LINE__ );
+    return grid;
+}
+
+// SVM training parameters
+CvSVMParams::CvSVMParams() :
+    svm_type(CvSVM::C_SVC), kernel_type(CvSVM::RBF), degree(0),
+    gamma(1), coef0(0), C(1), nu(0), p(0), class_weights(0)
+{
+    term_crit = cvTermCriteria( CV_TERMCRIT_ITER+CV_TERMCRIT_EPS, 1000, FLT_EPSILON );
+}
+
+
+CvSVMParams::CvSVMParams( int _svm_type, int _kernel_type,
+    double _degree, double _gamma, double _coef0,
+    double _Con, double _nu, double _p,
+    CvMat* _class_weights, CvTermCriteria _term_crit ) :
+    svm_type(_svm_type), kernel_type(_kernel_type),
+    degree(_degree), gamma(_gamma), coef0(_coef0),
+    C(_Con), nu(_nu), p(_p), class_weights(_class_weights), term_crit(_term_crit)
+{
+}
+
+
+/////////////////////////////////////// SVM kernel ///////////////////////////////////////
+
+CvSVMKernel::CvSVMKernel()
+{
+    clear();
+}
+
+
+void CvSVMKernel::clear()
+{
+    params = 0;
+    calc_func = 0;
+}
+
+
+CvSVMKernel::~CvSVMKernel()
+{
+}
+
+
+CvSVMKernel::CvSVMKernel( const CvSVMParams* _params, Calc _calc_func )
+{
+    clear();
+    create( _params, _calc_func );
+}
+
+
+bool CvSVMKernel::create( const CvSVMParams* _params, Calc _calc_func )
+{
+    clear();
+    params = _params;
+    calc_func = _calc_func;
+
+    if( !calc_func )
+        calc_func = params->kernel_type == CvSVM::RBF ? &CvSVMKernel::calc_rbf :
+                    params->kernel_type == CvSVM::POLY ? &CvSVMKernel::calc_poly :
+                    params->kernel_type == CvSVM::SIGMOID ? &CvSVMKernel::calc_sigmoid :
+                    &CvSVMKernel::calc_linear;
+
+    return true;
+}
+
+
+void CvSVMKernel::calc_non_rbf_base( int vcount, int var_count, const float** vecs,
+                                     const float* another, Qfloat* results,
+                                     double alpha, double beta )
+{
+    int j, k;
+    for( j = 0; j < vcount; j++ )
+    {
+        const float* sample = vecs[j];
+        double s = 0;
+        for( k = 0; k <= var_count - 4; k += 4 )
+            s += sample[k]*another[k] + sample[k+1]*another[k+1] +
+                 sample[k+2]*another[k+2] + sample[k+3]*another[k+3];
+        for( ; k < var_count; k++ )
+            s += sample[k]*another[k];
+        results[j] = (Qfloat)(s*alpha + beta);
+    }
+}
+
+
+void CvSVMKernel::calc_linear( int vcount, int var_count, const float** vecs,
+                               const float* another, Qfloat* results )
+{
+    calc_non_rbf_base( vcount, var_count, vecs, another, results, 1, 0 );
+}
+
+
+void CvSVMKernel::calc_poly( int vcount, int var_count, const float** vecs,
+                             const float* another, Qfloat* results )
+{
+    CvMat R = cvMat( 1, vcount, QFLOAT_TYPE, results );
+    calc_non_rbf_base( vcount, var_count, vecs, another, results, params->gamma, params->coef0 );
+    if( vcount > 0 )
+        cvPow( &R, &R, params->degree );
+}
+
+
+void CvSVMKernel::calc_sigmoid( int vcount, int var_count, const float** vecs,
+                                const float* another, Qfloat* results )
+{
+    int j;
+    calc_non_rbf_base( vcount, var_count, vecs, another, results,
+                       -2*params->gamma, -2*params->coef0 );
+    // TODO: speedup this
+    for( j = 0; j < vcount; j++ )
+    {
+        Qfloat t = results[j];
+        double e = exp(-fabs(t));
+        if( t > 0 )
+            results[j] = (Qfloat)((1. - e)/(1. + e));
+        else
+            results[j] = (Qfloat)((e - 1.)/(e + 1.));
+    }
+}
+
+
+void CvSVMKernel::calc_rbf( int vcount, int var_count, const float** vecs,
+                            const float* another, Qfloat* results )
+{
+    CvMat R = cvMat( 1, vcount, QFLOAT_TYPE, results );
+    double gamma = -params->gamma;
+    int j, k;
+
+    for( j = 0; j < vcount; j++ )
+    {
+        const float* sample = vecs[j];
+        double s = 0;
+
+        for( k = 0; k <= var_count - 4; k += 4 )
+        {
+            double t0 = sample[k] - another[k];
+            double t1 = sample[k+1] - another[k+1];
+
+            s += t0*t0 + t1*t1;
+
+            t0 = sample[k+2] - another[k+2];
+            t1 = sample[k+3] - another[k+3];
+
+            s += t0*t0 + t1*t1;
+        }
+
+        for( ; k < var_count; k++ )
+        {
+            double t0 = sample[k] - another[k];
+            s += t0*t0;
+        }
+        results[j] = (Qfloat)(s*gamma);
+    }
+
+    if( vcount > 0 )
+        cvExp( &R, &R );
+}
+
+
+void CvSVMKernel::calc( int vcount, int var_count, const float** vecs,
+                        const float* another, Qfloat* results )
+{
+    const Qfloat max_val = (Qfloat)(FLT_MAX*1e-3);
+    int j;
+    (this->*calc_func)( vcount, var_count, vecs, another, results );
+    for( j = 0; j < vcount; j++ )
+    {
+        if( results[j] > max_val )
+            results[j] = max_val;
+    }
+}
+
+
+// Generalized SMO+SVMlight algorithm
+// Solves:
+//
+//  min [0.5(\alpha^T Q \alpha) + b^T \alpha]
+//
+//      y^T \alpha = \delta
+//      y_i = +1 or -1
+//      0 <= alpha_i <= Cp for y_i = 1
+//      0 <= alpha_i <= Cn for y_i = -1
+//
+// Given:
+//
+//  Q, b, y, Cp, Cn, and an initial feasible point \alpha
+//  l is the size of vectors and matrices
+//  eps is the stopping criterion
+//
+// solution will be put in \alpha, objective value will be put in obj
+//
+
+void CvSVMSolver::clear()
+{
+    G = 0;
+    alpha = 0;
+    y = 0;
+    b = 0;
+    buf[0] = buf[1] = 0;
+    cvReleaseMemStorage( &storage );
+    kernel = 0;
+    select_working_set_func = 0;
+    calc_rho_func = 0;
+
+    rows = 0;
+    samples = 0;
+    get_row_func = 0;
+}
+
+
+CvSVMSolver::CvSVMSolver()
+{
+    storage = 0;
+    clear();
+}
+
+
+CvSVMSolver::~CvSVMSolver()
+{
+    clear();
+}
+
+
+CvSVMSolver::CvSVMSolver( int _sample_count, int _var_count, const float** _samples, schar* _y,
+                int _alpha_count, double* _alpha, double _Cp, double _Cn,
+                CvMemStorage* _storage, CvSVMKernel* _kernel, GetRow _get_row,
+                SelectWorkingSet _select_working_set, CalcRho _calc_rho )
+{
+    storage = 0;
+    create( _sample_count, _var_count, _samples, _y, _alpha_count, _alpha, _Cp, _Cn,
+            _storage, _kernel, _get_row, _select_working_set, _calc_rho );
+}
+
+
+bool CvSVMSolver::create( int _sample_count, int _var_count, const float** _samples, schar* _y,
+                int _alpha_count, double* _alpha, double _Cp, double _Cn,
+                CvMemStorage* _storage, CvSVMKernel* _kernel, GetRow _get_row,
+                SelectWorkingSet _select_working_set, CalcRho _calc_rho )
+{
+    bool ok = false;
+    int i, svm_type;
+
+    CV_FUNCNAME( "CvSVMSolver::create" );
+
+    __BEGIN__;
+
+    int rows_hdr_size;
+
+    clear();
+
+    sample_count = _sample_count;
+    var_count = _var_count;
+    samples = _samples;
+    y = _y;
+    alpha_count = _alpha_count;
+    alpha = _alpha;
+    kernel = _kernel;
+
+    C[0] = _Cn;
+    C[1] = _Cp;
+    eps = kernel->params->term_crit.epsilon;
+    max_iter = kernel->params->term_crit.max_iter;
+    storage = cvCreateChildMemStorage( _storage );
+
+    b = (double*)cvMemStorageAlloc( storage, alpha_count*sizeof(b[0]));
+    alpha_status = (schar*)cvMemStorageAlloc( storage, alpha_count*sizeof(alpha_status[0]));
+    G = (double*)cvMemStorageAlloc( storage, alpha_count*sizeof(G[0]));
+    for( i = 0; i < 2; i++ )
+        buf[i] = (Qfloat*)cvMemStorageAlloc( storage, sample_count*2*sizeof(buf[i][0]) );
+    svm_type = kernel->params->svm_type;
+
+    select_working_set_func = _select_working_set;
+    if( !select_working_set_func )
+        select_working_set_func = svm_type == CvSVM::NU_SVC || svm_type == CvSVM::NU_SVR ?
+        &CvSVMSolver::select_working_set_nu_svm : &CvSVMSolver::select_working_set;
+
+    calc_rho_func = _calc_rho;
+    if( !calc_rho_func )
+        calc_rho_func = svm_type == CvSVM::NU_SVC || svm_type == CvSVM::NU_SVR ?
+            &CvSVMSolver::calc_rho_nu_svm : &CvSVMSolver::calc_rho;
+
+    get_row_func = _get_row;
+    if( !get_row_func )
+        get_row_func = params->svm_type == CvSVM::EPS_SVR ||
+                       params->svm_type == CvSVM::NU_SVR ? &CvSVMSolver::get_row_svr :
+                       params->svm_type == CvSVM::C_SVC ||
+                       params->svm_type == CvSVM::NU_SVC ? &CvSVMSolver::get_row_svc :
+                       &CvSVMSolver::get_row_one_class;
+
+    cache_line_size = sample_count*sizeof(Qfloat);
+    // cache size = max(num_of_samples^2*sizeof(Qfloat)*0.25, 64Kb)
+    // (assuming that for large training sets ~25% of Q matrix is used)
+    cache_size = MAX( cache_line_size*sample_count/4, CV_SVM_MIN_CACHE_SIZE );
+
+    // the size of Q matrix row headers
+    rows_hdr_size = sample_count*sizeof(rows[0]);
+    if( rows_hdr_size > storage->block_size )
+        CV_ERROR( CV_StsOutOfRange, "Too small storage block size" );
+
+    lru_list.prev = lru_list.next = &lru_list;
+    rows = (CvSVMKernelRow*)cvMemStorageAlloc( storage, rows_hdr_size );
+    memset( rows, 0, rows_hdr_size );
+
+    ok = true;
+
+    __END__;
+
+    return ok;
+}
+
+
+float* CvSVMSolver::get_row_base( int i, bool* _existed )
+{
+    int i1 = i < sample_count ? i : i - sample_count;
+    CvSVMKernelRow* row = rows + i1;
+    bool existed = row->data != 0;
+    Qfloat* data;
+
+    if( existed || cache_size <= 0 )
+    {
+        CvSVMKernelRow* del_row = existed ? row : lru_list.prev;
+        data = del_row->data;
+        assert( data != 0 );
+
+        // delete row from the LRU list
+        del_row->data = 0;
+        del_row->prev->next = del_row->next;
+        del_row->next->prev = del_row->prev;
+    }
+    else
+    {
+        data = (Qfloat*)cvMemStorageAlloc( storage, cache_line_size );
+        cache_size -= cache_line_size;
+    }
+
+    // insert row into the LRU list
+    row->data = data;
+    row->prev = &lru_list;
+    row->next = lru_list.next;
+    row->prev->next = row->next->prev = row;
+
+    if( !existed )
+    {
+        kernel->calc( sample_count, var_count, samples, samples[i1], row->data );
+    }
+
+    if( _existed )
+        *_existed = existed;
+
+    return row->data;
+}
+
+
+float* CvSVMSolver::get_row_svc( int i, float* row, float*, bool existed )
+{
+    if( !existed )
+    {
+        const schar* _y = y;
+        int j, len = sample_count;
+        assert( _y && i < sample_count );
+
+        if( _y[i] > 0 )
+        {
+            for( j = 0; j < len; j++ )
+                row[j] = _y[j]*row[j];
+        }
+        else
+        {
+            for( j = 0; j < len; j++ )
+                row[j] = -_y[j]*row[j];
+        }
+    }
+    return row;
+}
+
+
+float* CvSVMSolver::get_row_one_class( int, float* row, float*, bool )
+{
+    return row;
+}
+
+
+float* CvSVMSolver::get_row_svr( int i, float* row, float* dst, bool )
+{
+    int j, len = sample_count;
+    Qfloat* dst_pos = dst;
+    Qfloat* dst_neg = dst + len;
+    if( i >= len )
+    {
+        Qfloat* temp;
+        CV_SWAP( dst_pos, dst_neg, temp );
+    }
+
+    for( j = 0; j < len; j++ )
+    {
+        Qfloat t = row[j];
+        dst_pos[j] = t;
+        dst_neg[j] = -t;
+    }
+    return dst;
+}
+
+
+
+float* CvSVMSolver::get_row( int i, float* dst )
+{
+    bool existed = false;
+    float* row = get_row_base( i, &existed );
+    return (this->*get_row_func)( i, row, dst, existed );
+}
+
+
+#undef is_upper_bound
+#define is_upper_bound(i) (alpha_status[i] > 0)
+
+#undef is_lower_bound
+#define is_lower_bound(i) (alpha_status[i] < 0)
+
+#undef is_free
+#define is_free(i) (alpha_status[i] == 0)
+
+#undef get_C
+#define get_C(i) (C[y[i]>0])
+
+#undef update_alpha_status
+#define update_alpha_status(i) \
+    alpha_status[i] = (schar)(alpha[i] >= get_C(i) ? 1 : alpha[i] <= 0 ? -1 : 0)
+
+#undef reconstruct_gradient
+#define reconstruct_gradient() /* empty for now */
+
+
+bool CvSVMSolver::solve_generic( CvSVMSolutionInfo& si )
+{
+    int iter = 0;
+    int i, j, k;
+
+    // 1. initialize gradient and alpha status
+    for( i = 0; i < alpha_count; i++ )
+    {
+        update_alpha_status(i);
+        G[i] = b[i];
+        if( fabs(G[i]) > 1e200 )
+            return false;
+    }
+
+    for( i = 0; i < alpha_count; i++ )
+    {
+        if( !is_lower_bound(i) )
+        {
+            const Qfloat *Q_i = get_row( i, buf[0] );
+            double alpha_i = alpha[i];
+
+            for( j = 0; j < alpha_count; j++ )
+                G[j] += alpha_i*Q_i[j];
+        }
+    }
+
+    // 2. optimization loop
+    for(;;)
+    {
+        const Qfloat *Q_i, *Q_j;
+        double C_i, C_j;
+        double old_alpha_i, old_alpha_j, alpha_i, alpha_j;
+        double delta_alpha_i, delta_alpha_j;
+
+#ifdef _DEBUG
+        for( i = 0; i < alpha_count; i++ )
+        {
+            if( fabs(G[i]) > 1e+300 )
+                return false;
+
+            if( fabs(alpha[i]) > 1e16 )
+                return false;
+        }
+#endif
+
+        if( (this->*select_working_set_func)( i, j ) != 0 || iter++ >= max_iter )
+            break;
+
+        Q_i = get_row( i, buf[0] );
+        Q_j = get_row( j, buf[1] );
+
+        C_i = get_C(i);
+        C_j = get_C(j);
+
+        alpha_i = old_alpha_i = alpha[i];
+        alpha_j = old_alpha_j = alpha[j];
+
+        if( y[i] != y[j] )
+        {
+            double denom = Q_i[i]+Q_j[j]+2*Q_i[j];
+            double delta = (-G[i]-G[j])/MAX(fabs(denom),FLT_EPSILON);
+            double diff = alpha_i - alpha_j;
+            alpha_i += delta;
+            alpha_j += delta;
+
+            if( diff > 0 && alpha_j < 0 )
+            {
+                alpha_j = 0;
+                alpha_i = diff;
+            }
+            else if( diff <= 0 && alpha_i < 0 )
+            {
+                alpha_i = 0;
+                alpha_j = -diff;
+            }
+
+            if( diff > C_i - C_j && alpha_i > C_i )
+            {
+                alpha_i = C_i;
+                alpha_j = C_i - diff;
+            }
+            else if( diff <= C_i - C_j && alpha_j > C_j )
+            {
+                alpha_j = C_j;
+                alpha_i = C_j + diff;
+            }
+        }
+        else
+        {
+            double denom = Q_i[i]+Q_j[j]-2*Q_i[j];
+            double delta = (G[i]-G[j])/MAX(fabs(denom),FLT_EPSILON);
+            double sum = alpha_i + alpha_j;
+            alpha_i -= delta;
+            alpha_j += delta;
+
+            if( sum > C_i && alpha_i > C_i )
+            {
+                alpha_i = C_i;
+                alpha_j = sum - C_i;
+            }
+            else if( sum <= C_i && alpha_j < 0)
+            {
+                alpha_j = 0;
+                alpha_i = sum;
+            }
+
+            if( sum > C_j && alpha_j > C_j )
+            {
+                alpha_j = C_j;
+                alpha_i = sum - C_j;
+            }
+            else if( sum <= C_j && alpha_i < 0 )
+            {
+                alpha_i = 0;
+                alpha_j = sum;
+            }
+        }
+
+        // update alpha
+        alpha[i] = alpha_i;
+        alpha[j] = alpha_j;
+        update_alpha_status(i);
+        update_alpha_status(j);
+
+        // update G
+        delta_alpha_i = alpha_i - old_alpha_i;
+        delta_alpha_j = alpha_j - old_alpha_j;
+
+        for( k = 0; k < alpha_count; k++ )
+            G[k] += Q_i[k]*delta_alpha_i + Q_j[k]*delta_alpha_j;
+    }
+
+    // calculate rho
+    (this->*calc_rho_func)( si.rho, si.r );
+
+    // calculate objective value
+    for( i = 0, si.obj = 0; i < alpha_count; i++ )
+        si.obj += alpha[i] * (G[i] + b[i]);
+
+    si.obj *= 0.5;
+
+    si.upper_bound_p = C[1];
+    si.upper_bound_n = C[0];
+
+    return true;
+}
+
+
+// return 1 if already optimal, return 0 otherwise
+bool
+CvSVMSolver::select_working_set( int& out_i, int& out_j )
+{
+    // return i,j which maximize -grad(f)^T d , under constraint
+    // if alpha_i == C, d != +1
+    // if alpha_i == 0, d != -1
+    double Gmax1 = -DBL_MAX;        // max { -grad(f)_i * d | y_i*d = +1 }
+    int Gmax1_idx = -1;
+
+    double Gmax2 = -DBL_MAX;        // max { -grad(f)_i * d | y_i*d = -1 }
+    int Gmax2_idx = -1;
+
+    int i;
+
+    for( i = 0; i < alpha_count; i++ )
+    {
+        double t;
+
+        if( y[i] > 0 )    // y = +1
+        {
+            if( !is_upper_bound(i) && (t = -G[i]) > Gmax1 )  // d = +1
+            {
+                Gmax1 = t;
+                Gmax1_idx = i;
+            }
+            if( !is_lower_bound(i) && (t = G[i]) > Gmax2 )  // d = -1
+            {
+                Gmax2 = t;
+                Gmax2_idx = i;
+            }
+        }
+        else        // y = -1
+        {
+            if( !is_upper_bound(i) && (t = -G[i]) > Gmax2 )  // d = +1
+            {
+                Gmax2 = t;
+                Gmax2_idx = i;
+            }
+            if( !is_lower_bound(i) && (t = G[i]) > Gmax1 )  // d = -1
+            {
+                Gmax1 = t;
+                Gmax1_idx = i;
+            }
+        }
+    }
+
+    out_i = Gmax1_idx;
+    out_j = Gmax2_idx;
+
+    return Gmax1 + Gmax2 < eps;
+}
+
+
+void
+CvSVMSolver::calc_rho( double& rho, double& r )
+{
+    int i, nr_free = 0;
+    double ub = DBL_MAX, lb = -DBL_MAX, sum_free = 0;
+
+    for( i = 0; i < alpha_count; i++ )
+    {
+        double yG = y[i]*G[i];
+
+        if( is_lower_bound(i) )
+        {
+            if( y[i] > 0 )
+                ub = MIN(ub,yG);
+            else
+                lb = MAX(lb,yG);
+        }
+        else if( is_upper_bound(i) )
+        {
+            if( y[i] < 0)
+                ub = MIN(ub,yG);
+            else
+                lb = MAX(lb,yG);
+        }
+        else
+        {
+            ++nr_free;
+            sum_free += yG;
+        }
+    }
+
+    rho = nr_free > 0 ? sum_free/nr_free : (ub + lb)*0.5;
+    r = 0;
+}
+
+
+bool
+CvSVMSolver::select_working_set_nu_svm( int& out_i, int& out_j )
+{
+    // return i,j which maximize -grad(f)^T d , under constraint
+    // if alpha_i == C, d != +1
+    // if alpha_i == 0, d != -1
+    double Gmax1 = -DBL_MAX;    // max { -grad(f)_i * d | y_i = +1, d = +1 }
+    int Gmax1_idx = -1;
+
+    double Gmax2 = -DBL_MAX;    // max { -grad(f)_i * d | y_i = +1, d = -1 }
+    int Gmax2_idx = -1;
+
+    double Gmax3 = -DBL_MAX;    // max { -grad(f)_i * d | y_i = -1, d = +1 }
+    int Gmax3_idx = -1;
+
+    double Gmax4 = -DBL_MAX;    // max { -grad(f)_i * d | y_i = -1, d = -1 }
+    int Gmax4_idx = -1;
+
+    int i;
+
+    for( i = 0; i < alpha_count; i++ )
+    {
+        double t;
+
+        if( y[i] > 0 )    // y == +1
+        {
+            if( !is_upper_bound(i) && (t = -G[i]) > Gmax1 )  // d = +1
+            {
+                Gmax1 = t;
+                Gmax1_idx = i;
+            }
+            if( !is_lower_bound(i) && (t = G[i]) > Gmax2 )  // d = -1
+            {
+                Gmax2 = t;
+                Gmax2_idx = i;
+            }
+        }
+        else        // y == -1
+        {
+            if( !is_upper_bound(i) && (t = -G[i]) > Gmax3 )  // d = +1
+            {
+                Gmax3 = t;
+                Gmax3_idx = i;
+            }
+            if( !is_lower_bound(i) && (t = G[i]) > Gmax4 )  // d = -1
+            {
+                Gmax4 = t;
+                Gmax4_idx = i;
+            }
+        }
+    }
+
+    if( MAX(Gmax1 + Gmax2, Gmax3 + Gmax4) < eps )
+        return 1;
+
+    if( Gmax1 + Gmax2 > Gmax3 + Gmax4 )
+    {
+        out_i = Gmax1_idx;
+        out_j = Gmax2_idx;
+    }
+    else
+    {
+        out_i = Gmax3_idx;
+        out_j = Gmax4_idx;
+    }
+    return 0;
+}
+
+
+void
+CvSVMSolver::calc_rho_nu_svm( double& rho, double& r )
+{
+    int nr_free1 = 0, nr_free2 = 0;
+    double ub1 = DBL_MAX, ub2 = DBL_MAX;
+    double lb1 = -DBL_MAX, lb2 = -DBL_MAX;
+    double sum_free1 = 0, sum_free2 = 0;
+    double r1, r2;
+
+    int i;
+
+    for( i = 0; i < alpha_count; i++ )
+    {
+        double G_i = G[i];
+        if( y[i] > 0 )
+        {
+            if( is_lower_bound(i) )
+                ub1 = MIN( ub1, G_i );
+            else if( is_upper_bound(i) )
+                lb1 = MAX( lb1, G_i );
+            else
+            {
+                ++nr_free1;
+                sum_free1 += G_i;
+            }
+        }
+        else
+        {
+            if( is_lower_bound(i) )
+                ub2 = MIN( ub2, G_i );
+            else if( is_upper_bound(i) )
+                lb2 = MAX( lb2, G_i );
+            else
+            {
+                ++nr_free2;
+                sum_free2 += G_i;
+            }
+        }
+    }
+
+    r1 = nr_free1 > 0 ? sum_free1/nr_free1 : (ub1 + lb1)*0.5;
+    r2 = nr_free2 > 0 ? sum_free2/nr_free2 : (ub2 + lb2)*0.5;
+
+    rho = (r1 - r2)*0.5;
+    r = (r1 + r2)*0.5;
+}
+
+
+/*
+///////////////////////// construct and solve various formulations ///////////////////////
+*/
+
+bool CvSVMSolver::solve_c_svc( int _sample_count, int _var_count, const float** _samples, schar* _y,
+                               double _Cp, double _Cn, CvMemStorage* _storage,
+                               CvSVMKernel* _kernel, double* _alpha, CvSVMSolutionInfo& _si )
+{
+    int i;
+
+    if( !create( _sample_count, _var_count, _samples, _y, _sample_count,
+                 _alpha, _Cp, _Cn, _storage, _kernel, &CvSVMSolver::get_row_svc,
+                 &CvSVMSolver::select_working_set, &CvSVMSolver::calc_rho ))
+        return false;
+
+    for( i = 0; i < sample_count; i++ )
+    {
+        alpha[i] = 0;
+        b[i] = -1;
+    }
+
+    if( !solve_generic( _si ))
+        return false;
+
+    for( i = 0; i < sample_count; i++ )
+        alpha[i] *= y[i];
+
+    return true;
+}
+
+
+bool CvSVMSolver::solve_nu_svc( int _sample_count, int _var_count, const float** _samples, schar* _y,
+                                CvMemStorage* _storage, CvSVMKernel* _kernel,
+                                double* _alpha, CvSVMSolutionInfo& _si )
+{
+    int i;
+    double sum_pos, sum_neg, inv_r;
+
+    if( !create( _sample_count, _var_count, _samples, _y, _sample_count,
+                 _alpha, 1., 1., _storage, _kernel, &CvSVMSolver::get_row_svc,
+                 &CvSVMSolver::select_working_set_nu_svm, &CvSVMSolver::calc_rho_nu_svm ))
+        return false;
+
+    sum_pos = kernel->params->nu * sample_count * 0.5;
+    sum_neg = kernel->params->nu * sample_count * 0.5;
+
+    for( i = 0; i < sample_count; i++ )
+    {
+        if( y[i] > 0 )
+        {
+            alpha[i] = MIN(1.0, sum_pos);
+            sum_pos -= alpha[i];
+        }
+        else
+        {
+            alpha[i] = MIN(1.0, sum_neg);
+            sum_neg -= alpha[i];
+        }
+        b[i] = 0;
+    }
+
+    if( !solve_generic( _si ))
+        return false;
+
+    inv_r = 1./_si.r;
+
+    for( i = 0; i < sample_count; i++ )
+        alpha[i] *= y[i]*inv_r;
+
+    _si.rho *= inv_r;
+    _si.obj *= (inv_r*inv_r);
+    _si.upper_bound_p = inv_r;
+    _si.upper_bound_n = inv_r;
+
+    return true;
+}
+
+
+bool CvSVMSolver::solve_one_class( int _sample_count, int _var_count, const float** _samples,
+                                   CvMemStorage* _storage, CvSVMKernel* _kernel,
+                                   double* _alpha, CvSVMSolutionInfo& _si )
+{
+    int i, n;
+    double nu = _kernel->params->nu;
+
+    if( !create( _sample_count, _var_count, _samples, 0, _sample_count,
+                 _alpha, 1., 1., _storage, _kernel, &CvSVMSolver::get_row_one_class,
+                 &CvSVMSolver::select_working_set, &CvSVMSolver::calc_rho ))
+        return false;
+
+    y = (schar*)cvMemStorageAlloc( storage, sample_count*sizeof(y[0]) );
+    n = cvRound( nu*sample_count );
+
+    for( i = 0; i < sample_count; i++ )
+    {
+        y[i] = 1;
+        b[i] = 0;
+        alpha[i] = i < n ? 1 : 0;
+    }
+
+    if( n < sample_count )
+        alpha[n] = nu * sample_count - n;
+    else
+        alpha[n-1] = nu * sample_count - (n-1);
+
+    return solve_generic(_si);
+}
+
+
+bool CvSVMSolver::solve_eps_svr( int _sample_count, int _var_count, const float** _samples,
+                                 const float* _y, CvMemStorage* _storage,
+                                 CvSVMKernel* _kernel, double* _alpha, CvSVMSolutionInfo& _si )
+{
+    int i;
+    double p = _kernel->params->p, kernel_param_c = _kernel->params->C;
+
+    if( !create( _sample_count, _var_count, _samples, 0,
+                 _sample_count*2, 0, kernel_param_c, kernel_param_c, _storage, _kernel, &CvSVMSolver::get_row_svr,
+                 &CvSVMSolver::select_working_set, &CvSVMSolver::calc_rho ))
+        return false;
+
+    y = (schar*)cvMemStorageAlloc( storage, sample_count*2*sizeof(y[0]) );
+    alpha = (double*)cvMemStorageAlloc( storage, alpha_count*sizeof(alpha[0]) );
+
+    for( i = 0; i < sample_count; i++ )
+    {
+        alpha[i] = 0;
+        b[i] = p - _y[i];
+        y[i] = 1;
+
+        alpha[i+sample_count] = 0;
+        b[i+sample_count] = p + _y[i];
+        y[i+sample_count] = -1;
+    }
+
+    if( !solve_generic( _si ))
+        return false;
+
+    for( i = 0; i < sample_count; i++ )
+        _alpha[i] = alpha[i] - alpha[i+sample_count];
+
+    return true;
+}
+
+
+bool CvSVMSolver::solve_nu_svr( int _sample_count, int _var_count, const float** _samples,
+                                const float* _y, CvMemStorage* _storage,
+                                CvSVMKernel* _kernel, double* _alpha, CvSVMSolutionInfo& _si )
+{
+    int i;
+    double kernel_param_c = _kernel->params->C, sum;
+
+    if( !create( _sample_count, _var_count, _samples, 0,
+                 _sample_count*2, 0, 1., 1., _storage, _kernel, &CvSVMSolver::get_row_svr,
+                 &CvSVMSolver::select_working_set_nu_svm, &CvSVMSolver::calc_rho_nu_svm ))
+        return false;
+
+    y = (schar*)cvMemStorageAlloc( storage, sample_count*2*sizeof(y[0]) );
+    alpha = (double*)cvMemStorageAlloc( storage, alpha_count*sizeof(alpha[0]) );
+    sum = kernel_param_c * _kernel->params->nu * sample_count * 0.5;
+
+    for( i = 0; i < sample_count; i++ )
+    {
+        alpha[i] = alpha[i + sample_count] = MIN(sum, kernel_param_c);
+        sum -= alpha[i];
+
+        b[i] = -_y[i];
+        y[i] = 1;
+
+        b[i + sample_count] = _y[i];
+        y[i + sample_count] = -1;
+    }
+
+    if( !solve_generic( _si ))
+        return false;
+
+    for( i = 0; i < sample_count; i++ )
+        _alpha[i] = alpha[i] - alpha[i+sample_count];
+
+    return true;
+}
+
+
+//////////////////////////////////////////////////////////////////////////////////////////
+
+CvSVM::CvSVM()
+{
+    decision_func = 0;
+    class_labels = 0;
+    class_weights = 0;
+    storage = 0;
+    var_idx = 0;
+    kernel = 0;
+    solver = 0;
+    default_model_name = "my_svm";
+
+    clear();
+}
+
+
+CvSVM::~CvSVM()
+{
+    clear();
+}
+
+
+void CvSVM::clear()
+{
+    cvFree( &decision_func );
+    cvReleaseMat( &class_labels );
+    cvReleaseMat( &class_weights );
+    cvReleaseMemStorage( &storage );
+    cvReleaseMat( &var_idx );
+    delete kernel;
+    delete solver;
+    kernel = 0;
+    solver = 0;
+    var_all = 0;
+    sv = 0;
+    sv_total = 0;
+}
+
+
+CvSVM::CvSVM( const CvMat* _train_data, const CvMat* _responses,
+    const CvMat* _var_idx, const CvMat* _sample_idx, CvSVMParams _params )
+{
+    decision_func = 0;
+    class_labels = 0;
+    class_weights = 0;
+    storage = 0;
+    var_idx = 0;
+    kernel = 0;
+    solver = 0;
+    default_model_name = "my_svm";
+
+    train( _train_data, _responses, _var_idx, _sample_idx, _params );
+}
+
+
+int CvSVM::get_support_vector_count() const
+{
+    return sv_total;
+}
+
+
+const float* CvSVM::get_support_vector(int i) const
+{
+    return sv && (unsigned)i < (unsigned)sv_total ? sv[i] : 0;
+}
+
+
+bool CvSVM::set_params( const CvSVMParams& _params )
+{
+    bool ok = false;
+
+    CV_FUNCNAME( "CvSVM::set_params" );
+
+    __BEGIN__;
+
+    int kernel_type, svm_type;
+
+    params = _params;
+
+    kernel_type = params.kernel_type;
+    svm_type = params.svm_type;
+
+    if( kernel_type != LINEAR && kernel_type != POLY &&
+        kernel_type != SIGMOID && kernel_type != RBF )
+        CV_ERROR( CV_StsBadArg, "Unknown/unsupported kernel type" );
+
+    if( kernel_type == LINEAR )
+        params.gamma = 1;
+    else if( params.gamma <= 0 )
+        CV_ERROR( CV_StsOutOfRange, "gamma parameter of the kernel must be positive" );
+
+    if( kernel_type != SIGMOID && kernel_type != POLY )
+        params.coef0 = 0;
+    else if( params.coef0 < 0 )
+        CV_ERROR( CV_StsOutOfRange, "The kernel parameter <coef0> must be positive or zero" );
+
+    if( kernel_type != POLY )
+        params.degree = 0;
+    else if( params.degree <= 0 )
+        CV_ERROR( CV_StsOutOfRange, "The kernel parameter <degree> must be positive" );
+
+    if( svm_type != C_SVC && svm_type != NU_SVC &&
+        svm_type != ONE_CLASS && svm_type != EPS_SVR &&
+        svm_type != NU_SVR )
+        CV_ERROR( CV_StsBadArg, "Unknown/unsupported SVM type" );
+
+    if( svm_type == ONE_CLASS || svm_type == NU_SVC )
+        params.C = 0;
+    else if( params.C <= 0 )
+        CV_ERROR( CV_StsOutOfRange, "The parameter C must be positive" );
+
+    if( svm_type == C_SVC || svm_type == EPS_SVR )
+        params.nu = 0;
+    else if( params.nu <= 0 || params.nu >= 1 )
+        CV_ERROR( CV_StsOutOfRange, "The parameter nu must be between 0 and 1" );
+
+    if( svm_type != EPS_SVR )
+        params.p = 0;
+    else if( params.p <= 0 )
+        CV_ERROR( CV_StsOutOfRange, "The parameter p must be positive" );
+
+    if( svm_type != C_SVC )
+        params.class_weights = 0;
+
+    params.term_crit = cvCheckTermCriteria( params.term_crit, DBL_EPSILON, INT_MAX );
+    params.term_crit.epsilon = MAX( params.term_crit.epsilon, DBL_EPSILON );
+    ok = true;
+
+    __END__;
+
+    return ok;
+}
+
+
+
+void CvSVM::create_kernel()
+{
+    kernel = new CvSVMKernel(&params,0);
+}
+
+
+void CvSVM::create_solver( )
+{
+    solver = new CvSVMSolver;
+}
+
+
+// switching function
+bool CvSVM::train1( int sample_count, int var_count, const float** samples,
+                    const void* _responses, double Cp, double Cn,
+                    CvMemStorage* _storage, double* alpha, double& rho )
+{
+    bool ok = false;
+
+    //CV_FUNCNAME( "CvSVM::train1" );
+
+    __BEGIN__;
+
+    CvSVMSolutionInfo si;
+    int svm_type = params.svm_type;
+
+    si.rho = 0;
+
+    ok = svm_type == C_SVC ? solver->solve_c_svc( sample_count, var_count, samples, (schar*)_responses,
+                                                  Cp, Cn, _storage, kernel, alpha, si ) :
+         svm_type == NU_SVC ? solver->solve_nu_svc( sample_count, var_count, samples, (schar*)_responses,
+                                                    _storage, kernel, alpha, si ) :
+         svm_type == ONE_CLASS ? solver->solve_one_class( sample_count, var_count, samples,
+                                                          _storage, kernel, alpha, si ) :
+         svm_type == EPS_SVR ? solver->solve_eps_svr( sample_count, var_count, samples, (float*)_responses,
+                                                      _storage, kernel, alpha, si ) :
+         svm_type == NU_SVR ? solver->solve_nu_svr( sample_count, var_count, samples, (float*)_responses,
+                                                    _storage, kernel, alpha, si ) : false;
+
+    rho = si.rho;
+
+    __END__;
+
+    return ok;
+}
+
+
+bool CvSVM::do_train( int svm_type, int sample_count, int var_count, const float** samples,
+                    const CvMat* responses, CvMemStorage* temp_storage, double* alpha )
+{
+    bool ok = false;
+
+    CV_FUNCNAME( "CvSVM::do_train" );
+
+    __BEGIN__;
+
+    CvSVMDecisionFunc* df = 0;
+    const int sample_size = var_count*sizeof(samples[0][0]);
+    int i, j, k;
+
+    cvClearMemStorage( storage );
+
+    if( svm_type == ONE_CLASS || svm_type == EPS_SVR || svm_type == NU_SVR )
+    {
+        int sv_count = 0;
+
+        CV_CALL( decision_func = df =
+            (CvSVMDecisionFunc*)cvAlloc( sizeof(df[0]) ));
+
+        df->rho = 0;
+        if( !train1( sample_count, var_count, samples, svm_type == ONE_CLASS ? 0 :
+            responses->data.i, 0, 0, temp_storage, alpha, df->rho ))
+            EXIT;
+
+        for( i = 0; i < sample_count; i++ )
+            sv_count += fabs(alpha[i]) > 0;
+
+        CV_Assert(sv_count != 0);
+
+        sv_total = df->sv_count = sv_count;
+        CV_CALL( df->alpha = (double*)cvMemStorageAlloc( storage, sv_count*sizeof(df->alpha[0])) );
+        CV_CALL( sv = (float**)cvMemStorageAlloc( storage, sv_count*sizeof(sv[0])));
+
+        for( i = k = 0; i < sample_count; i++ )
+        {
+            if( fabs(alpha[i]) > 0 )
+            {
+                CV_CALL( sv[k] = (float*)cvMemStorageAlloc( storage, sample_size ));
+                memcpy( sv[k], samples[i], sample_size );
+                df->alpha[k++] = alpha[i];
+            }
+        }
+    }
+    else
+    {
+        int class_count = class_labels->cols;
+        int* sv_tab = 0;
+        const float** temp_samples = 0;
+        int* class_ranges = 0;
+        schar* temp_y = 0;
+        assert( svm_type == CvSVM::C_SVC || svm_type == CvSVM::NU_SVC );
+
+        if( svm_type == CvSVM::C_SVC && params.class_weights )
+        {
+            const CvMat* cw = params.class_weights;
+
+            if( !CV_IS_MAT(cw) || (cw->cols != 1 && cw->rows != 1) ||
+                cw->rows + cw->cols - 1 != class_count ||
+                (CV_MAT_TYPE(cw->type) != CV_32FC1 && CV_MAT_TYPE(cw->type) != CV_64FC1) )
+                CV_ERROR( CV_StsBadArg, "params.class_weights must be 1d floating-point vector "
+                    "containing as many elements as the number of classes" );
+
+            CV_CALL( class_weights = cvCreateMat( cw->rows, cw->cols, CV_64F ));
+            CV_CALL( cvConvert( cw, class_weights ));
+            CV_CALL( cvScale( class_weights, class_weights, params.C ));
+        }
+
+        CV_CALL( decision_func = df = (CvSVMDecisionFunc*)cvAlloc(
+            (class_count*(class_count-1)/2)*sizeof(df[0])));
+
+        CV_CALL( sv_tab = (int*)cvMemStorageAlloc( temp_storage, sample_count*sizeof(sv_tab[0]) ));
+        memset( sv_tab, 0, sample_count*sizeof(sv_tab[0]) );
+        CV_CALL( class_ranges = (int*)cvMemStorageAlloc( temp_storage,
+                            (class_count + 1)*sizeof(class_ranges[0])));
+        CV_CALL( temp_samples = (const float**)cvMemStorageAlloc( temp_storage,
+                            sample_count*sizeof(temp_samples[0])));
+        CV_CALL( temp_y = (schar*)cvMemStorageAlloc( temp_storage, sample_count));
+
+        class_ranges[class_count] = 0;
+        cvSortSamplesByClasses( samples, responses, class_ranges, 0 );
+        //check that while cross-validation there were the samples from all the classes
+        if( class_ranges[class_count] <= 0 )
+            CV_ERROR( CV_StsBadArg, "While cross-validation one or more of the classes have "
+            "been fell out of the sample. Try to enlarge <CvSVMParams::k_fold>" );
+
+        if( svm_type == NU_SVC )
+        {
+            // check if nu is feasible
+            for(i = 0; i < class_count; i++ )
+            {
+                int ci = class_ranges[i+1] - class_ranges[i];
+                for( j = i+1; j< class_count; j++ )
+                {
+                    int cj = class_ranges[j+1] - class_ranges[j];
+                    if( params.nu*(ci + cj)*0.5 > MIN( ci, cj ) )
+                    {
+                        // !!!TODO!!! add some diagnostic
+                        EXIT; // exit immediately; will release the model and return NULL pointer
+                    }
+                }
+            }
+        }
+
+        // train n*(n-1)/2 classifiers
+        for( i = 0; i < class_count; i++ )
+        {
+            for( j = i+1; j < class_count; j++, df++ )
+            {
+                int si = class_ranges[i], ci = class_ranges[i+1] - si;
+                int sj = class_ranges[j], cj = class_ranges[j+1] - sj;
+                double Cp = params.C, Cn = Cp;
+                int k1 = 0, sv_count = 0;
+
+                for( k = 0; k < ci; k++ )
+                {
+                    temp_samples[k] = samples[si + k];
+                    temp_y[k] = 1;
+                }
+
+                for( k = 0; k < cj; k++ )
+                {
+                    temp_samples[ci + k] = samples[sj + k];
+                    temp_y[ci + k] = -1;
+                }
+
+                if( class_weights )
+                {
+                    Cp = class_weights->data.db[i];
+                    Cn = class_weights->data.db[j];
+                }
+
+                if( !train1( ci + cj, var_count, temp_samples, temp_y,
+                             Cp, Cn, temp_storage, alpha, df->rho ))
+                    EXIT;
+
+                for( k = 0; k < ci + cj; k++ )
+                    sv_count += fabs(alpha[k]) > 0;
+
+                df->sv_count = sv_count;
+
+                CV_CALL( df->alpha = (double*)cvMemStorageAlloc( temp_storage,
+                                                sv_count*sizeof(df->alpha[0])));
+                CV_CALL( df->sv_index = (int*)cvMemStorageAlloc( temp_storage,
+                                                sv_count*sizeof(df->sv_index[0])));
+
+                for( k = 0; k < ci; k++ )
+                {
+                    if( fabs(alpha[k]) > 0 )
+                    {
+                        sv_tab[si + k] = 1;
+                        df->sv_index[k1] = si + k;
+                        df->alpha[k1++] = alpha[k];
+                    }
+                }
+
+                for( k = 0; k < cj; k++ )
+                {
+                    if( fabs(alpha[ci + k]) > 0 )
+                    {
+                        sv_tab[sj + k] = 1;
+                        df->sv_index[k1] = sj + k;
+                        df->alpha[k1++] = alpha[ci + k];
+                    }
+                }
+            }
+        }
+
+        // allocate support vectors and initialize sv_tab
+        for( i = 0, k = 0; i < sample_count; i++ )
+        {
+            if( sv_tab[i] )
+                sv_tab[i] = ++k;
+        }
+
+        sv_total = k;
+        CV_CALL( sv = (float**)cvMemStorageAlloc( storage, sv_total*sizeof(sv[0])));
+
+        for( i = 0, k = 0; i < sample_count; i++ )
+        {
+            if( sv_tab[i] )
+            {
+                CV_CALL( sv[k] = (float*)cvMemStorageAlloc( storage, sample_size ));
+                memcpy( sv[k], samples[i], sample_size );
+                k++;
+            }
+        }
+
+        df = (CvSVMDecisionFunc*)decision_func;
+
+        // set sv pointers
+        for( i = 0; i < class_count; i++ )
+        {
+            for( j = i+1; j < class_count; j++, df++ )
+            {
+                for( k = 0; k < df->sv_count; k++ )
+                {
+                    df->sv_index[k] = sv_tab[df->sv_index[k]]-1;
+                    assert( (unsigned)df->sv_index[k] < (unsigned)sv_total );
+                }
+            }
+        }
+    }
+
+    optimize_linear_svm();
+    ok = true;
+
+    __END__;
+
+    return ok;
+}
+
+
+void CvSVM::optimize_linear_svm()
+{
+    // we optimize only linear SVM: compress all the support vectors into one.
+    if( params.kernel_type != LINEAR )
+        return;
+
+    int class_count = class_labels ? class_labels->cols :
+            params.svm_type == CvSVM::ONE_CLASS ? 1 : 0;
+
+    int i, df_count = class_count > 1 ? class_count*(class_count-1)/2 : 1;
+    CvSVMDecisionFunc* df = decision_func;
+
+    for( i = 0; i < df_count; i++ )
+    {
+        int sv_count = df[i].sv_count;
+        if( sv_count != 1 )
+            break;
+    }
+
+    // if every decision functions uses a single support vector;
+    // it's already compressed. skip it then.
+    if( i == df_count )
+        return;
+
+    int var_count = get_var_count();
+    cv::AutoBuffer<double> vbuf(var_count);
+    double* v = vbuf;
+    float** new_sv = (float**)cvMemStorageAlloc(storage, df_count*sizeof(new_sv[0]));
+
+    for( i = 0; i < df_count; i++ )
+    {
+        new_sv[i] = (float*)cvMemStorageAlloc(storage, var_count*sizeof(new_sv[i][0]));
+        float* dst = new_sv[i];
+        memset(v, 0, var_count*sizeof(v[0]));
+        int j, k, sv_count = df[i].sv_count;
+        for( j = 0; j < sv_count; j++ )
+        {
+            const float* src = class_count > 1 && df[i].sv_index ? sv[df[i].sv_index[j]] : sv[j];
+            double a = df[i].alpha[j];
+            for( k = 0; k < var_count; k++ )
+                v[k] += src[k]*a;
+        }
+        for( k = 0; k < var_count; k++ )
+            dst[k] = (float)v[k];
+        df[i].sv_count = 1;
+        df[i].alpha[0] = 1.;
+        if( class_count > 1 && df[i].sv_index )
+            df[i].sv_index[0] = i;
+    }
+
+    sv = new_sv;
+    sv_total = df_count;
+}
+
+
+bool CvSVM::train( const CvMat* _train_data, const CvMat* _responses,
+    const CvMat* _var_idx, const CvMat* _sample_idx, CvSVMParams _params )
+{
+    bool ok = false;
+    CvMat* responses = 0;
+    CvMemStorage* temp_storage = 0;
+    const float** samples = 0;
+
+    CV_FUNCNAME( "CvSVM::train" );
+
+    __BEGIN__;
+
+    int svm_type, sample_count, var_count, sample_size;
+    int block_size = 1 << 16;
+    double* alpha;
+
+    clear();
+    CV_CALL( set_params( _params ));
+
+    svm_type = _params.svm_type;
+
+    /* Prepare training data and related parameters */
+    CV_CALL( cvPrepareTrainData( "CvSVM::train", _train_data, CV_ROW_SAMPLE,
+                                 svm_type != CvSVM::ONE_CLASS ? _responses : 0,
+                                 svm_type == CvSVM::C_SVC ||
+                                 svm_type == CvSVM::NU_SVC ? CV_VAR_CATEGORICAL :
+                                 CV_VAR_ORDERED, _var_idx, _sample_idx,
+                                 false, &samples, &sample_count, &var_count, &var_all,
+                                 &responses, &class_labels, &var_idx ));
+
+
+    sample_size = var_count*sizeof(samples[0][0]);
+
+    // make the storage block size large enough to fit all
+    // the temporary vectors and output support vectors.
+    block_size = MAX( block_size, sample_count*(int)sizeof(CvSVMKernelRow));
+    block_size = MAX( block_size, sample_count*2*(int)sizeof(double) + 1024 );
+    block_size = MAX( block_size, sample_size*2 + 1024 );
+
+    CV_CALL( storage = cvCreateMemStorage(block_size + sizeof(CvMemBlock) + sizeof(CvSeqBlock)));
+    CV_CALL( temp_storage = cvCreateChildMemStorage(storage));
+    CV_CALL( alpha = (double*)cvMemStorageAlloc(temp_storage, sample_count*sizeof(double)));
+
+    create_kernel();
+    create_solver();
+
+    if( !do_train( svm_type, sample_count, var_count, samples, responses, temp_storage, alpha ))
+        EXIT;
+
+    ok = true; // model has been trained successfully
+
+    __END__;
+
+    delete solver;
+    solver = 0;
+    cvReleaseMemStorage( &temp_storage );
+    cvReleaseMat( &responses );
+    cvFree( &samples );
+
+    if( cvGetErrStatus() < 0 || !ok )
+        clear();
+
+    return ok;
+}
+
+struct indexedratio
+{
+    double val;
+    int ind;
+    int count_smallest, count_biggest;
+    void eval() { val = (double) count_smallest/(count_smallest+count_biggest); }
+};
+
+static int CV_CDECL
+icvCmpIndexedratio( const void* a, const void* b )
+{
+    return ((const indexedratio*)a)->val < ((const indexedratio*)b)->val ? -1
+    : ((const indexedratio*)a)->val > ((const indexedratio*)b)->val ? 1
+    : 0;
+}
+
+bool CvSVM::train_auto( const CvMat* _train_data, const CvMat* _responses,
+    const CvMat* _var_idx, const CvMat* _sample_idx, CvSVMParams _params, int k_fold,
+    CvParamGrid C_grid, CvParamGrid gamma_grid, CvParamGrid p_grid,
+    CvParamGrid nu_grid, CvParamGrid coef_grid, CvParamGrid degree_grid,
+    bool balanced)
+{
+    bool ok = false;
+    CvMat* responses = 0;
+    CvMat* responses_local = 0;
+    CvMemStorage* temp_storage = 0;
+    const float** samples = 0;
+    const float** samples_local = 0;
+
+    CV_FUNCNAME( "CvSVM::train_auto" );
+    __BEGIN__;
+
+    int svm_type, sample_count, var_count, sample_size;
+    int block_size = 1 << 16;
+    double* alpha;
+    RNG* rng = &theRNG();
+
+    // all steps are logarithmic and must be > 1
+    double degree_step = 10, g_step = 10, coef_step = 10, C_step = 10, nu_step = 10, p_step = 10;
+    double gamma = 0, curr_c = 0, degree = 0, coef = 0, p = 0, nu = 0;
+    double best_degree = 0, best_gamma = 0, best_coef = 0, best_C = 0, best_nu = 0, best_p = 0;
+    float min_error = FLT_MAX, error;
+
+    if( _params.svm_type == CvSVM::ONE_CLASS )
+    {
+        if(!train( _train_data, _responses, _var_idx, _sample_idx, _params ))
+            EXIT;
+        return true;
+    }
+
+    clear();
+
+    if( k_fold < 2 )
+        CV_ERROR( CV_StsBadArg, "Parameter <k_fold> must be > 1" );
+
+    CV_CALL(set_params( _params ));
+    svm_type = _params.svm_type;
+
+    // All the parameters except, possibly, <coef0> are positive.
+    // <coef0> is nonnegative
+    if( C_grid.step <= 1 )
+    {
+        C_grid.min_val = C_grid.max_val = params.C;
+        C_grid.step = 10;
+    }
+    else
+        CV_CALL(C_grid.check());
+
+    if( gamma_grid.step <= 1 )
+    {
+        gamma_grid.min_val = gamma_grid.max_val = params.gamma;
+        gamma_grid.step = 10;
+    }
+    else
+        CV_CALL(gamma_grid.check());
+
+    if( p_grid.step <= 1 )
+    {
+        p_grid.min_val = p_grid.max_val = params.p;
+        p_grid.step = 10;
+    }
+    else
+        CV_CALL(p_grid.check());
+
+    if( nu_grid.step <= 1 )
+    {
+        nu_grid.min_val = nu_grid.max_val = params.nu;
+        nu_grid.step = 10;
+    }
+    else
+        CV_CALL(nu_grid.check());
+
+    if( coef_grid.step <= 1 )
+    {
+        coef_grid.min_val = coef_grid.max_val = params.coef0;
+        coef_grid.step = 10;
+    }
+    else
+        CV_CALL(coef_grid.check());
+
+    if( degree_grid.step <= 1 )
+    {
+        degree_grid.min_val = degree_grid.max_val = params.degree;
+        degree_grid.step = 10;
+    }
+    else
+        CV_CALL(degree_grid.check());
+
+    // these parameters are not used:
+    if( params.kernel_type != CvSVM::POLY )
+        degree_grid.min_val = degree_grid.max_val = params.degree;
+    if( params.kernel_type == CvSVM::LINEAR )
+        gamma_grid.min_val = gamma_grid.max_val = params.gamma;
+    if( params.kernel_type != CvSVM::POLY && params.kernel_type != CvSVM::SIGMOID )
+        coef_grid.min_val = coef_grid.max_val = params.coef0;
+    if( svm_type == CvSVM::NU_SVC || svm_type == CvSVM::ONE_CLASS )
+        C_grid.min_val = C_grid.max_val = params.C;
+    if( svm_type == CvSVM::C_SVC || svm_type == CvSVM::EPS_SVR )
+        nu_grid.min_val = nu_grid.max_val = params.nu;
+    if( svm_type != CvSVM::EPS_SVR )
+        p_grid.min_val = p_grid.max_val = params.p;
+
+    CV_ASSERT( g_step > 1 && degree_step > 1 && coef_step > 1);
+    CV_ASSERT( p_step > 1 && C_step > 1 && nu_step > 1 );
+
+    /* Prepare training data and related parameters */
+    CV_CALL(cvPrepareTrainData( "CvSVM::train_auto", _train_data, CV_ROW_SAMPLE,
+                                 svm_type != CvSVM::ONE_CLASS ? _responses : 0,
+                                 svm_type == CvSVM::C_SVC ||
+                                 svm_type == CvSVM::NU_SVC ? CV_VAR_CATEGORICAL :
+                                 CV_VAR_ORDERED, _var_idx, _sample_idx,
+                                 false, &samples, &sample_count, &var_count, &var_all,
+                                 &responses, &class_labels, &var_idx ));
+
+    sample_size = var_count*sizeof(samples[0][0]);
+
+    // make the storage block size large enough to fit all
+    // the temporary vectors and output support vectors.
+    block_size = MAX( block_size, sample_count*(int)sizeof(CvSVMKernelRow));
+    block_size = MAX( block_size, sample_count*2*(int)sizeof(double) + 1024 );
+    block_size = MAX( block_size, sample_size*2 + 1024 );
+
+    CV_CALL( storage = cvCreateMemStorage(block_size + sizeof(CvMemBlock) + sizeof(CvSeqBlock)));
+    CV_CALL(temp_storage = cvCreateChildMemStorage(storage));
+    CV_CALL(alpha = (double*)cvMemStorageAlloc(temp_storage, sample_count*sizeof(double)));
+
+    create_kernel();
+    create_solver();
+
+    {
+    const int testset_size = sample_count/k_fold;
+    const int trainset_size = sample_count - testset_size;
+    const int last_testset_size = sample_count - testset_size*(k_fold-1);
+    const int last_trainset_size = sample_count - last_testset_size;
+    const bool is_regression = (svm_type == EPS_SVR) || (svm_type == NU_SVR);
+
+    size_t resp_elem_size = CV_ELEM_SIZE(responses->type);
+    size_t size = 2*last_trainset_size*sizeof(samples[0]);
+
+    samples_local = (const float**) cvAlloc( size );
+    memset( samples_local, 0, size );
+
+    responses_local = cvCreateMat( 1, trainset_size, CV_MAT_TYPE(responses->type) );
+    cvZero( responses_local );
+
+    // randomly permute samples and responses
+    for(int i = 0; i < sample_count; i++ )
+    {
+        int i1 = (*rng)(sample_count);
+        int i2 = (*rng)(sample_count);
+        const float* temp;
+        float t;
+        int y;
+
+        CV_SWAP( samples[i1], samples[i2], temp );
+        if( is_regression )
+            CV_SWAP( responses->data.fl[i1], responses->data.fl[i2], t );
+        else
+            CV_SWAP( responses->data.i[i1], responses->data.i[i2], y );
+    }
+
+    if (!is_regression && class_labels->cols==2 && balanced)
+    {
+        // count class samples
+        int num_0=0,num_1=0;
+        for (int i=0; i<sample_count; ++i)
+        {
+            if (responses->data.i[i]==class_labels->data.i[0])
+                ++num_0;
+            else
+                ++num_1;
+        }
+
+        int label_smallest_class;
+        int label_biggest_class;
+        if (num_0 < num_1)
+        {
+            label_biggest_class = class_labels->data.i[1];
+            label_smallest_class = class_labels->data.i[0];
+        }
+        else
+        {
+            label_biggest_class = class_labels->data.i[0];
+            label_smallest_class = class_labels->data.i[1];
+            int y;
+            CV_SWAP(num_0,num_1,y);
+        }
+        const double class_ratio = (double) num_0/sample_count;
+        // calculate class ratio of each fold
+        indexedratio *ratios=0;
+        ratios = (indexedratio*) cvAlloc(k_fold*sizeof(*ratios));
+        for (int k=0, i_begin=0; k<k_fold; ++k, i_begin+=testset_size)
+        {
+            int count0=0;
+            int count1=0;
+            int i_end = i_begin + (k<k_fold-1 ? testset_size : last_testset_size);
+            for (int i=i_begin; i<i_end; ++i)
+            {
+                if (responses->data.i[i]==label_smallest_class)
+                    ++count0;
+                else
+                    ++count1;
+            }
+            ratios[k].ind = k;
+            ratios[k].count_smallest = count0;
+            ratios[k].count_biggest = count1;
+            ratios[k].eval();
+        }
+        // initial distance
+        qsort(ratios, k_fold, sizeof(ratios[0]), icvCmpIndexedratio);
+        double old_dist = 0.0;
+        for (int k=0; k<k_fold; ++k)
+            old_dist += std::abs(ratios[k].val-class_ratio);
+        double new_dist = 1.0;
+        // iterate to make the folds more balanced
+        while (new_dist > 0.0)
+        {
+            if (ratios[0].count_biggest==0 || ratios[k_fold-1].count_smallest==0)
+                break; // we are not able to swap samples anymore
+            // what if we swap the samples, calculate the new distance
+            ratios[0].count_smallest++;
+            ratios[0].count_biggest--;
+            ratios[0].eval();
+            ratios[k_fold-1].count_smallest--;
+            ratios[k_fold-1].count_biggest++;
+            ratios[k_fold-1].eval();
+            qsort(ratios, k_fold, sizeof(ratios[0]), icvCmpIndexedratio);
+            new_dist = 0.0;
+            for (int k=0; k<k_fold; ++k)
+                new_dist += std::abs(ratios[k].val-class_ratio);
+            if (new_dist < old_dist)
+            {
+                // swapping really improves, so swap the samples
+                // index of the biggest_class sample from the minimum ratio fold
+                int i1 = ratios[0].ind * testset_size;
+                for ( ; i1<sample_count; ++i1)
+                {
+                    if (responses->data.i[i1]==label_biggest_class)
+                        break;
+                }
+                // index of the smallest_class sample from the maximum ratio fold
+                int i2 = ratios[k_fold-1].ind * testset_size;
+                for ( ; i2<sample_count; ++i2)
+                {
+                    if (responses->data.i[i2]==label_smallest_class)
+                        break;
+                }
+                // swap
+                const float* temp;
+                int y;
+                CV_SWAP( samples[i1], samples[i2], temp );
+                CV_SWAP( responses->data.i[i1], responses->data.i[i2], y );
+                old_dist = new_dist;
+            }
+            else
+                break; // does not improve, so break the loop
+        }
+        cvFree(&ratios);
+    }
+
+    int* cls_lbls = class_labels ? class_labels->data.i : 0;
+    curr_c = C_grid.min_val;
+    do
+    {
+      params.C = curr_c;
+      gamma = gamma_grid.min_val;
+      do
+      {
+        params.gamma = gamma;
+        p = p_grid.min_val;
+        do
+        {
+          params.p = p;
+          nu = nu_grid.min_val;
+          do
+          {
+            params.nu = nu;
+            coef = coef_grid.min_val;
+            do
+            {
+              params.coef0 = coef;
+              degree = degree_grid.min_val;
+              do
+              {
+                params.degree = degree;
+
+                float** test_samples_ptr = (float**)samples;
+                uchar* true_resp = responses->data.ptr;
+                int test_size = testset_size;
+                int train_size = trainset_size;
+
+                error = 0;
+                for(int k = 0; k < k_fold; k++ )
+                {
+                    memcpy( samples_local, samples, sizeof(samples[0])*test_size*k );
+                    memcpy( samples_local + test_size*k, test_samples_ptr + test_size,
+                        sizeof(samples[0])*(sample_count - testset_size*(k+1)) );
+
+                    memcpy( responses_local->data.ptr, responses->data.ptr, resp_elem_size*test_size*k );
+                    memcpy( responses_local->data.ptr + resp_elem_size*test_size*k,
+                        true_resp + resp_elem_size*test_size,
+                        resp_elem_size*(sample_count - testset_size*(k+1)) );
+
+                    if( k == k_fold - 1 )
+                    {
+                        test_size = last_testset_size;
+                        train_size = last_trainset_size;
+                        responses_local->cols = last_trainset_size;
+                    }
+
+                    // Train SVM on <train_size> samples
+                    if( !do_train( svm_type, train_size, var_count,
+                        (const float**)samples_local, responses_local, temp_storage, alpha ) )
+                        EXIT;
+
+                    // Compute test set error on <test_size> samples
+                    for(int i = 0; i < test_size; i++, true_resp += resp_elem_size, test_samples_ptr++ )
+                    {
+                        float resp = predict( *test_samples_ptr, var_count );
+                        error += is_regression ? powf( resp - *(float*)true_resp, 2 )
+                            : ((int)resp != cls_lbls[*(int*)true_resp]);
+                    }
+                }
+                if( min_error > error )
+                {
+                    min_error   = error;
+                    best_degree = degree;
+                    best_gamma  = gamma;
+                    best_coef   = coef;
+                    best_C      = curr_c;
+                    best_nu     = nu;
+                    best_p      = p;
+                }
+                degree *= degree_grid.step;
+              }
+              while( degree < degree_grid.max_val );
+              coef *= coef_grid.step;
+            }
+            while( coef < coef_grid.max_val );
+            nu *= nu_grid.step;
+          }
+          while( nu < nu_grid.max_val );
+          p *= p_grid.step;
+        }
+        while( p < p_grid.max_val );
+        gamma *= gamma_grid.step;
+      }
+      while( gamma < gamma_grid.max_val );
+      curr_c *= C_grid.step;
+    }
+    while( curr_c < C_grid.max_val );
+    }
+
+    min_error /= (float) sample_count;
+
+    params.C      = best_C;
+    params.nu     = best_nu;
+    params.p      = best_p;
+    params.gamma  = best_gamma;
+    params.degree = best_degree;
+    params.coef0  = best_coef;
+
+    CV_CALL(ok = do_train( svm_type, sample_count, var_count, samples, responses, temp_storage, alpha ));
+
+    __END__;
+
+    delete solver;
+    solver = 0;
+    cvReleaseMemStorage( &temp_storage );
+    cvReleaseMat( &responses );
+    cvReleaseMat( &responses_local );
+    cvFree( &samples );
+    cvFree( &samples_local );
+
+    if( cvGetErrStatus() < 0 || !ok )
+        clear();
+
+    return ok;
+}
+
+float CvSVM::predict( const float* row_sample, int row_len, bool returnDFVal ) const
+{
+    assert( kernel );
+    assert( row_sample );
+
+    int var_count = get_var_count();
+    assert( row_len == var_count );
+    (void)row_len;
+
+    int class_count = class_labels ? class_labels->cols :
+                  params.svm_type == ONE_CLASS ? 1 : 0;
+
+    float result = 0;
+    cv::AutoBuffer<float> _buffer(sv_total + (class_count+1)*2);
+    float* buffer = _buffer;
+
+    if( params.svm_type == EPS_SVR ||
+        params.svm_type == NU_SVR ||
+        params.svm_type == ONE_CLASS )
+    {
+        CvSVMDecisionFunc* df = (CvSVMDecisionFunc*)decision_func;
+        int i, sv_count = df->sv_count;
+        double sum = -df->rho;
+
+        kernel->calc( sv_count, var_count, (const float**)sv, row_sample, buffer );
+        for( i = 0; i < sv_count; i++ )
+            sum += buffer[i]*df->alpha[i];
+
+        result = params.svm_type == ONE_CLASS ? (float)(sum > 0) : (float)sum;
+    }
+    else if( params.svm_type == C_SVC ||
+             params.svm_type == NU_SVC )
+    {
+        CvSVMDecisionFunc* df = (CvSVMDecisionFunc*)decision_func;
+        int* vote = (int*)(buffer + sv_total);
+        int i, j, k;
+
+        memset( vote, 0, class_count*sizeof(vote[0]));
+        kernel->calc( sv_total, var_count, (const float**)sv, row_sample, buffer );
+        double sum = 0.;
+
+        for( i = 0; i < class_count; i++ )
+        {
+            for( j = i+1; j < class_count; j++, df++ )
+            {
+                sum = -df->rho;
+                int sv_count = df->sv_count;
+                for( k = 0; k < sv_count; k++ )
+                    sum += df->alpha[k]*buffer[df->sv_index[k]];
+
+                vote[sum > 0 ? i : j]++;
+            }
+        }
+
+        for( i = 1, k = 0; i < class_count; i++ )
+        {
+            if( vote[i] > vote[k] )
+                k = i;
+        }
+        result = returnDFVal && class_count == 2 ? (float)sum : (float)(class_labels->data.i[k]);
+    }
+    else
+        CV_Error( CV_StsBadArg, "INTERNAL ERROR: Unknown SVM type, "
+                                "the SVM structure is probably corrupted" );
+
+    return result;
+}
+
+float CvSVM::predict( const CvMat* sample, bool returnDFVal ) const
+{
+    float result = 0;
+    float* row_sample = 0;
+
+    CV_FUNCNAME( "CvSVM::predict" );
+
+    __BEGIN__;
+
+    int class_count;
+
+    if( !kernel )
+        CV_ERROR( CV_StsBadArg, "The SVM should be trained first" );
+
+    class_count = class_labels ? class_labels->cols :
+                  params.svm_type == ONE_CLASS ? 1 : 0;
+
+    CV_CALL( cvPreparePredictData( sample, var_all, var_idx,
+                                   class_count, 0, &row_sample ));
+    result = predict( row_sample, get_var_count(), returnDFVal );
+
+    __END__;
+
+    if( sample && (!CV_IS_MAT(sample) || sample->data.fl != row_sample) )
+        cvFree( &row_sample );
+
+    return result;
+}
+
+struct predict_body_svm : ParallelLoopBody {
+    predict_body_svm(const CvSVM* _pointer, float* _result, const CvMat* _samples, CvMat* _results)
+    {
+        pointer = _pointer;
+        result = _result;
+        samples = _samples;
+        results = _results;
+    }
+
+    const CvSVM* pointer;
+    float* result;
+    const CvMat* samples;
+    CvMat* results;
+
+    void operator()( const cv::Range& range ) const
+    {
+        for(int i = range.start; i < range.end; i++ )
+        {
+            CvMat sample;
+            cvGetRow( samples, &sample, i );
+            int r = (int)pointer->predict(&sample);
+            if (results)
+                results->data.fl[i] = (float)r;
+            if (i == 0)
+                *result = (float)r;
+    }
+    }
+};
+
+float CvSVM::predict(const CvMat* samples, CV_OUT CvMat* results) const
+{
+    float result = 0;
+    cv::parallel_for_(cv::Range(0, samples->rows),
+             predict_body_svm(this, &result, samples, results)
+    );
+    return result;
+}
+
+void CvSVM::predict( cv::InputArray _samples, cv::OutputArray _results ) const
+{
+    _results.create(_samples.size().height, 1, CV_32F);
+    CvMat samples = _samples.getMat(), results = _results.getMat();
+    predict(&samples, &results);
+}
+
+CvSVM::CvSVM( const Mat& _train_data, const Mat& _responses,
+              const Mat& _var_idx, const Mat& _sample_idx, CvSVMParams _params )
+{
+    decision_func = 0;
+    class_labels = 0;
+    class_weights = 0;
+    storage = 0;
+    var_idx = 0;
+    kernel = 0;
+    solver = 0;
+    default_model_name = "my_svm";
+
+    train( _train_data, _responses, _var_idx, _sample_idx, _params );
+}
+
+bool CvSVM::train( const Mat& _train_data, const Mat& _responses,
+                  const Mat& _var_idx, const Mat& _sample_idx, CvSVMParams _params )
+{
+    CvMat tdata = _train_data, responses = _responses, vidx = _var_idx, sidx = _sample_idx;
+    return train(&tdata, &responses, vidx.data.ptr ? &vidx : 0, sidx.data.ptr ? &sidx : 0, _params);
+}
+
+
+bool CvSVM::train_auto( const Mat& _train_data, const Mat& _responses,
+                       const Mat& _var_idx, const Mat& _sample_idx, CvSVMParams _params, int k_fold,
+                       CvParamGrid C_grid, CvParamGrid gamma_grid, CvParamGrid p_grid,
+                       CvParamGrid nu_grid, CvParamGrid coef_grid, CvParamGrid degree_grid, bool balanced )
+{
+    CvMat tdata = _train_data, responses = _responses, vidx = _var_idx, sidx = _sample_idx;
+    return train_auto(&tdata, &responses, vidx.data.ptr ? &vidx : 0,
+                      sidx.data.ptr ? &sidx : 0, _params, k_fold, C_grid, gamma_grid, p_grid,
+                      nu_grid, coef_grid, degree_grid, balanced);
+}
+
+float CvSVM::predict( const Mat& _sample, bool returnDFVal ) const
+{
+    CvMat sample = _sample;
+    return predict(&sample, returnDFVal);
+}
+
+
+void CvSVM::write_params( CvFileStorage* fs ) const
+{
+    //CV_FUNCNAME( "CvSVM::write_params" );
+
+    __BEGIN__;
+
+    int svm_type = params.svm_type;
+    int kernel_type = params.kernel_type;
+
+    const char* svm_type_str =
+        svm_type == CvSVM::C_SVC ? "C_SVC" :
+        svm_type == CvSVM::NU_SVC ? "NU_SVC" :
+        svm_type == CvSVM::ONE_CLASS ? "ONE_CLASS" :
+        svm_type == CvSVM::EPS_SVR ? "EPS_SVR" :
+        svm_type == CvSVM::NU_SVR ? "NU_SVR" : 0;
+    const char* kernel_type_str =
+        kernel_type == CvSVM::LINEAR ? "LINEAR" :
+        kernel_type == CvSVM::POLY ? "POLY" :
+        kernel_type == CvSVM::RBF ? "RBF" :
+        kernel_type == CvSVM::SIGMOID ? "SIGMOID" : 0;
+
+    if( svm_type_str )
+        cvWriteString( fs, "svm_type", svm_type_str );
+    else
+        cvWriteInt( fs, "svm_type", svm_type );
+
+    // save kernel
+    cvStartWriteStruct( fs, "kernel", CV_NODE_MAP + CV_NODE_FLOW );
+
+    if( kernel_type_str )
+        cvWriteString( fs, "type", kernel_type_str );
+    else
+        cvWriteInt( fs, "type", kernel_type );
+
+    if( kernel_type == CvSVM::POLY || !kernel_type_str )
+        cvWriteReal( fs, "degree", params.degree );
+
+    if( kernel_type != CvSVM::LINEAR || !kernel_type_str )
+        cvWriteReal( fs, "gamma", params.gamma );
+
+    if( kernel_type == CvSVM::POLY || kernel_type == CvSVM::SIGMOID || !kernel_type_str )
+        cvWriteReal( fs, "coef0", params.coef0 );
+
+    cvEndWriteStruct(fs);
+
+    if( svm_type == CvSVM::C_SVC || svm_type == CvSVM::EPS_SVR ||
+        svm_type == CvSVM::NU_SVR || !svm_type_str )
+        cvWriteReal( fs, "C", params.C );
+
+    if( svm_type == CvSVM::NU_SVC || svm_type == CvSVM::ONE_CLASS ||
+        svm_type == CvSVM::NU_SVR || !svm_type_str )
+        cvWriteReal( fs, "nu", params.nu );
+
+    if( svm_type == CvSVM::EPS_SVR || !svm_type_str )
+        cvWriteReal( fs, "p", params.p );
+
+    cvStartWriteStruct( fs, "term_criteria", CV_NODE_MAP + CV_NODE_FLOW );
+    if( params.term_crit.type & CV_TERMCRIT_EPS )
+        cvWriteReal( fs, "epsilon", params.term_crit.epsilon );
+    if( params.term_crit.type & CV_TERMCRIT_ITER )
+        cvWriteInt( fs, "iterations", params.term_crit.max_iter );
+    cvEndWriteStruct( fs );
+
+    __END__;
+}
+
+
+static bool isSvmModelApplicable(int sv_total, int var_all, int var_count, int class_count)
+{
+    return (sv_total > 0 && var_count > 0 && var_count <= var_all && class_count >= 0);
+}
+
+
+void CvSVM::write( CvFileStorage* fs, const char* name ) const
+{
+    CV_FUNCNAME( "CvSVM::write" );
+
+    __BEGIN__;
+
+    int i, var_count = get_var_count(), df_count;
+    int class_count = class_labels ? class_labels->cols :
+                      params.svm_type == CvSVM::ONE_CLASS ? 1 : 0;
+    const CvSVMDecisionFunc* df = decision_func;
+    if( !isSvmModelApplicable(sv_total, var_all, var_count, class_count) )
+    {
+        cvReleaseFileStorage( &fs );
+        fs = NULL;
+
+        CV_ERROR( CV_StsParseError, "SVM model data is invalid, check sv_count, var_* and class_count tags" );
+    }
+
+    cvStartWriteStruct( fs, name, CV_NODE_MAP, CV_TYPE_NAME_ML_SVM );
+
+    write_params( fs );
+
+    cvWriteInt( fs, "var_all", var_all );
+    cvWriteInt( fs, "var_count", var_count );
+
+    if( class_count )
+    {
+        cvWriteInt( fs, "class_count", class_count );
+
+        if( class_labels )
+            cvWrite( fs, "class_labels", class_labels );
+
+        if( class_weights )
+            cvWrite( fs, "class_weights", class_weights );
+    }
+
+    if( var_idx )
+        cvWrite( fs, "var_idx", var_idx );
+
+    // write the joint collection of support vectors
+    cvWriteInt( fs, "sv_total", sv_total );
+    cvStartWriteStruct( fs, "support_vectors", CV_NODE_SEQ );
+    for( i = 0; i < sv_total; i++ )
+    {
+        cvStartWriteStruct( fs, 0, CV_NODE_SEQ + CV_NODE_FLOW );
+        cvWriteRawData( fs, sv[i], var_count, "f" );
+        cvEndWriteStruct( fs );
+    }
+
+    cvEndWriteStruct( fs );
+
+    // write decision functions
+    df_count = class_count > 1 ? class_count*(class_count-1)/2 : 1;
+    df = decision_func;
+
+    cvStartWriteStruct( fs, "decision_functions", CV_NODE_SEQ );
+    for( i = 0; i < df_count; i++ )
+    {
+        int sv_count = df[i].sv_count;
+        cvStartWriteStruct( fs, 0, CV_NODE_MAP );
+        cvWriteInt( fs, "sv_count", sv_count );
+        cvWriteReal( fs, "rho", df[i].rho );
+        cvStartWriteStruct( fs, "alpha", CV_NODE_SEQ+CV_NODE_FLOW );
+        cvWriteRawData( fs, df[i].alpha, df[i].sv_count, "d" );
+        cvEndWriteStruct( fs );
+        if( class_count > 1 )
+        {
+            cvStartWriteStruct( fs, "index", CV_NODE_SEQ+CV_NODE_FLOW );
+            cvWriteRawData( fs, df[i].sv_index, df[i].sv_count, "i" );
+            cvEndWriteStruct( fs );
+        }
+        else
+            CV_ASSERT( sv_count == sv_total );
+        cvEndWriteStruct( fs );
+    }
+    cvEndWriteStruct( fs );
+    cvEndWriteStruct( fs );
+
+    __END__;
+}
+
+
+void CvSVM::read_params( CvFileStorage* fs, CvFileNode* svm_node )
+{
+    CV_FUNCNAME( "CvSVM::read_params" );
+
+    __BEGIN__;
+
+    int svm_type, kernel_type;
+    CvSVMParams _params;
+
+    CvFileNode* tmp_node = cvGetFileNodeByName( fs, svm_node, "svm_type" );
+    CvFileNode* kernel_node;
+    if( !tmp_node )
+        CV_ERROR( CV_StsBadArg, "svm_type tag is not found" );
+
+    if( CV_NODE_TYPE(tmp_node->tag) == CV_NODE_INT )
+        svm_type = cvReadInt( tmp_node, -1 );
+    else
+    {
+        const char* svm_type_str = cvReadString( tmp_node, "" );
+        svm_type =
+            strcmp( svm_type_str, "C_SVC" ) == 0 ? CvSVM::C_SVC :
+            strcmp( svm_type_str, "NU_SVC" ) == 0 ? CvSVM::NU_SVC :
+            strcmp( svm_type_str, "ONE_CLASS" ) == 0 ? CvSVM::ONE_CLASS :
+            strcmp( svm_type_str, "EPS_SVR" ) == 0 ? CvSVM::EPS_SVR :
+            strcmp( svm_type_str, "NU_SVR" ) == 0 ? CvSVM::NU_SVR : -1;
+
+        if( svm_type < 0 )
+            CV_ERROR( CV_StsParseError, "Missing of invalid SVM type" );
+    }
+
+    kernel_node = cvGetFileNodeByName( fs, svm_node, "kernel" );
+    if( !kernel_node )
+        CV_ERROR( CV_StsParseError, "SVM kernel tag is not found" );
+
+    tmp_node = cvGetFileNodeByName( fs, kernel_node, "type" );
+    if( !tmp_node )
+        CV_ERROR( CV_StsParseError, "SVM kernel type tag is not found" );
+
+    if( CV_NODE_TYPE(tmp_node->tag) == CV_NODE_INT )
+        kernel_type = cvReadInt( tmp_node, -1 );
+    else
+    {
+        const char* kernel_type_str = cvReadString( tmp_node, "" );
+        kernel_type =
+            strcmp( kernel_type_str, "LINEAR" ) == 0 ? CvSVM::LINEAR :
+            strcmp( kernel_type_str, "POLY" ) == 0 ? CvSVM::POLY :
+            strcmp( kernel_type_str, "RBF" ) == 0 ? CvSVM::RBF :
+            strcmp( kernel_type_str, "SIGMOID" ) == 0 ? CvSVM::SIGMOID : -1;
+
+        if( kernel_type < 0 )
+            CV_ERROR( CV_StsParseError, "Missing of invalid SVM kernel type" );
+    }
+
+    _params.svm_type = svm_type;
+    _params.kernel_type = kernel_type;
+    _params.degree = cvReadRealByName( fs, kernel_node, "degree", 0 );
+    _params.gamma = cvReadRealByName( fs, kernel_node, "gamma", 0 );
+    _params.coef0 = cvReadRealByName( fs, kernel_node, "coef0", 0 );
+
+    _params.C = cvReadRealByName( fs, svm_node, "C", 0 );
+    _params.nu = cvReadRealByName( fs, svm_node, "nu", 0 );
+    _params.p = cvReadRealByName( fs, svm_node, "p", 0 );
+    _params.class_weights = 0;
+
+    tmp_node = cvGetFileNodeByName( fs, svm_node, "term_criteria" );
+    if( tmp_node )
+    {
+        _params.term_crit.epsilon = cvReadRealByName( fs, tmp_node, "epsilon", -1. );
+        _params.term_crit.max_iter = cvReadIntByName( fs, tmp_node, "iterations", -1 );
+        _params.term_crit.type = (_params.term_crit.epsilon >= 0 ? CV_TERMCRIT_EPS : 0) +
+                               (_params.term_crit.max_iter >= 0 ? CV_TERMCRIT_ITER : 0);
+    }
+    else
+        _params.term_crit = cvTermCriteria( CV_TERMCRIT_EPS + CV_TERMCRIT_ITER, 1000, FLT_EPSILON );
+
+    set_params( _params );
+
+    __END__;
+}
+
+void CvSVM::read( CvFileStorage* fs, CvFileNode* svm_node )
+{
+    const double not_found_dbl = DBL_MAX;
+
+    CV_FUNCNAME( "CvSVM::read" );
+
+    __BEGIN__;
+
+    int i, var_count, df_count, class_count;
+    int block_size = 1 << 16, sv_size;
+    CvFileNode *sv_node, *df_node;
+    CvSVMDecisionFunc* df;
+    CvSeqReader reader;
+
+    if( !svm_node )
+        CV_ERROR( CV_StsParseError, "The requested element is not found" );
+
+    clear();
+
+    // read SVM parameters
+    read_params( fs, svm_node );
+
+    // and top-level data
+    sv_total = cvReadIntByName( fs, svm_node, "sv_total", -1 );
+    var_all = cvReadIntByName( fs, svm_node, "var_all", -1 );
+    var_count = cvReadIntByName( fs, svm_node, "var_count", var_all );
+    class_count = cvReadIntByName( fs, svm_node, "class_count", 0 );
+
+    if( !isSvmModelApplicable(sv_total, var_all, var_count, class_count) )
+        CV_ERROR( CV_StsParseError, "SVM model data is invalid, check sv_count, var_* and class_count tags" );
+
+    CV_CALL( class_labels = (CvMat*)cvReadByName( fs, svm_node, "class_labels" ));
+    CV_CALL( class_weights = (CvMat*)cvReadByName( fs, svm_node, "class_weights" ));
+    CV_CALL( var_idx = (CvMat*)cvReadByName( fs, svm_node, "var_idx" ));
+
+    if( class_count > 1 && (!class_labels ||
+        !CV_IS_MAT(class_labels) || class_labels->cols != class_count))
+        CV_ERROR( CV_StsParseError, "Array of class labels is missing or invalid" );
+
+    if( var_count < var_all && (!var_idx || !CV_IS_MAT(var_idx) || var_idx->cols != var_count) )
+        CV_ERROR( CV_StsParseError, "var_idx array is missing or invalid" );
+
+    // read support vectors
+    sv_node = cvGetFileNodeByName( fs, svm_node, "support_vectors" );
+    if( !sv_node || !CV_NODE_IS_SEQ(sv_node->tag))
+        CV_ERROR( CV_StsParseError, "Missing or invalid sequence of support vectors" );
+
+    block_size = MAX( block_size, sv_total*(int)sizeof(CvSVMKernelRow));
+    block_size = MAX( block_size, sv_total*2*(int)sizeof(double));
+    block_size = MAX( block_size, var_all*(int)sizeof(double));
+
+    CV_CALL( storage = cvCreateMemStorage(block_size + sizeof(CvMemBlock) + sizeof(CvSeqBlock)));
+    CV_CALL( sv = (float**)cvMemStorageAlloc( storage,
+                                sv_total*sizeof(sv[0]) ));
+
+    CV_CALL( cvStartReadSeq( sv_node->data.seq, &reader, 0 ));
+    sv_size = var_count*sizeof(sv[0][0]);
+
+    for( i = 0; i < sv_total; i++ )
+    {
+        CvFileNode* sv_elem = (CvFileNode*)reader.ptr;
+        CV_ASSERT( var_count == 1 || (CV_NODE_IS_SEQ(sv_elem->tag) &&
+                   sv_elem->data.seq->total == var_count) );
+
+        CV_CALL( sv[i] = (float*)cvMemStorageAlloc( storage, sv_size ));
+        CV_CALL( cvReadRawData( fs, sv_elem, sv[i], "f" ));
+        CV_NEXT_SEQ_ELEM( sv_node->data.seq->elem_size, reader );
+    }
+
+    // read decision functions
+    df_count = class_count > 1 ? class_count*(class_count-1)/2 : 1;
+    df_node = cvGetFileNodeByName( fs, svm_node, "decision_functions" );
+    if( !df_node || !CV_NODE_IS_SEQ(df_node->tag) ||
+        df_node->data.seq->total != df_count )
+        CV_ERROR( CV_StsParseError, "decision_functions is missing or is not a collection "
+                  "or has a wrong number of elements" );
+
+    CV_CALL( df = decision_func = (CvSVMDecisionFunc*)cvAlloc( df_count*sizeof(df[0]) ));
+    cvStartReadSeq( df_node->data.seq, &reader, 0 );
+
+    for( i = 0; i < df_count; i++ )
+    {
+        CvFileNode* df_elem = (CvFileNode*)reader.ptr;
+        CvFileNode* alpha_node = cvGetFileNodeByName( fs, df_elem, "alpha" );
+
+        int sv_count = cvReadIntByName( fs, df_elem, "sv_count", -1 );
+        if( sv_count <= 0 )
+            CV_ERROR( CV_StsParseError, "sv_count is missing or non-positive" );
+        df[i].sv_count = sv_count;
+
+        df[i].rho = cvReadRealByName( fs, df_elem, "rho", not_found_dbl );
+        if( fabs(df[i].rho - not_found_dbl) < DBL_EPSILON )
+            CV_ERROR( CV_StsParseError, "rho is missing" );
+
+        if( !alpha_node )
+            CV_ERROR( CV_StsParseError, "alpha is missing in the decision function" );
+
+        CV_CALL( df[i].alpha = (double*)cvMemStorageAlloc( storage,
+                                        sv_count*sizeof(df[i].alpha[0])));
+        CV_ASSERT( sv_count == 1 || (CV_NODE_IS_SEQ(alpha_node->tag) &&
+                   alpha_node->data.seq->total == sv_count) );
+        CV_CALL( cvReadRawData( fs, alpha_node, df[i].alpha, "d" ));
+
+        if( class_count > 1 )
+        {
+            CvFileNode* index_node = cvGetFileNodeByName( fs, df_elem, "index" );
+            if( !index_node )
+                CV_ERROR( CV_StsParseError, "index is missing in the decision function" );
+            CV_CALL( df[i].sv_index = (int*)cvMemStorageAlloc( storage,
+                                            sv_count*sizeof(df[i].sv_index[0])));
+            CV_ASSERT( sv_count == 1 || (CV_NODE_IS_SEQ(index_node->tag) &&
+                   index_node->data.seq->total == sv_count) );
+            CV_CALL( cvReadRawData( fs, index_node, df[i].sv_index, "i" ));
+        }
+        else
+            df[i].sv_index = 0;
+
+        CV_NEXT_SEQ_ELEM( df_node->data.seq->elem_size, reader );
+    }
+
+    if( cvReadIntByName(fs, svm_node, "optimize_linear", 1) != 0 )
+        optimize_linear_svm();
+    create_kernel();
+
+    __END__;
+}
+
+#if 0
+
+static void*
+icvCloneSVM( const void* _src )
+{
+    CvSVMModel* dst = 0;
+
+    CV_FUNCNAME( "icvCloneSVM" );
+
+    __BEGIN__;
+
+    const CvSVMModel* src = (const CvSVMModel*)_src;
+    int var_count, class_count;
+    int i, sv_total, df_count;
+    int sv_size;
+
+    if( !CV_IS_SVM(src) )
+        CV_ERROR( !src ? CV_StsNullPtr : CV_StsBadArg, "Input pointer is NULL or invalid" );
+
+    // 0. create initial CvSVMModel structure
+    CV_CALL( dst = icvCreateSVM() );
+    dst->params = src->params;
+    dst->params.weight_labels = 0;
+    dst->params.weights = 0;
+
+    dst->var_all = src->var_all;
+    if( src->class_labels )
+        dst->class_labels = cvCloneMat( src->class_labels );
+    if( src->class_weights )
+        dst->class_weights = cvCloneMat( src->class_weights );
+    if( src->comp_idx )
+        dst->comp_idx = cvCloneMat( src->comp_idx );
+
+    var_count = src->comp_idx ? src->comp_idx->cols : src->var_all;
+    class_count = src->class_labels ? src->class_labels->cols :
+                  src->params.svm_type == CvSVM::ONE_CLASS ? 1 : 0;
+    sv_total = dst->sv_total = src->sv_total;
+    CV_CALL( dst->storage = cvCreateMemStorage( src->storage->block_size ));
+    CV_CALL( dst->sv = (float**)cvMemStorageAlloc( dst->storage,
+                                    sv_total*sizeof(dst->sv[0]) ));
+
+    sv_size = var_count*sizeof(dst->sv[0][0]);
+
+    for( i = 0; i < sv_total; i++ )
+    {
+        CV_CALL( dst->sv[i] = (float*)cvMemStorageAlloc( dst->storage, sv_size ));
+        memcpy( dst->sv[i], src->sv[i], sv_size );
+    }
+
+    df_count = class_count > 1 ? class_count*(class_count-1)/2 : 1;
+
+    CV_CALL( dst->decision_func = cvAlloc( df_count*sizeof(CvSVMDecisionFunc) ));
+
+    for( i = 0; i < df_count; i++ )
+    {
+        const CvSVMDecisionFunc *sdf =
+            (const CvSVMDecisionFunc*)src->decision_func+i;
+        CvSVMDecisionFunc *ddf =
+            (CvSVMDecisionFunc*)dst->decision_func+i;
+        int sv_count = sdf->sv_count;
+        ddf->sv_count = sv_count;
+        ddf->rho = sdf->rho;
+        CV_CALL( ddf->alpha = (double*)cvMemStorageAlloc( dst->storage,
+                                        sv_count*sizeof(ddf->alpha[0])));
+        memcpy( ddf->alpha, sdf->alpha, sv_count*sizeof(ddf->alpha[0]));
+
+        if( class_count > 1 )
+        {
+            CV_CALL( ddf->sv_index = (int*)cvMemStorageAlloc( dst->storage,
+                                                sv_count*sizeof(ddf->sv_index[0])));
+            memcpy( ddf->sv_index, sdf->sv_index, sv_count*sizeof(ddf->sv_index[0]));
+        }
+        else
+            ddf->sv_index = 0;
+    }
+
+    __END__;
+
+    if( cvGetErrStatus() < 0 && dst )
+        icvReleaseSVM( &dst );
+
+    return dst;
+}
+
+static int icvRegisterSVMType()
+{
+    CvTypeInfo info;
+    memset( &info, 0, sizeof(info) );
+
+    info.flags = 0;
+    info.header_size = sizeof( info );
+    info.is_instance = icvIsSVM;
+    info.release = (CvReleaseFunc)icvReleaseSVM;
+    info.read = icvReadSVM;
+    info.write = icvWriteSVM;
+    info.clone = icvCloneSVM;
+    info.type_name = CV_TYPE_NAME_ML_SVM;
+    cvRegisterType( &info );
+
+    return 1;
+}
+
+
+static int svm = icvRegisterSVMType();
+
+/* The function trains SVM model with optimal parameters, obtained by using cross-validation.
+The parameters to be estimated should be indicated by setting theirs values to FLT_MAX.
+The optimal parameters are saved in <model_params> */
+CV_IMPL CvStatModel*
+cvTrainSVM_CrossValidation( const CvMat* train_data, int tflag,
+            const CvMat* responses,
+            CvStatModelParams* model_params,
+            const CvStatModelParams* cross_valid_params,
+            const CvMat* comp_idx,
+            const CvMat* sample_idx,
+            const CvParamGrid* degree_grid,
+            const CvParamGrid* gamma_grid,
+            const CvParamGrid* coef_grid,
+            const CvParamGrid* C_grid,
+            const CvParamGrid* nu_grid,
+            const CvParamGrid* p_grid )
+{
+    CvStatModel* svm = 0;
+
+    CV_FUNCNAME("cvTainSVMCrossValidation");
+    __BEGIN__;
+
+    double degree_step = 7,
+           g_step      = 15,
+           coef_step   = 14,
+           C_step      = 20,
+           nu_step     = 5,
+           p_step      = 7; // all steps must be > 1
+    double degree_begin = 0.01, degree_end = 2;
+    double g_begin      = 1e-5, g_end      = 0.5;
+    double coef_begin   = 0.1,  coef_end   = 300;
+    double C_begin      = 0.1,  C_end      = 6000;
+    double nu_begin     = 0.01,  nu_end    = 0.4;
+    double p_begin      = 0.01, p_end      = 100;
+
+    double rate = 0, gamma = 0, C = 0, degree = 0, coef = 0, p = 0, nu = 0;
+
+    double best_rate    = 0;
+    double best_degree  = degree_begin;
+    double best_gamma   = g_begin;
+    double best_coef    = coef_begin;
+    double best_C       = C_begin;
+    double best_nu      = nu_begin;
+    double best_p       = p_begin;
+
+    CvSVMModelParams svm_params, *psvm_params;
+    CvCrossValidationParams* cv_params = (CvCrossValidationParams*)cross_valid_params;
+    int svm_type, kernel;
+    int is_regression;
+
+    if( !model_params )
+        CV_ERROR( CV_StsBadArg, "" );
+    if( !cv_params )
+        CV_ERROR( CV_StsBadArg, "" );
+
+    svm_params = *(CvSVMModelParams*)model_params;
+    psvm_params = (CvSVMModelParams*)model_params;
+    svm_type = svm_params.svm_type;
+    kernel = svm_params.kernel_type;
+
+    svm_params.degree = svm_params.degree > 0 ? svm_params.degree : 1;
+    svm_params.gamma = svm_params.gamma > 0 ? svm_params.gamma : 1;
+    svm_params.coef0 = svm_params.coef0 > 0 ? svm_params.coef0 : 1e-6;
+    svm_params.C = svm_params.C > 0 ? svm_params.C : 1;
+    svm_params.nu = svm_params.nu > 0 ? svm_params.nu : 1;
+    svm_params.p = svm_params.p > 0 ? svm_params.p : 1;
+
+    if( degree_grid )
+    {
+        if( !(degree_grid->max_val == 0 && degree_grid->min_val == 0 &&
+              degree_grid->step == 0) )
+        {
+            if( degree_grid->min_val > degree_grid->max_val )
+                CV_ERROR( CV_StsBadArg,
+                "low bound of grid should be less then the upper one");
+            if( degree_grid->step <= 1 )
+                CV_ERROR( CV_StsBadArg, "grid step should be greater 1" );
+            degree_begin = degree_grid->min_val;
+            degree_end   = degree_grid->max_val;
+            degree_step  = degree_grid->step;
+        }
+    }
+    else
+        degree_begin = degree_end = svm_params.degree;
+
+    if( gamma_grid )
+    {
+        if( !(gamma_grid->max_val == 0 && gamma_grid->min_val == 0 &&
+              gamma_grid->step == 0) )
+        {
+            if( gamma_grid->min_val > gamma_grid->max_val )
+                CV_ERROR( CV_StsBadArg,
+                "low bound of grid should be less then the upper one");
+            if( gamma_grid->step <= 1 )
+                CV_ERROR( CV_StsBadArg, "grid step should be greater 1" );
+            g_begin = gamma_grid->min_val;
+            g_end   = gamma_grid->max_val;
+            g_step  = gamma_grid->step;
+        }
+    }
+    else
+        g_begin = g_end = svm_params.gamma;
+
+    if( coef_grid )
+    {
+        if( !(coef_grid->max_val == 0 && coef_grid->min_val == 0 &&
+              coef_grid->step == 0) )
+        {
+            if( coef_grid->min_val > coef_grid->max_val )
+                CV_ERROR( CV_StsBadArg,
+                "low bound of grid should be less then the upper one");
+            if( coef_grid->step <= 1 )
+                CV_ERROR( CV_StsBadArg, "grid step should be greater 1" );
+            coef_begin = coef_grid->min_val;
+            coef_end   = coef_grid->max_val;
+            coef_step  = coef_grid->step;
+        }
+    }
+    else
+        coef_begin = coef_end = svm_params.coef0;
+
+    if( C_grid )
+    {
+        if( !(C_grid->max_val == 0 && C_grid->min_val == 0 && C_grid->step == 0))
+        {
+            if( C_grid->min_val > C_grid->max_val )
+                CV_ERROR( CV_StsBadArg,
+                "low bound of grid should be less then the upper one");
+            if( C_grid->step <= 1 )
+                CV_ERROR( CV_StsBadArg, "grid step should be greater 1" );
+            C_begin = C_grid->min_val;
+            C_end   = C_grid->max_val;
+            C_step  = C_grid->step;
+        }
+    }
+    else
+        C_begin = C_end = svm_params.C;
+
+    if( nu_grid )
+    {
+        if(!(nu_grid->max_val == 0 && nu_grid->min_val == 0 && nu_grid->step==0))
+        {
+            if( nu_grid->min_val > nu_grid->max_val )
+                CV_ERROR( CV_StsBadArg,
+                "low bound of grid should be less then the upper one");
+            if( nu_grid->step <= 1 )
+                CV_ERROR( CV_StsBadArg, "grid step should be greater 1" );
+            nu_begin = nu_grid->min_val;
+            nu_end   = nu_grid->max_val;
+            nu_step  = nu_grid->step;
+        }
+    }
+    else
+        nu_begin = nu_end = svm_params.nu;
+
+    if( p_grid )
+    {
+        if( !(p_grid->max_val == 0 && p_grid->min_val == 0 && p_grid->step == 0))
+        {
+            if( p_grid->min_val > p_grid->max_val )
+                CV_ERROR( CV_StsBadArg,
+                "low bound of grid should be less then the upper one");
+            if( p_grid->step <= 1 )
+                CV_ERROR( CV_StsBadArg, "grid step should be greater 1" );
+            p_begin = p_grid->min_val;
+            p_end   = p_grid->max_val;
+            p_step  = p_grid->step;
+        }
+    }
+    else
+        p_begin = p_end = svm_params.p;
+
+    // these parameters are not used:
+    if( kernel != CvSVM::POLY )
+        degree_begin = degree_end = svm_params.degree;
+
+   if( kernel == CvSVM::LINEAR )
+        g_begin = g_end = svm_params.gamma;
+
+    if( kernel != CvSVM::POLY && kernel != CvSVM::SIGMOID )
+        coef_begin = coef_end = svm_params.coef0;
+
+    if( svm_type == CvSVM::NU_SVC || svm_type == CvSVM::ONE_CLASS )
+        C_begin = C_end = svm_params.C;
+
+    if( svm_type == CvSVM::C_SVC || svm_type == CvSVM::EPS_SVR )
+        nu_begin = nu_end = svm_params.nu;
+
+    if( svm_type != CvSVM::EPS_SVR )
+        p_begin = p_end = svm_params.p;
+
+    is_regression = cv_params->is_regression;
+    best_rate = is_regression ? FLT_MAX : 0;
+
+    assert( g_step > 1 && degree_step > 1 && coef_step > 1);
+    assert( p_step > 1 && C_step > 1 && nu_step > 1 );
+
+    for( degree = degree_begin; degree <= degree_end; degree *= degree_step )
+    {
+      svm_params.degree = degree;
+      //printf("degree = %.3f\n", degree );
+      for( gamma= g_begin; gamma <= g_end; gamma *= g_step )
+      {
+        svm_params.gamma = gamma;
+        //printf("   gamma = %.3f\n", gamma );
+        for( coef = coef_begin; coef <= coef_end; coef *= coef_step )
+        {
+          svm_params.coef0 = coef;
+          //printf("      coef = %.3f\n", coef );
+          for( C = C_begin; C <= C_end; C *= C_step )
+          {
+            svm_params.C = C;
+            //printf("         C = %.3f\n", C );
+            for( nu = nu_begin; nu <= nu_end; nu *= nu_step )
+            {
+              svm_params.nu = nu;
+              //printf("            nu = %.3f\n", nu );
+              for( p = p_begin; p <= p_end; p *= p_step )
+              {
+                int well;
+                svm_params.p = p;
+                //printf("               p = %.3f\n", p );
+
+                CV_CALL(rate = cvCrossValidation( train_data, tflag, responses, &cvTrainSVM,
+                    cross_valid_params, (CvStatModelParams*)&svm_params, comp_idx, sample_idx ));
+
+                well =  rate > best_rate && !is_regression || rate < best_rate && is_regression;
+                if( well || (rate == best_rate && C < best_C) )
+                {
+                    best_rate   = rate;
+                    best_degree = degree;
+                    best_gamma  = gamma;
+                    best_coef   = coef;
+                    best_C      = C;
+                    best_nu     = nu;
+                    best_p      = p;
+                }
+                //printf("                  rate = %.2f\n", rate );
+              }
+            }
+          }
+        }
+      }
+    }
+    //printf("The best:\nrate = %.2f%% degree = %f gamma = %f coef = %f c = %f nu = %f p = %f\n",
+      //  best_rate, best_degree, best_gamma, best_coef, best_C, best_nu, best_p );
+
+    psvm_params->C      = best_C;
+    psvm_params->nu     = best_nu;
+    psvm_params->p      = best_p;
+    psvm_params->gamma  = best_gamma;
+    psvm_params->degree = best_degree;
+    psvm_params->coef0  = best_coef;
+
+    CV_CALL(svm = cvTrainSVM( train_data, tflag, responses, model_params, comp_idx, sample_idx ));
+
+    __END__;
+
+    return svm;
+}
+
+#endif
+
+/* End of file. */
diff --git a/openbr/core/old_ml_tree.cpp b/openbr/core/old_ml_tree.cpp
new file mode 100644
index 0000000..2a60a91
--- /dev/null
+++ b/openbr/core/old_ml_tree.cpp
@@ -0,0 +1,4175 @@
+/*M///////////////////////////////////////////////////////////////////////////////////////
+//
+//  IMPORTANT: READ BEFORE DOWNLOADING, COPYING, INSTALLING OR USING.
+//
+//  By downloading, copying, installing or using the software you agree to this license.
+//  If you do not agree to this license, do not download, install,
+//  copy or use the software.
+//
+//
+//                        Intel License Agreement
+//
+// Copyright (C) 2000, Intel Corporation, all rights reserved.
+// Third party copyrights are property of their respective owners.
+//
+// Redistribution and use in source and binary forms, with or without modification,
+// are permitted provided that the following conditions are met:
+//
+//   * Redistribution's of source code must retain the above copyright notice,
+//     this list of conditions and the following disclaimer.
+//
+//   * Redistribution's in binary form must reproduce the above copyright notice,
+//     this list of conditions and the following disclaimer in the documentation
+//     and/or other materials provided with the distribution.
+//
+//   * The name of Intel Corporation may not be used to endorse or promote products
+//     derived from this software without specific prior written permission.
+//
+// This software is provided by the copyright holders and contributors "as is" and
+// any express or implied warranties, including, but not limited to, the implied
+// warranties of merchantability and fitness for a particular purpose are disclaimed.
+// In no event shall the Intel Corporation or contributors be liable for any direct,
+// indirect, incidental, special, exemplary, or consequential damages
+// (including, but not limited to, procurement of substitute goods or services;
+// loss of use, data, or profits; or business interruption) however caused
+// and on any theory of liability, whether in contract, strict liability,
+// or tort (including negligence or otherwise) arising in any way out of
+// the use of this software, even if advised of the possibility of such damage.
+//
+//M*/
+
+#include "old_ml_precomp.hpp"
+#include <ctype.h>
+
+using namespace cv;
+
+static const float ord_nan = FLT_MAX*0.5f;
+static const int min_block_size = 1 << 16;
+static const int block_size_delta = 1 << 10;
+
+CvDTreeTrainData::CvDTreeTrainData()
+{
+    var_idx = var_type = cat_count = cat_ofs = cat_map =
+        priors = priors_mult = counts = direction = split_buf = responses_copy = 0;
+    buf = 0;
+    tree_storage = temp_storage = 0;
+
+    clear();
+}
+
+
+CvDTreeTrainData::CvDTreeTrainData( const CvMat* _train_data, int _tflag,
+                      const CvMat* _responses, const CvMat* _var_idx,
+                      const CvMat* _sample_idx, const CvMat* _var_type,
+                      const CvMat* _missing_mask, const CvDTreeParams& _params,
+                      bool _shared, bool _add_labels )
+{
+    var_idx = var_type = cat_count = cat_ofs = cat_map =
+        priors = priors_mult = counts = direction = split_buf = responses_copy = 0;
+    buf = 0;
+
+    tree_storage = temp_storage = 0;
+
+    set_data( _train_data, _tflag, _responses, _var_idx, _sample_idx,
+              _var_type, _missing_mask, _params, _shared, _add_labels );
+}
+
+
+CvDTreeTrainData::~CvDTreeTrainData()
+{
+    clear();
+}
+
+
+bool CvDTreeTrainData::set_params( const CvDTreeParams& _params )
+{
+    bool ok = false;
+
+    CV_FUNCNAME( "CvDTreeTrainData::set_params" );
+
+    __BEGIN__;
+
+    // set parameters
+    params = _params;
+
+    if( params.max_categories < 2 )
+        CV_ERROR( CV_StsOutOfRange, "params.max_categories should be >= 2" );
+    params.max_categories = MIN( params.max_categories, 15 );
+
+    if( params.max_depth < 0 )
+        CV_ERROR( CV_StsOutOfRange, "params.max_depth should be >= 0" );
+    params.max_depth = MIN( params.max_depth, 25 );
+
+    params.min_sample_count = MAX(params.min_sample_count,1);
+
+    if( params.cv_folds < 0 )
+        CV_ERROR( CV_StsOutOfRange,
+        "params.cv_folds should be =0 (the tree is not pruned) "
+        "or n>0 (tree is pruned using n-fold cross-validation)" );
+
+    if( params.cv_folds == 1 )
+        params.cv_folds = 0;
+
+    if( params.regression_accuracy < 0 )
+        CV_ERROR( CV_StsOutOfRange, "params.regression_accuracy should be >= 0" );
+
+    ok = true;
+
+    __END__;
+
+    return ok;
+}
+
+template<typename T>
+class LessThanPtr
+{
+public:
+    bool operator()(T* a, T* b) const { return *a < *b; }
+};
+
+template<typename T, typename Idx>
+class LessThanIdx
+{
+public:
+    LessThanIdx( const T* _arr ) : arr(_arr) {}
+    bool operator()(Idx a, Idx b) const { return arr[a] < arr[b]; }
+    const T* arr;
+};
+
+class LessThanPairs
+{
+public:
+    bool operator()(const CvPair16u32s& a, const CvPair16u32s& b) const { return *a.i < *b.i; }
+};
+
+void CvDTreeTrainData::set_data( const CvMat* _train_data, int _tflag,
+    const CvMat* _responses, const CvMat* _var_idx, const CvMat* _sample_idx,
+    const CvMat* _var_type, const CvMat* _missing_mask, const CvDTreeParams& _params,
+    bool _shared, bool _add_labels, bool _update_data )
+{
+    CvMat* sample_indices = 0;
+    CvMat* var_type0 = 0;
+    CvMat* tmp_map = 0;
+    int** int_ptr = 0;
+    CvPair16u32s* pair16u32s_ptr = 0;
+    CvDTreeTrainData* data = 0;
+    float *_fdst = 0;
+    int *_idst = 0;
+    unsigned short* udst = 0;
+    int* idst = 0;
+
+    CV_FUNCNAME( "CvDTreeTrainData::set_data" );
+
+    __BEGIN__;
+
+    int sample_all = 0, r_type, cv_n;
+    int total_c_count = 0;
+    int tree_block_size, temp_block_size, max_split_size, nv_size, cv_size = 0;
+    int ds_step, dv_step, ms_step = 0, mv_step = 0; // {data|mask}{sample|var}_step
+    int vi, i, size;
+    char err[100];
+    const int *sidx = 0, *vidx = 0;
+
+    uint64 effective_buf_size = 0;
+    int effective_buf_height = 0, effective_buf_width = 0;
+
+    if( _update_data && data_root )
+    {
+        data = new CvDTreeTrainData( _train_data, _tflag, _responses, _var_idx,
+            _sample_idx, _var_type, _missing_mask, _params, _shared, _add_labels );
+
+        // compare new and old train data
+        if( !(data->var_count == var_count &&
+            cvNorm( data->var_type, var_type, CV_C ) < FLT_EPSILON &&
+            cvNorm( data->cat_count, cat_count, CV_C ) < FLT_EPSILON &&
+            cvNorm( data->cat_map, cat_map, CV_C ) < FLT_EPSILON) )
+            CV_ERROR( CV_StsBadArg,
+            "The new training data must have the same types and the input and output variables "
+            "and the same categories for categorical variables" );
+
+        cvReleaseMat( &priors );
+        cvReleaseMat( &priors_mult );
+        cvReleaseMat( &buf );
+        cvReleaseMat( &direction );
+        cvReleaseMat( &split_buf );
+        cvReleaseMemStorage( &temp_storage );
+
+        priors = data->priors; data->priors = 0;
+        priors_mult = data->priors_mult; data->priors_mult = 0;
+        buf = data->buf; data->buf = 0;
+        buf_count = data->buf_count; buf_size = data->buf_size;
+        sample_count = data->sample_count;
+
+        direction = data->direction; data->direction = 0;
+        split_buf = data->split_buf; data->split_buf = 0;
+        temp_storage = data->temp_storage; data->temp_storage = 0;
+        nv_heap = data->nv_heap; cv_heap = data->cv_heap;
+
+        data_root = new_node( 0, sample_count, 0, 0 );
+        EXIT;
+    }
+
+    clear();
+
+    var_all = 0;
+    rng = &cv::theRNG();
+
+    CV_CALL( set_params( _params ));
+
+    // check parameter types and sizes
+    CV_CALL( cvCheckTrainData( _train_data, _tflag, _missing_mask, &var_all, &sample_all ));
+
+    train_data = _train_data;
+    responses = _responses;
+
+    if( _tflag == CV_ROW_SAMPLE )
+    {
+        ds_step = _train_data->step/CV_ELEM_SIZE(_train_data->type);
+        dv_step = 1;
+        if( _missing_mask )
+            ms_step = _missing_mask->step, mv_step = 1;
+    }
+    else
+    {
+        dv_step = _train_data->step/CV_ELEM_SIZE(_train_data->type);
+        ds_step = 1;
+        if( _missing_mask )
+            mv_step = _missing_mask->step, ms_step = 1;
+    }
+    tflag = _tflag;
+
+    sample_count = sample_all;
+    var_count = var_all;
+
+    if( _sample_idx )
+    {
+        CV_CALL( sample_indices = cvPreprocessIndexArray( _sample_idx, sample_all ));
+        sidx = sample_indices->data.i;
+        sample_count = sample_indices->rows + sample_indices->cols - 1;
+    }
+
+    if( _var_idx )
+    {
+        CV_CALL( var_idx = cvPreprocessIndexArray( _var_idx, var_all ));
+        vidx = var_idx->data.i;
+        var_count = var_idx->rows + var_idx->cols - 1;
+    }
+
+    is_buf_16u = false;
+    if ( sample_count < 65536 )
+        is_buf_16u = true;
+
+    if( !CV_IS_MAT(_responses) ||
+        (CV_MAT_TYPE(_responses->type) != CV_32SC1 &&
+         CV_MAT_TYPE(_responses->type) != CV_32FC1) ||
+        (_responses->rows != 1 && _responses->cols != 1) ||
+        _responses->rows + _responses->cols - 1 != sample_all )
+        CV_ERROR( CV_StsBadArg, "The array of _responses must be an integer or "
+                  "floating-point vector containing as many elements as "
+                  "the total number of samples in the training data matrix" );
+
+    r_type = CV_VAR_CATEGORICAL;
+    if( _var_type )
+        CV_CALL( var_type0 = cvPreprocessVarType( _var_type, var_idx, var_count, &r_type ));
+
+    CV_CALL( var_type = cvCreateMat( 1, var_count+2, CV_32SC1 ));
+
+    cat_var_count = 0;
+    ord_var_count = -1;
+
+    is_classifier = r_type == CV_VAR_CATEGORICAL;
+
+    // step 0. calc the number of categorical vars
+    for( vi = 0; vi < var_count; vi++ )
+    {
+        char vt = var_type0 ? var_type0->data.ptr[vi] : CV_VAR_ORDERED;
+        var_type->data.i[vi] = vt == CV_VAR_CATEGORICAL ? cat_var_count++ : ord_var_count--;
+    }
+
+    ord_var_count = ~ord_var_count;
+    cv_n = params.cv_folds;
+    // set the two last elements of var_type array to be able
+    // to locate responses and cross-validation labels using
+    // the corresponding get_* functions.
+    var_type->data.i[var_count] = cat_var_count;
+    var_type->data.i[var_count+1] = cat_var_count+1;
+
+    // in case of single ordered predictor we need dummy cv_labels
+    // for safe split_node_data() operation
+    have_labels = cv_n > 0 || (ord_var_count == 1 && cat_var_count == 0) || _add_labels;
+
+    work_var_count = var_count + (is_classifier ? 1 : 0) // for responses class_labels
+                               + (have_labels ? 1 : 0); // for cv_labels
+
+    shared = _shared;
+    buf_count = shared ? 2 : 1;
+
+    buf_size = -1; // the member buf_size is obsolete
+
+    effective_buf_size = (uint64)(work_var_count + 1)*(uint64)sample_count * buf_count; // this is the total size of "CvMat buf" to be allocated
+    effective_buf_width = sample_count;
+    effective_buf_height = work_var_count+1;
+
+    if (effective_buf_width >= effective_buf_height)
+        effective_buf_height *= buf_count;
+    else
+        effective_buf_width *= buf_count;
+
+    if ((uint64)effective_buf_width * (uint64)effective_buf_height != effective_buf_size)
+    {
+        CV_Error(CV_StsBadArg, "The memory buffer cannot be allocated since its size exceeds integer fields limit");
+    }
+
+
+
+    if ( is_buf_16u )
+    {
+        CV_CALL( buf = cvCreateMat( effective_buf_height, effective_buf_width, CV_16UC1 ));
+        CV_CALL( pair16u32s_ptr = (CvPair16u32s*)cvAlloc( sample_count*sizeof(pair16u32s_ptr[0]) ));
+    }
+    else
+    {
+        CV_CALL( buf = cvCreateMat( effective_buf_height, effective_buf_width, CV_32SC1 ));
+        CV_CALL( int_ptr = (int**)cvAlloc( sample_count*sizeof(int_ptr[0]) ));
+    }
+
+    size = is_classifier ? (cat_var_count+1) : cat_var_count;
+    size = !size ? 1 : size;
+    CV_CALL( cat_count = cvCreateMat( 1, size, CV_32SC1 ));
+    CV_CALL( cat_ofs = cvCreateMat( 1, size, CV_32SC1 ));
+
+    size = is_classifier ? (cat_var_count + 1)*params.max_categories : cat_var_count*params.max_categories;
+    size = !size ? 1 : size;
+    CV_CALL( cat_map = cvCreateMat( 1, size, CV_32SC1 ));
+
+    // now calculate the maximum size of split,
+    // create memory storage that will keep nodes and splits of the decision tree
+    // allocate root node and the buffer for the whole training data
+    max_split_size = cvAlign(sizeof(CvDTreeSplit) +
+        (MAX(0,sample_count - 33)/32)*sizeof(int),sizeof(void*));
+    tree_block_size = MAX((int)sizeof(CvDTreeNode)*8, max_split_size);
+    tree_block_size = MAX(tree_block_size + block_size_delta, min_block_size);
+    CV_CALL( tree_storage = cvCreateMemStorage( tree_block_size ));
+    CV_CALL( node_heap = cvCreateSet( 0, sizeof(*node_heap), sizeof(CvDTreeNode), tree_storage ));
+
+    nv_size = var_count*sizeof(int);
+    nv_size = cvAlign(MAX( nv_size, (int)sizeof(CvSetElem) ), sizeof(void*));
+
+    temp_block_size = nv_size;
+
+    if( cv_n )
+    {
+        if( sample_count < cv_n*MAX(params.min_sample_count,10) )
+            CV_ERROR( CV_StsOutOfRange,
+                "The many folds in cross-validation for such a small dataset" );
+
+        cv_size = cvAlign( cv_n*(sizeof(int) + sizeof(double)*2), sizeof(double) );
+        temp_block_size = MAX(temp_block_size, cv_size);
+    }
+
+    temp_block_size = MAX( temp_block_size + block_size_delta, min_block_size );
+    CV_CALL( temp_storage = cvCreateMemStorage( temp_block_size ));
+    CV_CALL( nv_heap = cvCreateSet( 0, sizeof(*nv_heap), nv_size, temp_storage ));
+    if( cv_size )
+        CV_CALL( cv_heap = cvCreateSet( 0, sizeof(*cv_heap), cv_size, temp_storage ));
+
+    CV_CALL( data_root = new_node( 0, sample_count, 0, 0 ));
+
+    max_c_count = 1;
+
+    _fdst = 0;
+    _idst = 0;
+    if (ord_var_count)
+        _fdst = (float*)cvAlloc(sample_count*sizeof(_fdst[0]));
+    if (is_buf_16u && (cat_var_count || is_classifier))
+        _idst = (int*)cvAlloc(sample_count*sizeof(_idst[0]));
+
+    // transform the training data to convenient representation
+    for( vi = 0; vi <= var_count; vi++ )
+    {
+        int ci;
+        const uchar* mask = 0;
+        int64 m_step = 0, step;
+        const int* idata = 0;
+        const float* fdata = 0;
+        int num_valid = 0;
+
+        if( vi < var_count ) // analyze i-th input variable
+        {
+            int vi0 = vidx ? vidx[vi] : vi;
+            ci = get_var_type(vi);
+            step = ds_step; m_step = ms_step;
+            if( CV_MAT_TYPE(_train_data->type) == CV_32SC1 )
+                idata = _train_data->data.i + vi0*dv_step;
+            else
+                fdata = _train_data->data.fl + vi0*dv_step;
+            if( _missing_mask )
+                mask = _missing_mask->data.ptr + vi0*mv_step;
+        }
+        else // analyze _responses
+        {
+            ci = cat_var_count;
+            step = CV_IS_MAT_CONT(_responses->type) ?
+                1 : _responses->step / CV_ELEM_SIZE(_responses->type);
+            if( CV_MAT_TYPE(_responses->type) == CV_32SC1 )
+                idata = _responses->data.i;
+            else
+                fdata = _responses->data.fl;
+        }
+
+        if( (vi < var_count && ci>=0) ||
+            (vi == var_count && is_classifier) ) // process categorical variable or response
+        {
+            int c_count, prev_label;
+            int* c_map;
+
+            if (is_buf_16u)
+                udst = (unsigned short*)(buf->data.s + (size_t)vi*sample_count);
+            else
+                idst = buf->data.i + (size_t)vi*sample_count;
+
+            // copy data
+            for( i = 0; i < sample_count; i++ )
+            {
+                int val = INT_MAX, si = sidx ? sidx[i] : i;
+                if( !mask || !mask[(size_t)si*m_step] )
+                {
+                    if( idata )
+                        val = idata[(size_t)si*step];
+                    else
+                    {
+                        float t = fdata[(size_t)si*step];
+                        val = cvRound(t);
+                        if( fabs(t - val) > FLT_EPSILON )
+                        {
+                            sprintf( err, "%d-th value of %d-th (categorical) "
+                                "variable is not an integer", i, vi );
+                            CV_ERROR( CV_StsBadArg, err );
+                        }
+                    }
+
+                    if( val == INT_MAX )
+                    {
+                        sprintf( err, "%d-th value of %d-th (categorical) "
+                            "variable is too large", i, vi );
+                        CV_ERROR( CV_StsBadArg, err );
+                    }
+                    num_valid++;
+                }
+                if (is_buf_16u)
+                {
+                    _idst[i] = val;
+                    pair16u32s_ptr[i].u = udst + i;
+                    pair16u32s_ptr[i].i = _idst + i;
+                }
+                else
+                {
+                    idst[i] = val;
+                    int_ptr[i] = idst + i;
+                }
+            }
+
+            c_count = num_valid > 0;
+            if (is_buf_16u)
+            {
+                std::sort(pair16u32s_ptr, pair16u32s_ptr + sample_count, LessThanPairs());
+                // count the categories
+                for( i = 1; i < num_valid; i++ )
+                    if (*pair16u32s_ptr[i].i != *pair16u32s_ptr[i-1].i)
+                        c_count ++ ;
+            }
+            else
+            {
+                std::sort(int_ptr, int_ptr + sample_count, LessThanPtr<int>());
+                // count the categories
+                for( i = 1; i < num_valid; i++ )
+                    c_count += *int_ptr[i] != *int_ptr[i-1];
+            }
+
+            if( vi > 0 )
+                max_c_count = MAX( max_c_count, c_count );
+            cat_count->data.i[ci] = c_count;
+            cat_ofs->data.i[ci] = total_c_count;
+
+            // resize cat_map, if need
+            if( cat_map->cols < total_c_count + c_count )
+            {
+                tmp_map = cat_map;
+                CV_CALL( cat_map = cvCreateMat( 1,
+                    MAX(cat_map->cols*3/2,total_c_count+c_count), CV_32SC1 ));
+                for( i = 0; i < total_c_count; i++ )
+                    cat_map->data.i[i] = tmp_map->data.i[i];
+                cvReleaseMat( &tmp_map );
+            }
+
+            c_map = cat_map->data.i + total_c_count;
+            total_c_count += c_count;
+
+            c_count = -1;
+            if (is_buf_16u)
+            {
+                // compact the class indices and build the map
+                prev_label = ~*pair16u32s_ptr[0].i;
+                for( i = 0; i < num_valid; i++ )
+                {
+                    int cur_label = *pair16u32s_ptr[i].i;
+                    if( cur_label != prev_label )
+                        c_map[++c_count] = prev_label = cur_label;
+                    *pair16u32s_ptr[i].u = (unsigned short)c_count;
+                }
+                // replace labels for missing values with -1
+                for( ; i < sample_count; i++ )
+                    *pair16u32s_ptr[i].u = 65535;
+            }
+            else
+            {
+                // compact the class indices and build the map
+                prev_label = ~*int_ptr[0];
+                for( i = 0; i < num_valid; i++ )
+                {
+                    int cur_label = *int_ptr[i];
+                    if( cur_label != prev_label )
+                        c_map[++c_count] = prev_label = cur_label;
+                    *int_ptr[i] = c_count;
+                }
+                // replace labels for missing values with -1
+                for( ; i < sample_count; i++ )
+                    *int_ptr[i] = -1;
+            }
+        }
+        else if( ci < 0 ) // process ordered variable
+        {
+            if (is_buf_16u)
+                udst = (unsigned short*)(buf->data.s + (size_t)vi*sample_count);
+            else
+                idst = buf->data.i + (size_t)vi*sample_count;
+
+            for( i = 0; i < sample_count; i++ )
+            {
+                float val = ord_nan;
+                int si = sidx ? sidx[i] : i;
+                if( !mask || !mask[(size_t)si*m_step] )
+                {
+                    if( idata )
+                        val = (float)idata[(size_t)si*step];
+                    else
+                        val = fdata[(size_t)si*step];
+
+                    if( fabs(val) >= ord_nan )
+                    {
+                        sprintf( err, "%d-th value of %d-th (ordered) "
+                            "variable (=%g) is too large", i, vi, val );
+                        CV_ERROR( CV_StsBadArg, err );
+                    }
+                    num_valid++;
+                }
+
+                if (is_buf_16u)
+                    udst[i] = (unsigned short)i; // TODO: memory corruption may be here
+                else
+                    idst[i] = i;
+                _fdst[i] = val;
+
+            }
+            if (is_buf_16u)
+                std::sort(udst, udst + sample_count, LessThanIdx<float, unsigned short>(_fdst));
+            else
+                std::sort(idst, idst + sample_count, LessThanIdx<float, int>(_fdst));
+        }
+
+        if( vi < var_count )
+            data_root->set_num_valid(vi, num_valid);
+    }
+
+    // set sample labels
+    if (is_buf_16u)
+        udst = (unsigned short*)(buf->data.s + (size_t)work_var_count*sample_count);
+    else
+        idst = buf->data.i + (size_t)work_var_count*sample_count;
+
+    for (i = 0; i < sample_count; i++)
+    {
+        if (udst)
+            udst[i] = sidx ? (unsigned short)sidx[i] : (unsigned short)i;
+        else
+            idst[i] = sidx ? sidx[i] : i;
+    }
+
+    if( cv_n )
+    {
+        unsigned short* usdst = 0;
+        int* idst2 = 0;
+
+        if (is_buf_16u)
+        {
+            usdst = (unsigned short*)(buf->data.s + (size_t)(get_work_var_count()-1)*sample_count);
+            for( i = vi = 0; i < sample_count; i++ )
+            {
+                usdst[i] = (unsigned short)vi++;
+                vi &= vi < cv_n ? -1 : 0;
+            }
+
+            for( i = 0; i < sample_count; i++ )
+            {
+                int a = (*rng)(sample_count);
+                int b = (*rng)(sample_count);
+                unsigned short unsh = (unsigned short)vi;
+                CV_SWAP( usdst[a], usdst[b], unsh );
+            }
+        }
+        else
+        {
+            idst2 = buf->data.i + (size_t)(get_work_var_count()-1)*sample_count;
+            for( i = vi = 0; i < sample_count; i++ )
+            {
+                idst2[i] = vi++;
+                vi &= vi < cv_n ? -1 : 0;
+            }
+
+            for( i = 0; i < sample_count; i++ )
+            {
+                int a = (*rng)(sample_count);
+                int b = (*rng)(sample_count);
+                CV_SWAP( idst2[a], idst2[b], vi );
+            }
+        }
+    }
+
+    if ( cat_map )
+        cat_map->cols = MAX( total_c_count, 1 );
+
+    max_split_size = cvAlign(sizeof(CvDTreeSplit) +
+        (MAX(0,max_c_count - 33)/32)*sizeof(int),sizeof(void*));
+    CV_CALL( split_heap = cvCreateSet( 0, sizeof(*split_heap), max_split_size, tree_storage ));
+
+    have_priors = is_classifier && params.priors;
+    if( is_classifier )
+    {
+        int m = get_num_classes();
+        double sum = 0;
+        CV_CALL( priors = cvCreateMat( 1, m, CV_64F ));
+        for( i = 0; i < m; i++ )
+        {
+            double val = have_priors ? params.priors[i] : 1.;
+            if( val <= 0 )
+                CV_ERROR( CV_StsOutOfRange, "Every class weight should be positive" );
+            priors->data.db[i] = val;
+            sum += val;
+        }
+
+        // normalize weights
+        if( have_priors )
+            cvScale( priors, priors, 1./sum );
+
+        CV_CALL( priors_mult = cvCloneMat( priors ));
+        CV_CALL( counts = cvCreateMat( 1, m, CV_32SC1 ));
+    }
+
+
+    CV_CALL( direction = cvCreateMat( 1, sample_count, CV_8UC1 ));
+    CV_CALL( split_buf = cvCreateMat( 1, sample_count, CV_32SC1 ));
+
+    __END__;
+
+    if( data )
+        delete data;
+
+    if (_fdst)
+        cvFree( &_fdst );
+    if (_idst)
+        cvFree( &_idst );
+    cvFree( &int_ptr );
+    cvFree( &pair16u32s_ptr);
+    cvReleaseMat( &var_type0 );
+    cvReleaseMat( &sample_indices );
+    cvReleaseMat( &tmp_map );
+}
+
+void CvDTreeTrainData::do_responses_copy()
+{
+    responses_copy = cvCreateMat( responses->rows, responses->cols, responses->type );
+    cvCopy( responses, responses_copy);
+    responses = responses_copy;
+}
+
+CvDTreeNode* CvDTreeTrainData::subsample_data( const CvMat* _subsample_idx )
+{
+    CvDTreeNode* root = 0;
+    CvMat* isubsample_idx = 0;
+    CvMat* subsample_co = 0;
+
+    bool isMakeRootCopy = true;
+
+    CV_FUNCNAME( "CvDTreeTrainData::subsample_data" );
+
+    __BEGIN__;
+
+    if( !data_root )
+        CV_ERROR( CV_StsError, "No training data has been set" );
+
+    if( _subsample_idx )
+    {
+        CV_CALL( isubsample_idx = cvPreprocessIndexArray( _subsample_idx, sample_count ));
+
+        if( isubsample_idx->cols + isubsample_idx->rows - 1 == sample_count )
+        {
+            const int* sidx = isubsample_idx->data.i;
+            for( int i = 0; i < sample_count; i++ )
+            {
+                if( sidx[i] != i )
+                {
+                    isMakeRootCopy = false;
+                    break;
+                }
+            }
+        }
+        else
+            isMakeRootCopy = false;
+    }
+
+    if( isMakeRootCopy )
+    {
+        // make a copy of the root node
+        CvDTreeNode temp;
+        int i;
+        root = new_node( 0, 1, 0, 0 );
+        temp = *root;
+        *root = *data_root;
+        root->num_valid = temp.num_valid;
+        if( root->num_valid )
+        {
+            for( i = 0; i < var_count; i++ )
+                root->num_valid[i] = data_root->num_valid[i];
+        }
+        root->cv_Tn = temp.cv_Tn;
+        root->cv_node_risk = temp.cv_node_risk;
+        root->cv_node_error = temp.cv_node_error;
+    }
+    else
+    {
+        int* sidx = isubsample_idx->data.i;
+        // co - array of count/offset pairs (to handle duplicated values in _subsample_idx)
+        int* co, cur_ofs = 0;
+        int vi, i;
+        int workVarCount = get_work_var_count();
+        int count = isubsample_idx->rows + isubsample_idx->cols - 1;
+
+        root = new_node( 0, count, 1, 0 );
+
+        CV_CALL( subsample_co = cvCreateMat( 1, sample_count*2, CV_32SC1 ));
+        cvZero( subsample_co );
+        co = subsample_co->data.i;
+        for( i = 0; i < count; i++ )
+            co[sidx[i]*2]++;
+        for( i = 0; i < sample_count; i++ )
+        {
+            if( co[i*2] )
+            {
+                co[i*2+1] = cur_ofs;
+                cur_ofs += co[i*2];
+            }
+            else
+                co[i*2+1] = -1;
+        }
+
+        cv::AutoBuffer<uchar> inn_buf(sample_count*(2*sizeof(int) + sizeof(float)));
+        for( vi = 0; vi < workVarCount; vi++ )
+        {
+            int ci = get_var_type(vi);
+
+            if( ci >= 0 || vi >= var_count )
+            {
+                int num_valid = 0;
+//SAB                const int* src = CvDTreeTrainData::get_cat_var_data(data_root, vi, (int*)inn_buf.data());
+                const int* src = CvDTreeTrainData::get_cat_var_data(data_root, vi, (int*)(uchar*)inn_buf);
+
+                if (is_buf_16u)
+                {
+                    unsigned short* udst = (unsigned short*)(buf->data.s + root->buf_idx*get_length_subbuf() +
+                        (size_t)vi*sample_count + root->offset);
+                    for( i = 0; i < count; i++ )
+                    {
+                        int val = src[sidx[i]];
+                        udst[i] = (unsigned short)val;
+                        num_valid += val >= 0;
+                    }
+                }
+                else
+                {
+                    int* idst = buf->data.i + root->buf_idx*get_length_subbuf() +
+                        (size_t)vi*sample_count + root->offset;
+                    for( i = 0; i < count; i++ )
+                    {
+                        int val = src[sidx[i]];
+                        idst[i] = val;
+                        num_valid += val >= 0;
+                    }
+                }
+
+                if( vi < var_count )
+                    root->set_num_valid(vi, num_valid);
+            }
+            else
+            {
+//SAB                int *src_idx_buf = (int*)inn_buf.data();
+                int *src_idx_buf = (int*)(uchar*)inn_buf;
+                float *src_val_buf = (float*)(src_idx_buf + sample_count);
+                int* sample_indices_buf = (int*)(src_val_buf + sample_count);
+                const int* src_idx = 0;
+                const float* src_val = 0;
+                get_ord_var_data( data_root, vi, src_val_buf, src_idx_buf, &src_val, &src_idx, sample_indices_buf );
+                int j = 0, idx, count_i;
+                int num_valid = data_root->get_num_valid(vi);
+
+                if (is_buf_16u)
+                {
+                    unsigned short* udst_idx = (unsigned short*)(buf->data.s + root->buf_idx*get_length_subbuf() +
+                        (size_t)vi*sample_count + data_root->offset);
+                    for( i = 0; i < num_valid; i++ )
+                    {
+                        idx = src_idx[i];
+                        count_i = co[idx*2];
+                        if( count_i )
+                            for( cur_ofs = co[idx*2+1]; count_i > 0; count_i--, j++, cur_ofs++ )
+                                udst_idx[j] = (unsigned short)cur_ofs;
+                    }
+
+                    root->set_num_valid(vi, j);
+
+                    for( ; i < sample_count; i++ )
+                    {
+                        idx = src_idx[i];
+                        count_i = co[idx*2];
+                        if( count_i )
+                            for( cur_ofs = co[idx*2+1]; count_i > 0; count_i--, j++, cur_ofs++ )
+                                udst_idx[j] = (unsigned short)cur_ofs;
+                    }
+                }
+                else
+                {
+                    int* idst_idx = buf->data.i + root->buf_idx*get_length_subbuf() +
+                        (size_t)vi*sample_count + root->offset;
+                    for( i = 0; i < num_valid; i++ )
+                    {
+                        idx = src_idx[i];
+                        count_i = co[idx*2];
+                        if( count_i )
+                            for( cur_ofs = co[idx*2+1]; count_i > 0; count_i--, j++, cur_ofs++ )
+                                idst_idx[j] = cur_ofs;
+                    }
+
+                    root->set_num_valid(vi, j);
+
+                    for( ; i < sample_count; i++ )
+                    {
+                        idx = src_idx[i];
+                        count_i = co[idx*2];
+                        if( count_i )
+                            for( cur_ofs = co[idx*2+1]; count_i > 0; count_i--, j++, cur_ofs++ )
+                                idst_idx[j] = cur_ofs;
+                    }
+                }
+            }
+        }
+        // sample indices subsampling
+//SAB        const int* sample_idx_src = get_sample_indices(data_root, (int*)inn_buf.data());
+        const int* sample_idx_src = get_sample_indices(data_root, (int*)(uchar*)inn_buf);
+        if (is_buf_16u)
+        {
+            unsigned short* sample_idx_dst = (unsigned short*)(buf->data.s + root->buf_idx*get_length_subbuf() +
+                (size_t)workVarCount*sample_count + root->offset);
+            for (i = 0; i < count; i++)
+                sample_idx_dst[i] = (unsigned short)sample_idx_src[sidx[i]];
+        }
+        else
+        {
+            int* sample_idx_dst = buf->data.i + root->buf_idx*get_length_subbuf() +
+                (size_t)workVarCount*sample_count + root->offset;
+            for (i = 0; i < count; i++)
+                sample_idx_dst[i] = sample_idx_src[sidx[i]];
+        }
+    }
+
+    __END__;
+
+    cvReleaseMat( &isubsample_idx );
+    cvReleaseMat( &subsample_co );
+
+    return root;
+}
+
+
+void CvDTreeTrainData::get_vectors( const CvMat* _subsample_idx,
+                                    float* values, uchar* missing,
+                                    float* _responses, bool get_class_idx )
+{
+    CvMat* subsample_idx = 0;
+    CvMat* subsample_co = 0;
+
+    CV_FUNCNAME( "CvDTreeTrainData::get_vectors" );
+
+    __BEGIN__;
+
+    int i, vi, total = sample_count, count = total, cur_ofs = 0;
+    int* sidx = 0;
+    int* co = 0;
+
+    cv::AutoBuffer<uchar> inn_buf(sample_count*(2*sizeof(int) + sizeof(float)));
+    if( _subsample_idx )
+    {
+        CV_CALL( subsample_idx = cvPreprocessIndexArray( _subsample_idx, sample_count ));
+        sidx = subsample_idx->data.i;
+        CV_CALL( subsample_co = cvCreateMat( 1, sample_count*2, CV_32SC1 ));
+        co = subsample_co->data.i;
+        cvZero( subsample_co );
+        count = subsample_idx->cols + subsample_idx->rows - 1;
+        for( i = 0; i < count; i++ )
+            co[sidx[i]*2]++;
+        for( i = 0; i < total; i++ )
+        {
+            int count_i = co[i*2];
+            if( count_i )
+            {
+                co[i*2+1] = cur_ofs*var_count;
+                cur_ofs += count_i;
+            }
+        }
+    }
+
+    if( missing )
+        memset( missing, 1, count*var_count );
+
+    for( vi = 0; vi < var_count; vi++ )
+    {
+        int ci = get_var_type(vi);
+        if( ci >= 0 ) // categorical
+        {
+            float* dst = values + vi;
+            uchar* m = missing ? missing + vi : 0;
+//SAB            const int* src = get_cat_var_data(data_root, vi, (int*)inn_buf.data());
+            const int* src = get_cat_var_data(data_root, vi, (int*)(uchar*)inn_buf);
+
+            for( i = 0; i < count; i++, dst += var_count )
+            {
+                int idx = sidx ? sidx[i] : i;
+                int val = src[idx];
+                *dst = (float)val;
+                if( m )
+                {
+                    *m = (!is_buf_16u && val < 0) || (is_buf_16u && (val == 65535));
+                    m += var_count;
+                }
+            }
+        }
+        else // ordered
+        {
+            float* dst = values + vi;
+            uchar* m = missing ? missing + vi : 0;
+            int count1 = data_root->get_num_valid(vi);
+//SAB            float *src_val_buf = (float*)inn_buf.data();
+            float *src_val_buf = (float*)(uchar*)inn_buf;
+            int* src_idx_buf = (int*)(src_val_buf + sample_count);
+            int* sample_indices_buf = src_idx_buf + sample_count;
+            const float *src_val = 0;
+            const int* src_idx = 0;
+            get_ord_var_data(data_root, vi, src_val_buf, src_idx_buf, &src_val, &src_idx, sample_indices_buf);
+
+            for( i = 0; i < count1; i++ )
+            {
+                int idx = src_idx[i];
+                int count_i = 1;
+                if( co )
+                {
+                    count_i = co[idx*2];
+                    cur_ofs = co[idx*2+1];
+                }
+                else
+                    cur_ofs = idx*var_count;
+                if( count_i )
+                {
+                    float val = src_val[i];
+                    for( ; count_i > 0; count_i--, cur_ofs += var_count )
+                    {
+                        dst[cur_ofs] = val;
+                        if( m )
+                            m[cur_ofs] = 0;
+                    }
+                }
+            }
+        }
+    }
+
+    // copy responses
+    if( _responses )
+    {
+        if( is_classifier )
+        {
+//SAB            const int* src = get_class_labels(data_root, (int*)inn_buf.data());
+            const int* src = get_class_labels(data_root, (int*)(uchar*)inn_buf);
+            for( i = 0; i < count; i++ )
+            {
+                int idx = sidx ? sidx[i] : i;
+                int val = get_class_idx ? src[idx] :
+                    cat_map->data.i[cat_ofs->data.i[cat_var_count]+src[idx]];
+                _responses[i] = (float)val;
+            }
+        }
+        else
+        {
+//SAB            float* val_buf = (float*)inn_buf.data();
+            float* val_buf = (float*)(uchar*)inn_buf;
+            int* sample_idx_buf = (int*)(val_buf + sample_count);
+            const float* _values = get_ord_responses(data_root, val_buf, sample_idx_buf);
+            for( i = 0; i < count; i++ )
+            {
+                int idx = sidx ? sidx[i] : i;
+                _responses[i] = _values[idx];
+            }
+        }
+    }
+
+    __END__;
+
+    cvReleaseMat( &subsample_idx );
+    cvReleaseMat( &subsample_co );
+}
+
+
+CvDTreeNode* CvDTreeTrainData::new_node( CvDTreeNode* parent, int count,
+                                         int storage_idx, int offset )
+{
+    CvDTreeNode* node = (CvDTreeNode*)cvSetNew( node_heap );
+
+    node->sample_count = count;
+    node->depth = parent ? parent->depth + 1 : 0;
+    node->parent = parent;
+    node->left = node->right = 0;
+    node->split = 0;
+    node->value = 0;
+    node->class_idx = 0;
+    node->maxlr = 0.;
+
+    node->buf_idx = storage_idx;
+    node->offset = offset;
+    if( nv_heap )
+        node->num_valid = (int*)cvSetNew( nv_heap );
+    else
+        node->num_valid = 0;
+    node->alpha = node->node_risk = node->tree_risk = node->tree_error = 0.;
+    node->complexity = 0;
+
+    if( params.cv_folds > 0 && cv_heap )
+    {
+        int cv_n = params.cv_folds;
+        node->Tn = INT_MAX;
+        node->cv_Tn = (int*)cvSetNew( cv_heap );
+        node->cv_node_risk = (double*)cvAlignPtr(node->cv_Tn + cv_n, sizeof(double));
+        node->cv_node_error = node->cv_node_risk + cv_n;
+    }
+    else
+    {
+        node->Tn = 0;
+        node->cv_Tn = 0;
+        node->cv_node_risk = 0;
+        node->cv_node_error = 0;
+    }
+
+    return node;
+}
+
+
+CvDTreeSplit* CvDTreeTrainData::new_split_ord( int vi, float cmp_val,
+                int split_point, int inversed, float quality )
+{
+    CvDTreeSplit* split = (CvDTreeSplit*)cvSetNew( split_heap );
+    split->var_idx = vi;
+    split->condensed_idx = INT_MIN;
+    split->ord.c = cmp_val;
+    split->ord.split_point = split_point;
+    split->inversed = inversed;
+    split->quality = quality;
+    split->next = 0;
+
+    return split;
+}
+
+
+CvDTreeSplit* CvDTreeTrainData::new_split_cat( int vi, float quality )
+{
+    CvDTreeSplit* split = (CvDTreeSplit*)cvSetNew( split_heap );
+    int i, n = (max_c_count + 31)/32;
+
+    split->var_idx = vi;
+    split->condensed_idx = INT_MIN;
+    split->inversed = 0;
+    split->quality = quality;
+    for( i = 0; i < n; i++ )
+        split->subset[i] = 0;
+    split->next = 0;
+
+    return split;
+}
+
+
+void CvDTreeTrainData::free_node( CvDTreeNode* node )
+{
+    CvDTreeSplit* split = node->split;
+    free_node_data( node );
+    while( split )
+    {
+        CvDTreeSplit* next = split->next;
+        cvSetRemoveByPtr( split_heap, split );
+        split = next;
+    }
+    node->split = 0;
+    cvSetRemoveByPtr( node_heap, node );
+}
+
+
+void CvDTreeTrainData::free_node_data( CvDTreeNode* node )
+{
+    if( node->num_valid )
+    {
+        cvSetRemoveByPtr( nv_heap, node->num_valid );
+        node->num_valid = 0;
+    }
+    // do not free cv_* fields, as all the cross-validation related data is released at once.
+}
+
+
+void CvDTreeTrainData::free_train_data()
+{
+    cvReleaseMat( &counts );
+    cvReleaseMat( &buf );
+    cvReleaseMat( &direction );
+    cvReleaseMat( &split_buf );
+    cvReleaseMemStorage( &temp_storage );
+    cvReleaseMat( &responses_copy );
+    cv_heap = nv_heap = 0;
+}
+
+
+void CvDTreeTrainData::clear()
+{
+    free_train_data();
+
+    cvReleaseMemStorage( &tree_storage );
+
+    cvReleaseMat( &var_idx );
+    cvReleaseMat( &var_type );
+    cvReleaseMat( &cat_count );
+    cvReleaseMat( &cat_ofs );
+    cvReleaseMat( &cat_map );
+    cvReleaseMat( &priors );
+    cvReleaseMat( &priors_mult );
+
+    node_heap = split_heap = 0;
+
+    sample_count = var_all = var_count = max_c_count = ord_var_count = cat_var_count = 0;
+    have_labels = have_priors = is_classifier = false;
+
+    buf_count = buf_size = 0;
+    shared = false;
+
+    data_root = 0;
+
+    rng = &cv::theRNG();
+}
+
+
+int CvDTreeTrainData::get_num_classes() const
+{
+    return is_classifier ? cat_count->data.i[cat_var_count] : 0;
+}
+
+
+int CvDTreeTrainData::get_var_type(int vi) const
+{
+    return var_type->data.i[vi];
+}
+
+void CvDTreeTrainData::get_ord_var_data( CvDTreeNode* n, int vi, float* ord_values_buf, int* sorted_indices_buf,
+                                         const float** ord_values, const int** sorted_indices, int* sample_indices_buf )
+{
+    int vidx = var_idx ? var_idx->data.i[vi] : vi;
+    int node_sample_count = n->sample_count;
+    int td_step = train_data->step/CV_ELEM_SIZE(train_data->type);
+
+    const int* sample_indices = get_sample_indices(n, sample_indices_buf);
+
+    if( !is_buf_16u )
+        *sorted_indices = buf->data.i + n->buf_idx*get_length_subbuf() +
+        (size_t)vi*sample_count + n->offset;
+    else {
+        const unsigned short* short_indices = (const unsigned short*)(buf->data.s + n->buf_idx*get_length_subbuf() +
+            (size_t)vi*sample_count + n->offset );
+        for( int i = 0; i < node_sample_count; i++ )
+            sorted_indices_buf[i] = short_indices[i];
+        *sorted_indices = sorted_indices_buf;
+    }
+
+    if( tflag == CV_ROW_SAMPLE )
+    {
+        for( int i = 0; i < node_sample_count &&
+            ((((*sorted_indices)[i] >= 0) && !is_buf_16u) || (((*sorted_indices)[i] != 65535) && is_buf_16u)); i++ )
+        {
+            int idx = (*sorted_indices)[i];
+            idx = sample_indices[idx];
+            ord_values_buf[i] = *(train_data->data.fl + idx * td_step + vidx);
+        }
+    }
+    else
+        for( int i = 0; i < node_sample_count &&
+            ((((*sorted_indices)[i] >= 0) && !is_buf_16u) || (((*sorted_indices)[i] != 65535) && is_buf_16u)); i++ )
+        {
+            int idx = (*sorted_indices)[i];
+            idx = sample_indices[idx];
+            ord_values_buf[i] = *(train_data->data.fl + vidx* td_step + idx);
+        }
+
+    *ord_values = ord_values_buf;
+}
+
+
+const int* CvDTreeTrainData::get_class_labels( CvDTreeNode* n, int* labels_buf )
+{
+    if (is_classifier)
+        return get_cat_var_data( n, var_count, labels_buf);
+    return 0;
+}
+
+const int* CvDTreeTrainData::get_sample_indices( CvDTreeNode* n, int* indices_buf )
+{
+    return get_cat_var_data( n, get_work_var_count(), indices_buf );
+}
+
+const float* CvDTreeTrainData::get_ord_responses( CvDTreeNode* n, float* values_buf, int*sample_indices_buf )
+{
+    int _sample_count = n->sample_count;
+    int r_step = CV_IS_MAT_CONT(responses->type) ? 1 : responses->step/CV_ELEM_SIZE(responses->type);
+    const int* indices = get_sample_indices(n, sample_indices_buf);
+
+    for( int i = 0; i < _sample_count &&
+        (((indices[i] >= 0) && !is_buf_16u) || ((indices[i] != 65535) && is_buf_16u)); i++ )
+    {
+        int idx = indices[i];
+        values_buf[i] = *(responses->data.fl + idx * r_step);
+    }
+
+    return values_buf;
+}
+
+
+const int* CvDTreeTrainData::get_cv_labels( CvDTreeNode* n, int* labels_buf )
+{
+    if (have_labels)
+        return get_cat_var_data( n, get_work_var_count()- 1, labels_buf);
+    return 0;
+}
+
+
+const int* CvDTreeTrainData::get_cat_var_data( CvDTreeNode* n, int vi, int* cat_values_buf)
+{
+    const int* cat_values = 0;
+    if( !is_buf_16u )
+        cat_values = buf->data.i + n->buf_idx*get_length_subbuf() +
+            (size_t)vi*sample_count + n->offset;
+    else {
+        const unsigned short* short_values = (const unsigned short*)(buf->data.s + n->buf_idx*get_length_subbuf() +
+            (size_t)vi*sample_count + n->offset);
+        for( int i = 0; i < n->sample_count; i++ )
+            cat_values_buf[i] = short_values[i];
+        cat_values = cat_values_buf;
+    }
+    return cat_values;
+}
+
+
+int CvDTreeTrainData::get_child_buf_idx( CvDTreeNode* n )
+{
+    int idx = n->buf_idx + 1;
+    if( idx >= buf_count )
+        idx = shared ? 1 : 0;
+    return idx;
+}
+
+
+void CvDTreeTrainData::write_params( CvFileStorage* fs ) const
+{
+    CV_FUNCNAME( "CvDTreeTrainData::write_params" );
+
+    __BEGIN__;
+
+    int vi, vcount = var_count;
+
+    cvWriteInt( fs, "is_classifier", is_classifier ? 1 : 0 );
+    cvWriteInt( fs, "var_all", var_all );
+    cvWriteInt( fs, "var_count", var_count );
+    cvWriteInt( fs, "ord_var_count", ord_var_count );
+    cvWriteInt( fs, "cat_var_count", cat_var_count );
+
+    cvStartWriteStruct( fs, "training_params", CV_NODE_MAP );
+    cvWriteInt( fs, "use_surrogates", params.use_surrogates ? 1 : 0 );
+
+    if( is_classifier )
+    {
+        cvWriteInt( fs, "max_categories", params.max_categories );
+    }
+    else
+    {
+        cvWriteReal( fs, "regression_accuracy", params.regression_accuracy );
+    }
+
+    cvWriteInt( fs, "max_depth", params.max_depth );
+    cvWriteInt( fs, "min_sample_count", params.min_sample_count );
+    cvWriteInt( fs, "cross_validation_folds", params.cv_folds );
+
+    if( params.cv_folds > 1 )
+    {
+        cvWriteInt( fs, "use_1se_rule", params.use_1se_rule ? 1 : 0 );
+        cvWriteInt( fs, "truncate_pruned_tree", params.truncate_pruned_tree ? 1 : 0 );
+    }
+
+    if( priors )
+        cvWrite( fs, "priors", priors );
+
+    cvEndWriteStruct( fs );
+
+    if( var_idx )
+        cvWrite( fs, "var_idx", var_idx );
+
+    cvStartWriteStruct( fs, "var_type", CV_NODE_SEQ+CV_NODE_FLOW );
+
+    for( vi = 0; vi < vcount; vi++ )
+        cvWriteInt( fs, 0, var_type->data.i[vi] >= 0 );
+
+    cvEndWriteStruct( fs );
+
+    if( cat_count && (cat_var_count > 0 || is_classifier) )
+    {
+        CV_ASSERT( cat_count != 0 );
+        cvWrite( fs, "cat_count", cat_count );
+        cvWrite( fs, "cat_map", cat_map );
+    }
+
+    __END__;
+}
+
+
+void CvDTreeTrainData::read_params( CvFileStorage* fs, CvFileNode* node )
+{
+    CV_FUNCNAME( "CvDTreeTrainData::read_params" );
+
+    __BEGIN__;
+
+    CvFileNode *tparams_node, *vartype_node;
+    CvSeqReader reader;
+    int vi, max_split_size, tree_block_size;
+
+    is_classifier = (cvReadIntByName( fs, node, "is_classifier" ) != 0);
+    var_all = cvReadIntByName( fs, node, "var_all" );
+    var_count = cvReadIntByName( fs, node, "var_count", var_all );
+    cat_var_count = cvReadIntByName( fs, node, "cat_var_count" );
+    ord_var_count = cvReadIntByName( fs, node, "ord_var_count" );
+
+    tparams_node = cvGetFileNodeByName( fs, node, "training_params" );
+
+    if( tparams_node ) // training parameters are not necessary
+    {
+        params.use_surrogates = cvReadIntByName( fs, tparams_node, "use_surrogates", 1 ) != 0;
+
+        if( is_classifier )
+        {
+            params.max_categories = cvReadIntByName( fs, tparams_node, "max_categories" );
+        }
+        else
+        {
+            params.regression_accuracy =
+                (float)cvReadRealByName( fs, tparams_node, "regression_accuracy" );
+        }
+
+        params.max_depth = cvReadIntByName( fs, tparams_node, "max_depth" );
+        params.min_sample_count = cvReadIntByName( fs, tparams_node, "min_sample_count" );
+        params.cv_folds = cvReadIntByName( fs, tparams_node, "cross_validation_folds" );
+
+        if( params.cv_folds > 1 )
+        {
+            params.use_1se_rule = cvReadIntByName( fs, tparams_node, "use_1se_rule" ) != 0;
+            params.truncate_pruned_tree =
+                cvReadIntByName( fs, tparams_node, "truncate_pruned_tree" ) != 0;
+        }
+
+        priors = (CvMat*)cvReadByName( fs, tparams_node, "priors" );
+        if( priors )
+        {
+            if( !CV_IS_MAT(priors) )
+                CV_ERROR( CV_StsParseError, "priors must stored as a matrix" );
+            priors_mult = cvCloneMat( priors );
+        }
+    }
+
+    CV_CALL( var_idx = (CvMat*)cvReadByName( fs, node, "var_idx" ));
+    if( var_idx )
+    {
+        if( !CV_IS_MAT(var_idx) ||
+            (var_idx->cols != 1 && var_idx->rows != 1) ||
+            var_idx->cols + var_idx->rows - 1 != var_count ||
+            CV_MAT_TYPE(var_idx->type) != CV_32SC1 )
+            CV_ERROR( CV_StsParseError,
+                "var_idx (if exist) must be valid 1d integer vector containing <var_count> elements" );
+
+        for( vi = 0; vi < var_count; vi++ )
+            if( (unsigned)var_idx->data.i[vi] >= (unsigned)var_all )
+                CV_ERROR( CV_StsOutOfRange, "some of var_idx elements are out of range" );
+    }
+
+    ////// read var type
+    CV_CALL( var_type = cvCreateMat( 1, var_count + 2, CV_32SC1 ));
+
+    cat_var_count = 0;
+    ord_var_count = -1;
+    vartype_node = cvGetFileNodeByName( fs, node, "var_type" );
+
+    if( vartype_node && CV_NODE_TYPE(vartype_node->tag) == CV_NODE_INT && var_count == 1 )
+        var_type->data.i[0] = vartype_node->data.i ? cat_var_count++ : ord_var_count--;
+    else
+    {
+        if( !vartype_node || CV_NODE_TYPE(vartype_node->tag) != CV_NODE_SEQ ||
+            vartype_node->data.seq->total != var_count )
+            CV_ERROR( CV_StsParseError, "var_type must exist and be a sequence of 0's and 1's" );
+
+        cvStartReadSeq( vartype_node->data.seq, &reader );
+
+        for( vi = 0; vi < var_count; vi++ )
+        {
+            CvFileNode* n = (CvFileNode*)reader.ptr;
+            if( CV_NODE_TYPE(n->tag) != CV_NODE_INT || (n->data.i & ~1) )
+                CV_ERROR( CV_StsParseError, "var_type must exist and be a sequence of 0's and 1's" );
+            var_type->data.i[vi] = n->data.i ? cat_var_count++ : ord_var_count--;
+            CV_NEXT_SEQ_ELEM( reader.seq->elem_size, reader );
+        }
+    }
+    var_type->data.i[var_count] = cat_var_count;
+
+    ord_var_count = ~ord_var_count;
+    //////
+
+    if( cat_var_count > 0 || is_classifier )
+    {
+        int ccount, total_c_count = 0;
+        CV_CALL( cat_count = (CvMat*)cvReadByName( fs, node, "cat_count" ));
+        CV_CALL( cat_map = (CvMat*)cvReadByName( fs, node, "cat_map" ));
+
+        if( !CV_IS_MAT(cat_count) || !CV_IS_MAT(cat_map) ||
+            (cat_count->cols != 1 && cat_count->rows != 1) ||
+            CV_MAT_TYPE(cat_count->type) != CV_32SC1 ||
+            cat_count->cols + cat_count->rows - 1 != cat_var_count + is_classifier ||
+            (cat_map->cols != 1 && cat_map->rows != 1) ||
+            CV_MAT_TYPE(cat_map->type) != CV_32SC1 )
+            CV_ERROR( CV_StsParseError,
+            "Both cat_count and cat_map must exist and be valid 1d integer vectors of an appropriate size" );
+
+        ccount = cat_var_count + is_classifier;
+
+        CV_CALL( cat_ofs = cvCreateMat( 1, ccount + 1, CV_32SC1 ));
+        cat_ofs->data.i[0] = 0;
+        max_c_count = 1;
+
+        for( vi = 0; vi < ccount; vi++ )
+        {
+            int val = cat_count->data.i[vi];
+            if( val <= 0 )
+                CV_ERROR( CV_StsOutOfRange, "some of cat_count elements are out of range" );
+            max_c_count = MAX( max_c_count, val );
+            cat_ofs->data.i[vi+1] = total_c_count += val;
+        }
+
+        if( cat_map->cols + cat_map->rows - 1 != total_c_count )
+            CV_ERROR( CV_StsBadSize,
+            "cat_map vector length is not equal to the total number of categories in all categorical vars" );
+    }
+
+    max_split_size = cvAlign(sizeof(CvDTreeSplit) +
+        (MAX(0,max_c_count - 33)/32)*sizeof(int),sizeof(void*));
+
+    tree_block_size = MAX((int)sizeof(CvDTreeNode)*8, max_split_size);
+    tree_block_size = MAX(tree_block_size + block_size_delta, min_block_size);
+    CV_CALL( tree_storage = cvCreateMemStorage( tree_block_size ));
+    CV_CALL( node_heap = cvCreateSet( 0, sizeof(node_heap[0]),
+            sizeof(CvDTreeNode), tree_storage ));
+    CV_CALL( split_heap = cvCreateSet( 0, sizeof(split_heap[0]),
+            max_split_size, tree_storage ));
+
+    __END__;
+}
+
+/////////////////////// Decision Tree /////////////////////////
+CvDTreeParams::CvDTreeParams() : max_categories(10), max_depth(INT_MAX), min_sample_count(10),
+    cv_folds(10), use_surrogates(true), use_1se_rule(true),
+    truncate_pruned_tree(true), regression_accuracy(0.01f), priors(0)
+{}
+
+CvDTreeParams::CvDTreeParams( int _max_depth, int _min_sample_count,
+                              float _regression_accuracy, bool _use_surrogates,
+                              int _max_categories, int _cv_folds,
+                              bool _use_1se_rule, bool _truncate_pruned_tree,
+                              const float* _priors ) :
+    max_categories(_max_categories), max_depth(_max_depth),
+    min_sample_count(_min_sample_count), cv_folds (_cv_folds),
+    use_surrogates(_use_surrogates), use_1se_rule(_use_1se_rule),
+    truncate_pruned_tree(_truncate_pruned_tree),
+    regression_accuracy(_regression_accuracy),
+    priors(_priors)
+{}
+
+CvDTree::CvDTree()
+{
+    data = 0;
+    var_importance = 0;
+    default_model_name = "my_tree";
+
+    clear();
+}
+
+
+void CvDTree::clear()
+{
+    cvReleaseMat( &var_importance );
+    if( data )
+    {
+        if( !data->shared )
+            delete data;
+        else
+            free_tree();
+        data = 0;
+    }
+    root = 0;
+    pruned_tree_idx = -1;
+}
+
+
+CvDTree::~CvDTree()
+{
+    clear();
+}
+
+
+const CvDTreeNode* CvDTree::get_root() const
+{
+    return root;
+}
+
+
+int CvDTree::get_pruned_tree_idx() const
+{
+    return pruned_tree_idx;
+}
+
+
+CvDTreeTrainData* CvDTree::get_data()
+{
+    return data;
+}
+
+
+bool CvDTree::train( const CvMat* _train_data, int _tflag,
+                     const CvMat* _responses, const CvMat* _var_idx,
+                     const CvMat* _sample_idx, const CvMat* _var_type,
+                     const CvMat* _missing_mask, CvDTreeParams _params )
+{
+    bool result = false;
+
+    CV_FUNCNAME( "CvDTree::train" );
+
+    __BEGIN__;
+
+    clear();
+    data = new CvDTreeTrainData( _train_data, _tflag, _responses,
+                                 _var_idx, _sample_idx, _var_type,
+                                 _missing_mask, _params, false );
+    CV_CALL( result = do_train(0) );
+
+    __END__;
+
+    return result;
+}
+
+bool CvDTree::train( const Mat& _train_data, int _tflag,
+                    const Mat& _responses, const Mat& _var_idx,
+                    const Mat& _sample_idx, const Mat& _var_type,
+                    const Mat& _missing_mask, CvDTreeParams _params )
+{
+    train_data_hdr = _train_data;
+    train_data_mat = _train_data;
+    responses_hdr = _responses;
+    responses_mat = _responses;
+
+    CvMat vidx=_var_idx, sidx=_sample_idx, vtype=_var_type, mmask=_missing_mask;
+
+    return train(&train_data_hdr, _tflag, &responses_hdr, vidx.data.ptr ? &vidx : 0, sidx.data.ptr ? &sidx : 0,
+                 vtype.data.ptr ? &vtype : 0, mmask.data.ptr ? &mmask : 0, _params);
+}
+
+
+bool CvDTree::train( CvMLData* _data, CvDTreeParams _params )
+{
+   bool result = false;
+
+    CV_FUNCNAME( "CvDTree::train" );
+
+    __BEGIN__;
+
+    const CvMat* values = _data->get_values();
+    const CvMat* response = _data->get_responses();
+    const CvMat* missing = _data->get_missing();
+    const CvMat* var_types = _data->get_var_types();
+    const CvMat* train_sidx = _data->get_train_sample_idx();
+    const CvMat* var_idx = _data->get_var_idx();
+
+    CV_CALL( result = train( values, CV_ROW_SAMPLE, response, var_idx,
+        train_sidx, var_types, missing, _params ) );
+
+    __END__;
+
+    return result;
+}
+
+bool CvDTree::train( CvDTreeTrainData* _data, const CvMat* _subsample_idx )
+{
+    bool result = false;
+
+    CV_FUNCNAME( "CvDTree::train" );
+
+    __BEGIN__;
+
+    clear();
+    data = _data;
+    data->shared = true;
+    CV_CALL( result = do_train(_subsample_idx));
+
+    __END__;
+
+    return result;
+}
+
+
+bool CvDTree::do_train( const CvMat* _subsample_idx )
+{
+    bool result = false;
+
+    CV_FUNCNAME( "CvDTree::do_train" );
+
+    __BEGIN__;
+
+    root = data->subsample_data( _subsample_idx );
+
+    CV_CALL( try_split_node(root));
+
+    if( root->split )
+    {
+        CV_Assert( root->left );
+        CV_Assert( root->right );
+
+        if( data->params.cv_folds > 0 )
+            CV_CALL( prune_cv() );
+
+        if( !data->shared )
+            data->free_train_data();
+
+        result = true;
+    }
+
+    __END__;
+
+    return result;
+}
+
+
+void CvDTree::try_split_node( CvDTreeNode* node )
+{
+    CvDTreeSplit* best_split = 0;
+    int i, n = node->sample_count, vi;
+    bool can_split = true;
+    double quality_scale;
+
+    calc_node_value( node );
+
+    if( node->sample_count <= data->params.min_sample_count ||
+        node->depth >= data->params.max_depth )
+        can_split = false;
+
+    if( can_split && data->is_classifier )
+    {
+        // check if we have a "pure" node,
+        // we assume that cls_count is filled by calc_node_value()
+        int* cls_count = data->counts->data.i;
+        int nz = 0, m = data->get_num_classes();
+        for( i = 0; i < m; i++ )
+            nz += cls_count[i] != 0;
+        if( nz == 1 ) // there is only one class
+            can_split = false;
+    }
+    else if( can_split )
+    {
+        if( sqrt(node->node_risk)/n < data->params.regression_accuracy )
+            can_split = false;
+    }
+
+    if( can_split )
+    {
+        best_split = find_best_split(node);
+        // TODO: check the split quality ...
+        node->split = best_split;
+    }
+    if( !can_split || !best_split )
+    {
+        data->free_node_data(node);
+        return;
+    }
+
+    quality_scale = calc_node_dir( node );
+    if( data->params.use_surrogates )
+    {
+        // find all the surrogate splits
+        // and sort them by their similarity to the primary one
+        for( vi = 0; vi < data->var_count; vi++ )
+        {
+            CvDTreeSplit* split;
+            int ci = data->get_var_type(vi);
+
+            if( vi == best_split->var_idx )
+                continue;
+
+            if( ci >= 0 )
+                split = find_surrogate_split_cat( node, vi );
+            else
+                split = find_surrogate_split_ord( node, vi );
+
+            if( split )
+            {
+                // insert the split
+                CvDTreeSplit* prev_split = node->split;
+                split->quality = (float)(split->quality*quality_scale);
+
+                while( prev_split->next &&
+                       prev_split->next->quality > split->quality )
+                    prev_split = prev_split->next;
+                split->next = prev_split->next;
+                prev_split->next = split;
+            }
+        }
+    }
+    split_node_data( node );
+    try_split_node( node->left );
+    try_split_node( node->right );
+}
+
+
+// calculate direction (left(-1),right(1),missing(0))
+// for each sample using the best split
+// the function returns scale coefficients for surrogate split quality factors.
+// the scale is applied to normalize surrogate split quality relatively to the
+// best (primary) split quality. That is, if a surrogate split is absolutely
+// identical to the primary split, its quality will be set to the maximum value =
+// quality of the primary split; otherwise, it will be lower.
+// besides, the function compute node->maxlr,
+// minimum possible quality (w/o considering the above mentioned scale)
+// for a surrogate split. Surrogate splits with quality less than node->maxlr
+// are not discarded.
+double CvDTree::calc_node_dir( CvDTreeNode* node )
+{
+    char* dir = (char*)data->direction->data.ptr;
+    int i, n = node->sample_count, vi = node->split->var_idx;
+    double L, R;
+
+    assert( !node->split->inversed );
+
+    if( data->get_var_type(vi) >= 0 ) // split on categorical var
+    {
+        cv::AutoBuffer<int> inn_buf(n*(!data->have_priors ? 1 : 2));
+//SAB        int* labels_buf = inn_buf.data();
+        int* labels_buf = inn_buf;
+        const int* labels = data->get_cat_var_data( node, vi, labels_buf );
+        const int* subset = node->split->subset;
+        if( !data->have_priors )
+        {
+            int sum = 0, sum_abs = 0;
+
+            for( i = 0; i < n; i++ )
+            {
+                int idx = labels[i];
+                int d = ( ((idx >= 0)&&(!data->is_buf_16u)) || ((idx != 65535)&&(data->is_buf_16u)) ) ?
+                    CV_DTREE_CAT_DIR(idx,subset) : 0;
+                sum += d; sum_abs += d & 1;
+                dir[i] = (char)d;
+            }
+
+            R = (sum_abs + sum) >> 1;
+            L = (sum_abs - sum) >> 1;
+        }
+        else
+        {
+            const double* priors = data->priors_mult->data.db;
+            double sum = 0, sum_abs = 0;
+            int* responses_buf = labels_buf + n;
+            const int* responses = data->get_class_labels(node, responses_buf);
+
+            for( i = 0; i < n; i++ )
+            {
+                int idx = labels[i];
+                double w = priors[responses[i]];
+                int d = idx >= 0 ? CV_DTREE_CAT_DIR(idx,subset) : 0;
+                sum += d*w; sum_abs += (d & 1)*w;
+                dir[i] = (char)d;
+            }
+
+            R = (sum_abs + sum) * 0.5;
+            L = (sum_abs - sum) * 0.5;
+        }
+    }
+    else // split on ordered var
+    {
+        int split_point = node->split->ord.split_point;
+        int n1 = node->get_num_valid(vi);
+        cv::AutoBuffer<uchar> inn_buf(n*(sizeof(int)*(data->have_priors ? 3 : 2) + sizeof(float)));
+//SAB        float* val_buf = (float*)inn_buf.data();
+        float* val_buf = (float*)(uchar*)inn_buf;
+        int* sorted_buf = (int*)(val_buf + n);
+        int* sample_idx_buf = sorted_buf + n;
+        const float* val = 0;
+        const int* sorted = 0;
+        data->get_ord_var_data( node, vi, val_buf, sorted_buf, &val, &sorted, sample_idx_buf);
+
+        assert( 0 <= split_point && split_point < n1-1 );
+
+        if( !data->have_priors )
+        {
+            for( i = 0; i <= split_point; i++ )
+                dir[sorted[i]] = (char)-1;
+            for( ; i < n1; i++ )
+                dir[sorted[i]] = (char)1;
+            for( ; i < n; i++ )
+                dir[sorted[i]] = (char)0;
+
+            L = split_point-1;
+            R = n1 - split_point + 1;
+        }
+        else
+        {
+            const double* priors = data->priors_mult->data.db;
+            int* responses_buf = sample_idx_buf + n;
+            const int* responses = data->get_class_labels(node, responses_buf);
+            L = R = 0;
+
+            for( i = 0; i <= split_point; i++ )
+            {
+                int idx = sorted[i];
+                double w = priors[responses[idx]];
+                dir[idx] = (char)-1;
+                L += w;
+            }
+
+            for( ; i < n1; i++ )
+            {
+                int idx = sorted[i];
+                double w = priors[responses[idx]];
+                dir[idx] = (char)1;
+                R += w;
+            }
+
+            for( ; i < n; i++ )
+                dir[sorted[i]] = (char)0;
+        }
+    }
+    node->maxlr = MAX( L, R );
+    return node->split->quality/(L + R);
+}
+
+
+namespace cv
+{
+
+template<> void DefaultDeleter<CvDTreeSplit>::operator ()(CvDTreeSplit* obj) const
+{
+    fastFree(obj);
+}
+
+DTreeBestSplitFinder::DTreeBestSplitFinder( CvDTree* _tree, CvDTreeNode* _node)
+{
+    tree = _tree;
+    node = _node;
+    splitSize = tree->get_data()->split_heap->elem_size;
+
+    bestSplit.reset((CvDTreeSplit*)fastMalloc(splitSize));
+    memset(bestSplit.get(), 0, splitSize);
+    bestSplit->quality = -1;
+    bestSplit->condensed_idx = INT_MIN;
+    split.reset((CvDTreeSplit*)fastMalloc(splitSize));
+    memset(split.get(), 0, splitSize);
+    //haveSplit = false;
+}
+
+DTreeBestSplitFinder::DTreeBestSplitFinder( const DTreeBestSplitFinder& finder, Split )
+{
+    tree = finder.tree;
+    node = finder.node;
+    splitSize = tree->get_data()->split_heap->elem_size;
+
+    bestSplit.reset((CvDTreeSplit*)fastMalloc(splitSize));
+    memcpy(bestSplit.get(), finder.bestSplit.get(), splitSize);
+    split.reset((CvDTreeSplit*)fastMalloc(splitSize));
+    memset(split.get(), 0, splitSize);
+}
+
+void DTreeBestSplitFinder::operator()(const BlockedRange& range)
+{
+    int vi, vi1 = range.begin(), vi2 = range.end();
+    int n = node->sample_count;
+    CvDTreeTrainData* data = tree->get_data();
+    AutoBuffer<uchar> inn_buf(2*n*(sizeof(int) + sizeof(float)));
+
+    for( vi = vi1; vi < vi2; vi++ )
+    {
+        CvDTreeSplit *res;
+        int ci = data->get_var_type(vi);
+        if( node->get_num_valid(vi) <= 1 )
+            continue;
+
+        if( data->is_classifier )
+        {
+            if( ci >= 0 )
+//SAB                res = tree->find_split_cat_class( node, vi, bestSplit->quality, split, inn_buf.data() );
+                res = tree->find_split_cat_class( node, vi, bestSplit->quality, split, inn_buf );
+            else
+//SAB                res = tree->find_split_ord_class( node, vi, bestSplit->quality, split, inn_buf.data() );
+                res = tree->find_split_ord_class( node, vi, bestSplit->quality, split, inn_buf );
+        }
+        else
+        {
+            if( ci >= 0 )
+//SAB                res = tree->find_split_cat_reg( node, vi, bestSplit->quality, split, inn_buf.data() );
+                res = tree->find_split_cat_reg( node, vi, bestSplit->quality, split, inn_buf );
+            else
+//SAB                res = tree->find_split_ord_reg( node, vi, bestSplit->quality, split, inn_buf.data() );
+                res = tree->find_split_ord_reg( node, vi, bestSplit->quality, split, inn_buf );
+        }
+
+        if( res && bestSplit->quality < split->quality )
+                memcpy( bestSplit.get(), split.get(), splitSize );
+    }
+}
+
+void DTreeBestSplitFinder::join( DTreeBestSplitFinder& rhs )
+{
+    if( bestSplit->quality < rhs.bestSplit->quality )
+        memcpy( bestSplit.get(), rhs.bestSplit.get(), splitSize );
+}
+}
+
+
+CvDTreeSplit* CvDTree::find_best_split( CvDTreeNode* node )
+{
+    DTreeBestSplitFinder finder( this, node );
+
+    cv::parallel_reduce(cv::BlockedRange(0, data->var_count), finder);
+
+    CvDTreeSplit *bestSplit = 0;
+    if( finder.bestSplit->quality > 0 )
+    {
+        bestSplit = data->new_split_cat( 0, -1.0f );
+        memcpy( bestSplit, finder.bestSplit, finder.splitSize );
+    }
+
+    return bestSplit;
+}
+
+CvDTreeSplit* CvDTree::find_split_ord_class( CvDTreeNode* node, int vi,
+                                             float init_quality, CvDTreeSplit* _split, uchar* _ext_buf )
+{
+    const float epsilon = FLT_EPSILON*2;
+    int n = node->sample_count;
+    int n1 = node->get_num_valid(vi);
+    int m = data->get_num_classes();
+
+    int base_size = 2*m*sizeof(int);
+    cv::AutoBuffer<uchar> inn_buf(base_size);
+    if( !_ext_buf )
+      inn_buf.allocate(base_size + n*(3*sizeof(int)+sizeof(float)));
+//SAB    uchar* base_buf = inn_buf.data();
+    uchar* base_buf = inn_buf;
+    uchar* ext_buf = _ext_buf ? _ext_buf : base_buf + base_size;
+    float* values_buf = (float*)ext_buf;
+    int* sorted_indices_buf = (int*)(values_buf + n);
+    int* sample_indices_buf = sorted_indices_buf + n;
+    const float* values = 0;
+    const int* sorted_indices = 0;
+    data->get_ord_var_data( node, vi, values_buf, sorted_indices_buf, &values,
+                            &sorted_indices, sample_indices_buf );
+    int* responses_buf =  sample_indices_buf + n;
+    const int* responses = data->get_class_labels( node, responses_buf );
+
+    const int* rc0 = data->counts->data.i;
+    int* lc = (int*)base_buf;
+    int* rc = lc + m;
+    int i, best_i = -1;
+    double lsum2 = 0, rsum2 = 0, best_val = init_quality;
+    const double* priors = data->have_priors ? data->priors_mult->data.db : 0;
+
+    // init arrays of class instance counters on both sides of the split
+    for( i = 0; i < m; i++ )
+    {
+        lc[i] = 0;
+        rc[i] = rc0[i];
+    }
+
+    // compensate for missing values
+    for( i = n1; i < n; i++ )
+    {
+        rc[responses[sorted_indices[i]]]--;
+    }
+
+    if( !priors )
+    {
+        int L = 0, R = n1;
+
+        for( i = 0; i < m; i++ )
+            rsum2 += (double)rc[i]*rc[i];
+
+        for( i = 0; i < n1 - 1; i++ )
+        {
+            int idx = responses[sorted_indices[i]];
+            int lv, rv;
+            L++; R--;
+            lv = lc[idx]; rv = rc[idx];
+            lsum2 += lv*2 + 1;
+            rsum2 -= rv*2 - 1;
+            lc[idx] = lv + 1; rc[idx] = rv - 1;
+
+            if( values[i] + epsilon < values[i+1] )
+            {
+                double val = (lsum2*R + rsum2*L)/((double)L*R);
+                if( best_val < val )
+                {
+                    best_val = val;
+                    best_i = i;
+                }
+            }
+        }
+    }
+    else
+    {
+        double L = 0, R = 0;
+        for( i = 0; i < m; i++ )
+        {
+            double wv = rc[i]*priors[i];
+            R += wv;
+            rsum2 += wv*wv;
+        }
+
+        for( i = 0; i < n1 - 1; i++ )
+        {
+            int idx = responses[sorted_indices[i]];
+            int lv, rv;
+            double p = priors[idx], p2 = p*p;
+            L += p; R -= p;
+            lv = lc[idx]; rv = rc[idx];
+            lsum2 += p2*(lv*2 + 1);
+            rsum2 -= p2*(rv*2 - 1);
+            lc[idx] = lv + 1; rc[idx] = rv - 1;
+
+            if( values[i] + epsilon < values[i+1] )
+            {
+                double val = (lsum2*R + rsum2*L)/((double)L*R);
+                if( best_val < val )
+                {
+                    best_val = val;
+                    best_i = i;
+                }
+            }
+        }
+    }
+
+    CvDTreeSplit* split = 0;
+    if( best_i >= 0 )
+    {
+        split = _split ? _split : data->new_split_ord( 0, 0.0f, 0, 0, 0.0f );
+        split->var_idx = vi;
+        split->ord.c = (values[best_i] + values[best_i+1])*0.5f;
+        split->ord.split_point = best_i;
+        split->inversed = 0;
+        split->quality = (float)best_val;
+    }
+    return split;
+}
+
+
+void CvDTree::cluster_categories( const int* vectors, int n, int m,
+                                int* csums, int k, int* labels )
+{
+    // TODO: consider adding priors (class weights) and sample weights to the clustering algorithm
+    int iters = 0, max_iters = 100;
+    int i, j, idx;
+    cv::AutoBuffer<double> buf(n + k);
+//SAB    double *v_weights = buf.data(), *c_weights = buf.data() + n;
+    double *v_weights = buf, *c_weights = buf + n;
+    bool modified = true;
+    RNG* r = data->rng;
+
+    // assign labels randomly
+    for( i = 0; i < n; i++ )
+    {
+        int sum = 0;
+        const int* v = vectors + i*m;
+        labels[i] = i < k ? i : r->uniform(0, k);
+
+        // compute weight of each vector
+        for( j = 0; j < m; j++ )
+            sum += v[j];
+        v_weights[i] = sum ? 1./sum : 0.;
+    }
+
+    for( i = 0; i < n; i++ )
+    {
+        int i1 = (*r)(n);
+        int i2 = (*r)(n);
+        CV_SWAP( labels[i1], labels[i2], j );
+    }
+
+    for( iters = 0; iters <= max_iters; iters++ )
+    {
+        // calculate csums
+        for( i = 0; i < k; i++ )
+        {
+            for( j = 0; j < m; j++ )
+                csums[i*m + j] = 0;
+        }
+
+        for( i = 0; i < n; i++ )
+        {
+            const int* v = vectors + i*m;
+            int* s = csums + labels[i]*m;
+            for( j = 0; j < m; j++ )
+                s[j] += v[j];
+        }
+
+        // exit the loop here, when we have up-to-date csums
+        if( iters == max_iters || !modified )
+            break;
+
+        modified = false;
+
+        // calculate weight of each cluster
+        for( i = 0; i < k; i++ )
+        {
+            const int* s = csums + i*m;
+            int sum = 0;
+            for( j = 0; j < m; j++ )
+                sum += s[j];
+            c_weights[i] = sum ? 1./sum : 0;
+        }
+
+        // now for each vector determine the closest cluster
+        for( i = 0; i < n; i++ )
+        {
+            const int* v = vectors + i*m;
+            double alpha = v_weights[i];
+            double min_dist2 = DBL_MAX;
+            int min_idx = -1;
+
+            for( idx = 0; idx < k; idx++ )
+            {
+                const int* s = csums + idx*m;
+                double dist2 = 0., beta = c_weights[idx];
+                for( j = 0; j < m; j++ )
+                {
+                    double t = v[j]*alpha - s[j]*beta;
+                    dist2 += t*t;
+                }
+                if( min_dist2 > dist2 )
+                {
+                    min_dist2 = dist2;
+                    min_idx = idx;
+                }
+            }
+
+            if( min_idx != labels[i] )
+                modified = true;
+            labels[i] = min_idx;
+        }
+    }
+}
+
+
+CvDTreeSplit* CvDTree::find_split_cat_class( CvDTreeNode* node, int vi, float init_quality,
+                                             CvDTreeSplit* _split, uchar* _ext_buf )
+{
+    int ci = data->get_var_type(vi);
+    int n = node->sample_count;
+    int m = data->get_num_classes();
+    int _mi = data->cat_count->data.i[ci], mi = _mi;
+
+    int base_size = m*(3 + mi)*sizeof(int) + (mi+1)*sizeof(double);
+    if( m > 2 && mi > data->params.max_categories )
+        base_size += (m*std::min(data->params.max_categories, n) + mi)*sizeof(int);
+    else
+        base_size += mi*sizeof(int*);
+    cv::AutoBuffer<uchar> inn_buf(base_size);
+    if( !_ext_buf )
+        inn_buf.allocate(base_size + 2*n*sizeof(int));
+//SAB    uchar* base_buf = inn_buf.data();
+    uchar* base_buf = inn_buf;
+    uchar* ext_buf = _ext_buf ? _ext_buf : base_buf + base_size;
+
+    int* lc = (int*)base_buf;
+    int* rc = lc + m;
+    int* _cjk = rc + m*2, *cjk = _cjk;
+    double* c_weights = (double*)alignPtr(cjk + m*mi, sizeof(double));
+
+    int* labels_buf = (int*)ext_buf;
+    const int* labels = data->get_cat_var_data(node, vi, labels_buf);
+    int* responses_buf = labels_buf + n;
+    const int* responses = data->get_class_labels(node, responses_buf);
+
+    int* cluster_labels = 0;
+    int** int_ptr = 0;
+    int i, j, k, idx;
+    double L = 0, R = 0;
+    double best_val = init_quality;
+    int prevcode = 0, best_subset = -1, subset_i, subset_n, subtract = 0;
+    const double* priors = data->priors_mult->data.db;
+
+    // init array of counters:
+    // c_{jk} - number of samples that have vi-th input variable = j and response = k.
+    for( j = -1; j < mi; j++ )
+        for( k = 0; k < m; k++ )
+            cjk[j*m + k] = 0;
+
+    for( i = 0; i < n; i++ )
+    {
+       j = ( labels[i] == 65535 && data->is_buf_16u) ? -1 : labels[i];
+       k = responses[i];
+       cjk[j*m + k]++;
+    }
+
+    if( m > 2 )
+    {
+        if( mi > data->params.max_categories )
+        {
+            mi = MIN(data->params.max_categories, n);
+            cjk = (int*)(c_weights + _mi);
+            cluster_labels = cjk + m*mi;
+            cluster_categories( _cjk, _mi, m, cjk, mi, cluster_labels );
+        }
+        subset_i = 1;
+        subset_n = 1 << mi;
+    }
+    else
+    {
+        assert( m == 2 );
+        int_ptr = (int**)(c_weights + _mi);
+        for( j = 0; j < mi; j++ )
+            int_ptr[j] = cjk + j*2 + 1;
+        std::sort(int_ptr, int_ptr + mi, LessThanPtr<int>());
+        subset_i = 0;
+        subset_n = mi;
+    }
+
+    for( k = 0; k < m; k++ )
+    {
+        int sum = 0;
+        for( j = 0; j < mi; j++ )
+            sum += cjk[j*m + k];
+        rc[k] = sum;
+        lc[k] = 0;
+    }
+
+    for( j = 0; j < mi; j++ )
+    {
+        double sum = 0;
+        for( k = 0; k < m; k++ )
+            sum += cjk[j*m + k]*priors[k];
+        c_weights[j] = sum;
+        R += c_weights[j];
+    }
+
+    for( ; subset_i < subset_n; subset_i++ )
+    {
+        double weight;
+        int* crow;
+        double lsum2 = 0, rsum2 = 0;
+
+        if( m == 2 )
+            idx = (int)(int_ptr[subset_i] - cjk)/2;
+        else
+        {
+            int graycode = (subset_i>>1)^subset_i;
+            int diff = graycode ^ prevcode;
+
+            // determine index of the changed bit.
+            Cv32suf u;
+            idx = diff >= (1 << 16) ? 16 : 0;
+            u.f = (float)(((diff >> 16) | diff) & 65535);
+            idx += (u.i >> 23) - 127;
+            subtract = graycode < prevcode;
+            prevcode = graycode;
+        }
+
+        crow = cjk + idx*m;
+        weight = c_weights[idx];
+        if( weight < FLT_EPSILON )
+            continue;
+
+        if( !subtract )
+        {
+            for( k = 0; k < m; k++ )
+            {
+                int t = crow[k];
+                int lval = lc[k] + t;
+                int rval = rc[k] - t;
+                double p = priors[k], p2 = p*p;
+                lsum2 += p2*lval*lval;
+                rsum2 += p2*rval*rval;
+                lc[k] = lval; rc[k] = rval;
+            }
+            L += weight;
+            R -= weight;
+        }
+        else
+        {
+            for( k = 0; k < m; k++ )
+            {
+                int t = crow[k];
+                int lval = lc[k] - t;
+                int rval = rc[k] + t;
+                double p = priors[k], p2 = p*p;
+                lsum2 += p2*lval*lval;
+                rsum2 += p2*rval*rval;
+                lc[k] = lval; rc[k] = rval;
+            }
+            L -= weight;
+            R += weight;
+        }
+
+        if( L > FLT_EPSILON && R > FLT_EPSILON )
+        {
+            double val = (lsum2*R + rsum2*L)/((double)L*R);
+            if( best_val < val )
+            {
+                best_val = val;
+                best_subset = subset_i;
+            }
+        }
+    }
+
+    CvDTreeSplit* split = 0;
+    if( best_subset >= 0 )
+    {
+        split = _split ? _split : data->new_split_cat( 0, -1.0f );
+        split->var_idx = vi;
+        split->quality = (float)best_val;
+        memset( split->subset, 0, (data->max_c_count + 31)/32 * sizeof(int));
+        if( m == 2 )
+        {
+            for( i = 0; i <= best_subset; i++ )
+            {
+                idx = (int)(int_ptr[i] - cjk) >> 1;
+                split->subset[idx >> 5] |= 1 << (idx & 31);
+            }
+        }
+        else
+        {
+            for( i = 0; i < _mi; i++ )
+            {
+                idx = cluster_labels ? cluster_labels[i] : i;
+                if( best_subset & (1 << idx) )
+                    split->subset[i >> 5] |= 1 << (i & 31);
+            }
+        }
+    }
+    return split;
+}
+
+
+CvDTreeSplit* CvDTree::find_split_ord_reg( CvDTreeNode* node, int vi, float init_quality, CvDTreeSplit* _split, uchar* _ext_buf )
+{
+    const float epsilon = FLT_EPSILON*2;
+    int n = node->sample_count;
+    int n1 = node->get_num_valid(vi);
+
+    cv::AutoBuffer<uchar> inn_buf;
+    if( !_ext_buf )
+        inn_buf.allocate(2*n*(sizeof(int) + sizeof(float)));
+//SAB    uchar* ext_buf = _ext_buf ? _ext_buf : inn_buf.data();
+    uchar* ext_buf = _ext_buf ? _ext_buf : inn_buf;
+    float* values_buf = (float*)ext_buf;
+    int* sorted_indices_buf = (int*)(values_buf + n);
+    int* sample_indices_buf = sorted_indices_buf + n;
+    const float* values = 0;
+    const int* sorted_indices = 0;
+    data->get_ord_var_data( node, vi, values_buf, sorted_indices_buf, &values, &sorted_indices, sample_indices_buf );
+    float* responses_buf =  (float*)(sample_indices_buf + n);
+    const float* responses = data->get_ord_responses( node, responses_buf, sample_indices_buf );
+
+    int i, best_i = -1;
+    double best_val = init_quality, lsum = 0, rsum = node->value*n;
+    int L = 0, R = n1;
+
+    // compensate for missing values
+    for( i = n1; i < n; i++ )
+        rsum -= responses[sorted_indices[i]];
+
+    // find the optimal split
+    for( i = 0; i < n1 - 1; i++ )
+    {
+        float t = responses[sorted_indices[i]];
+        L++; R--;
+        lsum += t;
+        rsum -= t;
+
+        if( values[i] + epsilon < values[i+1] )
+        {
+            double val = (lsum*lsum*R + rsum*rsum*L)/((double)L*R);
+            if( best_val < val )
+            {
+                best_val = val;
+                best_i = i;
+            }
+        }
+    }
+
+    CvDTreeSplit* split = 0;
+    if( best_i >= 0 )
+    {
+        split = _split ? _split : data->new_split_ord( 0, 0.0f, 0, 0, 0.0f );
+        split->var_idx = vi;
+        split->ord.c = (values[best_i] + values[best_i+1])*0.5f;
+        split->ord.split_point = best_i;
+        split->inversed = 0;
+        split->quality = (float)best_val;
+    }
+    return split;
+}
+
+CvDTreeSplit* CvDTree::find_split_cat_reg( CvDTreeNode* node, int vi, float init_quality, CvDTreeSplit* _split, uchar* _ext_buf )
+{
+    int ci = data->get_var_type(vi);
+    int n = node->sample_count;
+    int mi = data->cat_count->data.i[ci];
+
+    int base_size = (mi+2)*sizeof(double) + (mi+1)*(sizeof(int) + sizeof(double*));
+    cv::AutoBuffer<uchar> inn_buf(base_size);
+    if( !_ext_buf )
+        inn_buf.allocate(base_size + n*(2*sizeof(int) + sizeof(float)));
+//SAB    uchar* base_buf = inn_buf.data();
+    uchar* base_buf = inn_buf;
+    uchar* ext_buf = _ext_buf ? _ext_buf : base_buf + base_size;
+    int* labels_buf = (int*)ext_buf;
+    const int* labels = data->get_cat_var_data(node, vi, labels_buf);
+    float* responses_buf = (float*)(labels_buf + n);
+    int* sample_indices_buf = (int*)(responses_buf + n);
+    const float* responses = data->get_ord_responses(node, responses_buf, sample_indices_buf);
+
+    double* sum = (double*)cv::alignPtr(base_buf,sizeof(double)) + 1;
+    int* counts = (int*)(sum + mi) + 1;
+    double** sum_ptr = (double**)(counts + mi);
+    int i, L = 0, R = 0;
+    double best_val = init_quality, lsum = 0, rsum = 0;
+    int best_subset = -1, subset_i;
+
+    for( i = -1; i < mi; i++ )
+        sum[i] = counts[i] = 0;
+
+    // calculate sum response and weight of each category of the input var
+    for( i = 0; i < n; i++ )
+    {
+        int idx = ( (labels[i] == 65535) && data->is_buf_16u ) ? -1 : labels[i];
+        double s = sum[idx] + responses[i];
+        int nc = counts[idx] + 1;
+        sum[idx] = s;
+        counts[idx] = nc;
+    }
+
+    // calculate average response in each category
+    for( i = 0; i < mi; i++ )
+    {
+        R += counts[i];
+        rsum += sum[i];
+        sum[i] /= MAX(counts[i],1);
+        sum_ptr[i] = sum + i;
+    }
+
+    std::sort(sum_ptr, sum_ptr + mi, LessThanPtr<double>());
+
+    // revert back to unnormalized sums
+    // (there should be a very little loss of accuracy)
+    for( i = 0; i < mi; i++ )
+        sum[i] *= counts[i];
+
+    for( subset_i = 0; subset_i < mi-1; subset_i++ )
+    {
+        int idx = (int)(sum_ptr[subset_i] - sum);
+        int ni = counts[idx];
+
+        if( ni )
+        {
+            double s = sum[idx];
+            lsum += s; L += ni;
+            rsum -= s; R -= ni;
+
+            if( L && R )
+            {
+                double val = (lsum*lsum*R + rsum*rsum*L)/((double)L*R);
+                if( best_val < val )
+                {
+                    best_val = val;
+                    best_subset = subset_i;
+                }
+            }
+        }
+    }
+
+    CvDTreeSplit* split = 0;
+    if( best_subset >= 0 )
+    {
+        split = _split ? _split : data->new_split_cat( 0, -1.0f);
+        split->var_idx = vi;
+        split->quality = (float)best_val;
+        memset( split->subset, 0, (data->max_c_count + 31)/32 * sizeof(int));
+        for( i = 0; i <= best_subset; i++ )
+        {
+            int idx = (int)(sum_ptr[i] - sum);
+            split->subset[idx >> 5] |= 1 << (idx & 31);
+        }
+    }
+    return split;
+}
+
+CvDTreeSplit* CvDTree::find_surrogate_split_ord( CvDTreeNode* node, int vi, uchar* _ext_buf )
+{
+    const float epsilon = FLT_EPSILON*2;
+    const char* dir = (char*)data->direction->data.ptr;
+    int n = node->sample_count, n1 = node->get_num_valid(vi);
+    cv::AutoBuffer<uchar> inn_buf;
+    if( !_ext_buf )
+        inn_buf.allocate( n*(sizeof(int)*(data->have_priors ? 3 : 2) + sizeof(float)) );
+//SAB    uchar* ext_buf = _ext_buf ? _ext_buf : inn_buf.data();
+    uchar* ext_buf = _ext_buf ? _ext_buf : inn_buf;
+    float* values_buf = (float*)ext_buf;
+    int* sorted_indices_buf = (int*)(values_buf + n);
+    int* sample_indices_buf = sorted_indices_buf + n;
+    const float* values = 0;
+    const int* sorted_indices = 0;
+    data->get_ord_var_data( node, vi, values_buf, sorted_indices_buf, &values, &sorted_indices, sample_indices_buf );
+    // LL - number of samples that both the primary and the surrogate splits send to the left
+    // LR - ... primary split sends to the left and the surrogate split sends to the right
+    // RL - ... primary split sends to the right and the surrogate split sends to the left
+    // RR - ... both send to the right
+    int i, best_i = -1, best_inversed = 0;
+    double best_val;
+
+    if( !data->have_priors )
+    {
+        int LL = 0, RL = 0, LR, RR;
+        int worst_val = cvFloor(node->maxlr), _best_val = worst_val;
+        int sum = 0, sum_abs = 0;
+
+        for( i = 0; i < n1; i++ )
+        {
+            int d = dir[sorted_indices[i]];
+            sum += d; sum_abs += d & 1;
+        }
+
+        // sum_abs = R + L; sum = R - L
+        RR = (sum_abs + sum) >> 1;
+        LR = (sum_abs - sum) >> 1;
+
+        // initially all the samples are sent to the right by the surrogate split,
+        // LR of them are sent to the left by primary split, and RR - to the right.
+        // now iteratively compute LL, LR, RL and RR for every possible surrogate split value.
+        for( i = 0; i < n1 - 1; i++ )
+        {
+            int d = dir[sorted_indices[i]];
+
+            if( d < 0 )
+            {
+                LL++; LR--;
+                if( LL + RR > _best_val && values[i] + epsilon < values[i+1] )
+                {
+                    best_val = LL + RR;
+                    best_i = i; best_inversed = 0;
+                }
+            }
+            else if( d > 0 )
+            {
+                RL++; RR--;
+                if( RL + LR > _best_val && values[i] + epsilon < values[i+1] )
+                {
+                    best_val = RL + LR;
+                    best_i = i; best_inversed = 1;
+                }
+            }
+        }
+        best_val = _best_val;
+    }
+    else
+    {
+        double LL = 0, RL = 0, LR, RR;
+        double worst_val = node->maxlr;
+        double sum = 0, sum_abs = 0;
+        const double* priors = data->priors_mult->data.db;
+        int* responses_buf = sample_indices_buf + n;
+        const int* responses = data->get_class_labels(node, responses_buf);
+        best_val = worst_val;
+
+        for( i = 0; i < n1; i++ )
+        {
+            int idx = sorted_indices[i];
+            double w = priors[responses[idx]];
+            int d = dir[idx];
+            sum += d*w; sum_abs += (d & 1)*w;
+        }
+
+        // sum_abs = R + L; sum = R - L
+        RR = (sum_abs + sum)*0.5;
+        LR = (sum_abs - sum)*0.5;
+
+        // initially all the samples are sent to the right by the surrogate split,
+        // LR of them are sent to the left by primary split, and RR - to the right.
+        // now iteratively compute LL, LR, RL and RR for every possible surrogate split value.
+        for( i = 0; i < n1 - 1; i++ )
+        {
+            int idx = sorted_indices[i];
+            double w = priors[responses[idx]];
+            int d = dir[idx];
+
+            if( d < 0 )
+            {
+                LL += w; LR -= w;
+                if( LL + RR > best_val && values[i] + epsilon < values[i+1] )
+                {
+                    best_val = LL + RR;
+                    best_i = i; best_inversed = 0;
+                }
+            }
+            else if( d > 0 )
+            {
+                RL += w; RR -= w;
+                if( RL + LR > best_val && values[i] + epsilon < values[i+1] )
+                {
+                    best_val = RL + LR;
+                    best_i = i; best_inversed = 1;
+                }
+            }
+        }
+    }
+    return best_i >= 0 && best_val > node->maxlr ? data->new_split_ord( vi,
+        (values[best_i] + values[best_i+1])*0.5f, best_i, best_inversed, (float)best_val ) : 0;
+}
+
+
+CvDTreeSplit* CvDTree::find_surrogate_split_cat( CvDTreeNode* node, int vi, uchar* _ext_buf )
+{
+    const char* dir = (char*)data->direction->data.ptr;
+    int n = node->sample_count;
+    int i, mi = data->cat_count->data.i[data->get_var_type(vi)], l_win = 0;
+
+    int base_size = (2*(mi+1)+1)*sizeof(double) + (!data->have_priors ? 2*(mi+1)*sizeof(int) : 0);
+    cv::AutoBuffer<uchar> inn_buf(base_size);
+    if( !_ext_buf )
+        inn_buf.allocate(base_size + n*(sizeof(int) + (data->have_priors ? sizeof(int) : 0)));
+//SAB    uchar* base_buf = inn_buf.data();
+    uchar* base_buf = inn_buf;
+    uchar* ext_buf = _ext_buf ? _ext_buf : base_buf + base_size;
+
+    int* labels_buf = (int*)ext_buf;
+    const int* labels = data->get_cat_var_data(node, vi, labels_buf);
+    // LL - number of samples that both the primary and the surrogate splits send to the left
+    // LR - ... primary split sends to the left and the surrogate split sends to the right
+    // RL - ... primary split sends to the right and the surrogate split sends to the left
+    // RR - ... both send to the right
+    CvDTreeSplit* split = data->new_split_cat( vi, 0 );
+    double best_val = 0;
+    double* lc = (double*)cv::alignPtr(base_buf,sizeof(double)) + 1;
+    double* rc = lc + mi + 1;
+
+    for( i = -1; i < mi; i++ )
+        lc[i] = rc[i] = 0;
+
+    // for each category calculate the weight of samples
+    // sent to the left (lc) and to the right (rc) by the primary split
+    if( !data->have_priors )
+    {
+        int* _lc = (int*)rc + 1;
+        int* _rc = _lc + mi + 1;
+
+        for( i = -1; i < mi; i++ )
+            _lc[i] = _rc[i] = 0;
+
+        for( i = 0; i < n; i++ )
+        {
+            int idx = ( (labels[i] == 65535) && (data->is_buf_16u) ) ? -1 : labels[i];
+            int d = dir[i];
+            int sum = _lc[idx] + d;
+            int sum_abs = _rc[idx] + (d & 1);
+            _lc[idx] = sum; _rc[idx] = sum_abs;
+        }
+
+        for( i = 0; i < mi; i++ )
+        {
+            int sum = _lc[i];
+            int sum_abs = _rc[i];
+            lc[i] = (sum_abs - sum) >> 1;
+            rc[i] = (sum_abs + sum) >> 1;
+        }
+    }
+    else
+    {
+        const double* priors = data->priors_mult->data.db;
+        int* responses_buf = labels_buf + n;
+        const int* responses = data->get_class_labels(node, responses_buf);
+
+        for( i = 0; i < n; i++ )
+        {
+            int idx = ( (labels[i] == 65535) && (data->is_buf_16u) ) ? -1 : labels[i];
+            double w = priors[responses[i]];
+            int d = dir[i];
+            double sum = lc[idx] + d*w;
+            double sum_abs = rc[idx] + (d & 1)*w;
+            lc[idx] = sum; rc[idx] = sum_abs;
+        }
+
+        for( i = 0; i < mi; i++ )
+        {
+            double sum = lc[i];
+            double sum_abs = rc[i];
+            lc[i] = (sum_abs - sum) * 0.5;
+            rc[i] = (sum_abs + sum) * 0.5;
+        }
+    }
+
+    // 2. now form the split.
+    // in each category send all the samples to the same direction as majority
+    for( i = 0; i < mi; i++ )
+    {
+        double lval = lc[i], rval = rc[i];
+        if( lval > rval )
+        {
+            split->subset[i >> 5] |= 1 << (i & 31);
+            best_val += lval;
+            l_win++;
+        }
+        else
+            best_val += rval;
+    }
+
+    split->quality = (float)best_val;
+    if( split->quality <= node->maxlr || l_win == 0 || l_win == mi )
+        cvSetRemoveByPtr( data->split_heap, split ), split = 0;
+
+    return split;
+}
+
+
+void CvDTree::calc_node_value( CvDTreeNode* node )
+{
+    int i, j, k, n = node->sample_count, cv_n = data->params.cv_folds;
+    int m = data->get_num_classes();
+
+    int base_size = data->is_classifier ? m*cv_n*sizeof(int) : 2*cv_n*sizeof(double)+cv_n*sizeof(int);
+    int ext_size = n*(sizeof(int) + (data->is_classifier ? sizeof(int) : sizeof(int)+sizeof(float)));
+    cv::AutoBuffer<uchar> inn_buf(base_size + ext_size);
+//SAB    uchar* base_buf = inn_buf.data();
+    uchar* base_buf = inn_buf;
+    uchar* ext_buf = base_buf + base_size;
+
+    int* cv_labels_buf = (int*)ext_buf;
+    const int* cv_labels = data->get_cv_labels(node, cv_labels_buf);
+
+    if( data->is_classifier )
+    {
+        // in case of classification tree:
+        //  * node value is the label of the class that has the largest weight in the node.
+        //  * node risk is the weighted number of misclassified samples,
+        //  * j-th cross-validation fold value and risk are calculated as above,
+        //    but using the samples with cv_labels(*)!=j.
+        //  * j-th cross-validation fold error is calculated as the weighted number of
+        //    misclassified samples with cv_labels(*)==j.
+
+        // compute the number of instances of each class
+        int* cls_count = data->counts->data.i;
+        int* responses_buf = cv_labels_buf + n;
+        const int* responses = data->get_class_labels(node, responses_buf);
+        int* cv_cls_count = (int*)base_buf;
+        double max_val = -1, total_weight = 0;
+        int max_k = -1;
+        double* priors = data->priors_mult->data.db;
+
+        for( k = 0; k < m; k++ )
+            cls_count[k] = 0;
+
+        if( cv_n == 0 )
+        {
+            for( i = 0; i < n; i++ )
+                cls_count[responses[i]]++;
+        }
+        else
+        {
+            for( j = 0; j < cv_n; j++ )
+                for( k = 0; k < m; k++ )
+                    cv_cls_count[j*m + k] = 0;
+
+            for( i = 0; i < n; i++ )
+            {
+                j = cv_labels[i]; k = responses[i];
+                cv_cls_count[j*m + k]++;
+            }
+
+            for( j = 0; j < cv_n; j++ )
+                for( k = 0; k < m; k++ )
+                    cls_count[k] += cv_cls_count[j*m + k];
+        }
+
+        if( data->have_priors && node->parent == 0 )
+        {
+            // compute priors_mult from priors, take the sample ratio into account.
+            double sum = 0;
+            for( k = 0; k < m; k++ )
+            {
+                int n_k = cls_count[k];
+                priors[k] = data->priors->data.db[k]*(n_k ? 1./n_k : 0.);
+                sum += priors[k];
+            }
+            sum = 1./sum;
+            for( k = 0; k < m; k++ )
+                priors[k] *= sum;
+        }
+
+        for( k = 0; k < m; k++ )
+        {
+            double val = cls_count[k]*priors[k];
+            total_weight += val;
+            if( max_val < val )
+            {
+                max_val = val;
+                max_k = k;
+            }
+        }
+
+        node->class_idx = max_k;
+        node->value = data->cat_map->data.i[
+            data->cat_ofs->data.i[data->cat_var_count] + max_k];
+        node->node_risk = total_weight - max_val;
+
+        for( j = 0; j < cv_n; j++ )
+        {
+            double sum_k = 0, sum = 0, max_val_k = 0;
+            max_val = -1; max_k = -1;
+
+            for( k = 0; k < m; k++ )
+            {
+                double w = priors[k];
+                double val_k = cv_cls_count[j*m + k]*w;
+                double val = cls_count[k]*w - val_k;
+                sum_k += val_k;
+                sum += val;
+                if( max_val < val )
+                {
+                    max_val = val;
+                    max_val_k = val_k;
+                    max_k = k;
+                }
+            }
+
+            node->cv_Tn[j] = INT_MAX;
+            node->cv_node_risk[j] = sum - max_val;
+            node->cv_node_error[j] = sum_k - max_val_k;
+        }
+    }
+    else
+    {
+        // in case of regression tree:
+        //  * node value is 1/n*sum_i(Y_i), where Y_i is i-th response,
+        //    n is the number of samples in the node.
+        //  * node risk is the sum of squared errors: sum_i((Y_i - <node_value>)^2)
+        //  * j-th cross-validation fold value and risk are calculated as above,
+        //    but using the samples with cv_labels(*)!=j.
+        //  * j-th cross-validation fold error is calculated
+        //    using samples with cv_labels(*)==j as the test subset:
+        //    error_j = sum_(i,cv_labels(i)==j)((Y_i - <node_value_j>)^2),
+        //    where node_value_j is the node value calculated
+        //    as described in the previous bullet, and summation is done
+        //    over the samples with cv_labels(*)==j.
+
+        double sum = 0, sum2 = 0;
+        float* values_buf = (float*)(cv_labels_buf + n);
+        int* sample_indices_buf = (int*)(values_buf + n);
+        const float* values = data->get_ord_responses(node, values_buf, sample_indices_buf);
+        double *cv_sum = 0, *cv_sum2 = 0;
+        int* cv_count = 0;
+
+        if( cv_n == 0 )
+        {
+            for( i = 0; i < n; i++ )
+            {
+                double t = values[i];
+                sum += t;
+                sum2 += t*t;
+            }
+        }
+        else
+        {
+            cv_sum = (double*)base_buf;
+            cv_sum2 = cv_sum + cv_n;
+            cv_count = (int*)(cv_sum2 + cv_n);
+
+            for( j = 0; j < cv_n; j++ )
+            {
+                cv_sum[j] = cv_sum2[j] = 0.;
+                cv_count[j] = 0;
+            }
+
+            for( i = 0; i < n; i++ )
+            {
+                j = cv_labels[i];
+                double t = values[i];
+                double s = cv_sum[j] + t;
+                double s2 = cv_sum2[j] + t*t;
+                int nc = cv_count[j] + 1;
+                cv_sum[j] = s;
+                cv_sum2[j] = s2;
+                cv_count[j] = nc;
+            }
+
+            for( j = 0; j < cv_n; j++ )
+            {
+                sum += cv_sum[j];
+                sum2 += cv_sum2[j];
+            }
+        }
+
+        node->node_risk = sum2 - (sum/n)*sum;
+        node->value = sum/n;
+
+        for( j = 0; j < cv_n; j++ )
+        {
+            double s = cv_sum[j], si = sum - s;
+            double s2 = cv_sum2[j], s2i = sum2 - s2;
+            int c = cv_count[j], ci = n - c;
+            double r = si/MAX(ci,1);
+            node->cv_node_risk[j] = s2i - r*r*ci;
+            node->cv_node_error[j] = s2 - 2*r*s + c*r*r;
+            node->cv_Tn[j] = INT_MAX;
+        }
+    }
+}
+
+
+void CvDTree::complete_node_dir( CvDTreeNode* node )
+{
+    int vi, i, n = node->sample_count, nl, nr, d0 = 0, d1 = -1;
+    int nz = n - node->get_num_valid(node->split->var_idx);
+    char* dir = (char*)data->direction->data.ptr;
+
+    // try to complete direction using surrogate splits
+    if( nz && data->params.use_surrogates )
+    {
+        cv::AutoBuffer<uchar> inn_buf(n*(2*sizeof(int)+sizeof(float)));
+        CvDTreeSplit* split = node->split->next;
+        for( ; split != 0 && nz; split = split->next )
+        {
+            int inversed_mask = split->inversed ? -1 : 0;
+            vi = split->var_idx;
+
+            if( data->get_var_type(vi) >= 0 ) // split on categorical var
+            {
+//SAB                int* labels_buf = (int*)inn_buf.data();
+                int* labels_buf = (int*)(uchar*)inn_buf;
+                const int* labels = data->get_cat_var_data(node, vi, labels_buf);
+                const int* subset = split->subset;
+
+                for( i = 0; i < n; i++ )
+                {
+                    int idx = labels[i];
+                    if( !dir[i] && ( ((idx >= 0)&&(!data->is_buf_16u)) || ((idx != 65535)&&(data->is_buf_16u)) ))
+
+                    {
+                        int d = CV_DTREE_CAT_DIR(idx,subset);
+                        dir[i] = (char)((d ^ inversed_mask) - inversed_mask);
+                        if( --nz )
+                            break;
+                    }
+                }
+            }
+            else // split on ordered var
+            {
+//SAB                float* values_buf = (float*)inn_buf.data();
+                float* values_buf = (float*)(uchar*)inn_buf;
+                int* sorted_indices_buf = (int*)(values_buf + n);
+                int* sample_indices_buf = sorted_indices_buf + n;
+                const float* values = 0;
+                const int* sorted_indices = 0;
+                data->get_ord_var_data( node, vi, values_buf, sorted_indices_buf, &values, &sorted_indices, sample_indices_buf );
+                int split_point = split->ord.split_point;
+                int n1 = node->get_num_valid(vi);
+
+                assert( 0 <= split_point && split_point < n-1 );
+
+                for( i = 0; i < n1; i++ )
+                {
+                    int idx = sorted_indices[i];
+                    if( !dir[idx] )
+                    {
+                        int d = i <= split_point ? -1 : 1;
+                        dir[idx] = (char)((d ^ inversed_mask) - inversed_mask);
+                        if( --nz )
+                            break;
+                    }
+                }
+            }
+        }
+    }
+
+    // find the default direction for the rest
+    if( nz )
+    {
+        for( i = nr = 0; i < n; i++ )
+            nr += dir[i] > 0;
+        nl = n - nr - nz;
+        d0 = nl > nr ? -1 : nr > nl;
+    }
+
+    // make sure that every sample is directed either to the left or to the right
+    for( i = 0; i < n; i++ )
+    {
+        int d = dir[i];
+        if( !d )
+        {
+            d = d0;
+            if( !d )
+                d = d1, d1 = -d1;
+        }
+        d = d > 0;
+        dir[i] = (char)d; // remap (-1,1) to (0,1)
+    }
+}
+
+
+void CvDTree::split_node_data( CvDTreeNode* node )
+{
+    int vi, i, n = node->sample_count, nl, nr, scount = data->sample_count;
+    char* dir = (char*)data->direction->data.ptr;
+    CvDTreeNode *left = 0, *right = 0;
+    int* new_idx = data->split_buf->data.i;
+    int new_buf_idx = data->get_child_buf_idx( node );
+    int work_var_count = data->get_work_var_count();
+    CvMat* buf = data->buf;
+    size_t length_buf_row = data->get_length_subbuf();
+    cv::AutoBuffer<uchar> inn_buf(n*(3*sizeof(int) + sizeof(float)));
+//SAB    int* temp_buf = (int*)inn_buf.data();
+    int* temp_buf = (int*)(uchar*)inn_buf;
+
+    complete_node_dir(node);
+
+    for( i = nl = nr = 0; i < n; i++ )
+    {
+        int d = dir[i];
+        // initialize new indices for splitting ordered variables
+        new_idx[i] = (nl & (d-1)) | (nr & -d); // d ? ri : li
+        nr += d;
+        nl += d^1;
+    }
+
+    bool split_input_data;
+    node->left = left = data->new_node( node, nl, new_buf_idx, node->offset );
+    node->right = right = data->new_node( node, nr, new_buf_idx, node->offset + nl );
+
+    split_input_data = node->depth + 1 < data->params.max_depth &&
+        (node->left->sample_count > data->params.min_sample_count ||
+        node->right->sample_count > data->params.min_sample_count);
+
+    // split ordered variables, keep both halves sorted.
+    for( vi = 0; vi < data->var_count; vi++ )
+    {
+        int ci = data->get_var_type(vi);
+
+        if( ci >= 0 || !split_input_data )
+            continue;
+
+        int n1 = node->get_num_valid(vi);
+        float* src_val_buf = (float*)(uchar*)(temp_buf + n);
+        int* src_sorted_idx_buf = (int*)(src_val_buf + n);
+        int* src_sample_idx_buf = src_sorted_idx_buf + n;
+        const float* src_val = 0;
+        const int* src_sorted_idx = 0;
+        data->get_ord_var_data(node, vi, src_val_buf, src_sorted_idx_buf, &src_val, &src_sorted_idx, src_sample_idx_buf);
+
+        for(i = 0; i < n; i++)
+            temp_buf[i] = src_sorted_idx[i];
+
+        if (data->is_buf_16u)
+        {
+            unsigned short *ldst, *rdst, *ldst0, *rdst0;
+            //unsigned short tl, tr;
+            ldst0 = ldst = (unsigned short*)(buf->data.s + left->buf_idx*length_buf_row +
+                vi*scount + left->offset);
+            rdst0 = rdst = (unsigned short*)(ldst + nl);
+
+            // split sorted
+            for( i = 0; i < n1; i++ )
+            {
+                int idx = temp_buf[i];
+                int d = dir[idx];
+                idx = new_idx[idx];
+                if (d)
+                {
+                    *rdst = (unsigned short)idx;
+                    rdst++;
+                }
+                else
+                {
+                    *ldst = (unsigned short)idx;
+                    ldst++;
+                }
+            }
+
+            left->set_num_valid(vi, (int)(ldst - ldst0));
+            right->set_num_valid(vi, (int)(rdst - rdst0));
+
+            // split missing
+            for( ; i < n; i++ )
+            {
+                int idx = temp_buf[i];
+                int d = dir[idx];
+                idx = new_idx[idx];
+                if (d)
+                {
+                    *rdst = (unsigned short)idx;
+                    rdst++;
+                }
+                else
+                {
+                    *ldst = (unsigned short)idx;
+                    ldst++;
+                }
+            }
+        }
+        else
+        {
+            int *ldst0, *ldst, *rdst0, *rdst;
+            ldst0 = ldst = buf->data.i + left->buf_idx*length_buf_row +
+                vi*scount + left->offset;
+            rdst0 = rdst = buf->data.i + right->buf_idx*length_buf_row +
+                vi*scount + right->offset;
+
+            // split sorted
+            for( i = 0; i < n1; i++ )
+            {
+                int idx = temp_buf[i];
+                int d = dir[idx];
+                idx = new_idx[idx];
+                if (d)
+                {
+                    *rdst = idx;
+                    rdst++;
+                }
+                else
+                {
+                    *ldst = idx;
+                    ldst++;
+                }
+            }
+
+            left->set_num_valid(vi, (int)(ldst - ldst0));
+            right->set_num_valid(vi, (int)(rdst - rdst0));
+
+            // split missing
+            for( ; i < n; i++ )
+            {
+                int idx = temp_buf[i];
+                int d = dir[idx];
+                idx = new_idx[idx];
+                if (d)
+                {
+                    *rdst = idx;
+                    rdst++;
+                }
+                else
+                {
+                    *ldst = idx;
+                    ldst++;
+                }
+            }
+        }
+    }
+
+    // split categorical vars, responses and cv_labels using new_idx relocation table
+    for( vi = 0; vi < work_var_count; vi++ )
+    {
+        int ci = data->get_var_type(vi);
+        int n1 = node->get_num_valid(vi), nr1 = 0;
+
+        if( ci < 0 || (vi < data->var_count && !split_input_data) )
+            continue;
+
+        int *src_lbls_buf = temp_buf + n;
+        const int* src_lbls = data->get_cat_var_data(node, vi, src_lbls_buf);
+
+        for(i = 0; i < n; i++)
+            temp_buf[i] = src_lbls[i];
+
+        if (data->is_buf_16u)
+        {
+            unsigned short *ldst = (unsigned short *)(buf->data.s + left->buf_idx*length_buf_row +
+                vi*scount + left->offset);
+            unsigned short *rdst = (unsigned short *)(buf->data.s + right->buf_idx*length_buf_row +
+                vi*scount + right->offset);
+
+            for( i = 0; i < n; i++ )
+            {
+                int d = dir[i];
+                int idx = temp_buf[i];
+                if (d)
+                {
+                    *rdst = (unsigned short)idx;
+                    rdst++;
+                    nr1 += (idx != 65535 )&d;
+                }
+                else
+                {
+                    *ldst = (unsigned short)idx;
+                    ldst++;
+                }
+            }
+
+            if( vi < data->var_count )
+            {
+                left->set_num_valid(vi, n1 - nr1);
+                right->set_num_valid(vi, nr1);
+            }
+        }
+        else
+        {
+            int *ldst = buf->data.i + left->buf_idx*length_buf_row +
+                vi*scount + left->offset;
+            int *rdst = buf->data.i + right->buf_idx*length_buf_row +
+                vi*scount + right->offset;
+
+            for( i = 0; i < n; i++ )
+            {
+                int d = dir[i];
+                int idx = temp_buf[i];
+                if (d)
+                {
+                    *rdst = idx;
+                    rdst++;
+                    nr1 += (idx >= 0)&d;
+                }
+                else
+                {
+                    *ldst = idx;
+                    ldst++;
+                }
+
+            }
+
+            if( vi < data->var_count )
+            {
+                left->set_num_valid(vi, n1 - nr1);
+                right->set_num_valid(vi, nr1);
+            }
+        }
+    }
+
+
+    // split sample indices
+    int *sample_idx_src_buf = temp_buf + n;
+    const int* sample_idx_src = data->get_sample_indices(node, sample_idx_src_buf);
+
+    for(i = 0; i < n; i++)
+        temp_buf[i] = sample_idx_src[i];
+
+    int pos = data->get_work_var_count();
+    if (data->is_buf_16u)
+    {
+        unsigned short* ldst = (unsigned short*)(buf->data.s + left->buf_idx*length_buf_row +
+            pos*scount + left->offset);
+        unsigned short* rdst = (unsigned short*)(buf->data.s + right->buf_idx*length_buf_row +
+            pos*scount + right->offset);
+        for (i = 0; i < n; i++)
+        {
+            int d = dir[i];
+            unsigned short idx = (unsigned short)temp_buf[i];
+            if (d)
+            {
+                *rdst = idx;
+                rdst++;
+            }
+            else
+            {
+                *ldst = idx;
+                ldst++;
+            }
+        }
+    }
+    else
+    {
+        int* ldst = buf->data.i + left->buf_idx*length_buf_row +
+            pos*scount + left->offset;
+        int* rdst = buf->data.i + right->buf_idx*length_buf_row +
+            pos*scount + right->offset;
+        for (i = 0; i < n; i++)
+        {
+            int d = dir[i];
+            int idx = temp_buf[i];
+            if (d)
+            {
+                *rdst = idx;
+                rdst++;
+            }
+            else
+            {
+                *ldst = idx;
+                ldst++;
+            }
+        }
+    }
+
+    // deallocate the parent node data that is not needed anymore
+    data->free_node_data(node);
+}
+
+float CvDTree::calc_error( CvMLData* _data, int type, std::vector<float> *resp )
+{
+    float err = 0;
+    const CvMat* values = _data->get_values();
+    const CvMat* response = _data->get_responses();
+    const CvMat* missing = _data->get_missing();
+    const CvMat* sample_idx = (type == CV_TEST_ERROR) ? _data->get_test_sample_idx() : _data->get_train_sample_idx();
+    const CvMat* var_types = _data->get_var_types();
+    int* sidx = sample_idx ? sample_idx->data.i : 0;
+    int r_step = CV_IS_MAT_CONT(response->type) ?
+                1 : response->step / CV_ELEM_SIZE(response->type);
+    bool is_classifier = var_types->data.ptr[var_types->cols-1] == CV_VAR_CATEGORICAL;
+    int sample_count = sample_idx ? sample_idx->cols : 0;
+    sample_count = (type == CV_TRAIN_ERROR && sample_count == 0) ? values->rows : sample_count;
+    float* pred_resp = 0;
+    if( resp && (sample_count > 0) )
+    {
+        resp->resize( sample_count );
+        pred_resp = &((*resp)[0]);
+    }
+
+    if ( is_classifier )
+    {
+        for( int i = 0; i < sample_count; i++ )
+        {
+            CvMat sample, miss;
+            int si = sidx ? sidx[i] : i;
+            cvGetRow( values, &sample, si );
+            if( missing )
+                cvGetRow( missing, &miss, si );
+            float r = (float)predict( &sample, missing ? &miss : 0 )->value;
+            if( pred_resp )
+                pred_resp[i] = r;
+            int d = fabs((double)r - response->data.fl[(size_t)si*r_step]) <= FLT_EPSILON ? 0 : 1;
+            err += d;
+        }
+        err = sample_count ? err / (float)sample_count * 100 : -FLT_MAX;
+    }
+    else
+    {
+        for( int i = 0; i < sample_count; i++ )
+        {
+            CvMat sample, miss;
+            int si = sidx ? sidx[i] : i;
+            cvGetRow( values, &sample, si );
+            if( missing )
+                cvGetRow( missing, &miss, si );
+            float r = (float)predict( &sample, missing ? &miss : 0 )->value;
+            if( pred_resp )
+                pred_resp[i] = r;
+            float d = r - response->data.fl[(size_t)si*r_step];
+            err += d*d;
+        }
+        err = sample_count ? err / (float)sample_count : -FLT_MAX;
+    }
+    return err;
+}
+
+void CvDTree::prune_cv()
+{
+    CvMat* ab = 0;
+    CvMat* temp = 0;
+    CvMat* err_jk = 0;
+
+    // 1. build tree sequence for each cv fold, calculate error_{Tj,beta_k}.
+    // 2. choose the best tree index (if need, apply 1SE rule).
+    // 3. store the best index and cut the branches.
+
+    CV_FUNCNAME( "CvDTree::prune_cv" );
+
+    __BEGIN__;
+
+    int ti, j, tree_count = 0, cv_n = data->params.cv_folds, n = root->sample_count;
+    // currently, 1SE for regression is not implemented
+    bool use_1se = data->params.use_1se_rule != 0 && data->is_classifier;
+    double* err;
+    double min_err = 0, min_err_se = 0;
+    int min_idx = -1;
+
+    CV_CALL( ab = cvCreateMat( 1, 256, CV_64F ));
+
+    // build the main tree sequence, calculate alpha's
+    for(;;tree_count++)
+    {
+        double min_alpha = update_tree_rnc(tree_count, -1);
+        if( cut_tree(tree_count, -1, min_alpha) )
+            break;
+
+        if( ab->cols <= tree_count )
+        {
+            CV_CALL( temp = cvCreateMat( 1, ab->cols*3/2, CV_64F ));
+            for( ti = 0; ti < ab->cols; ti++ )
+                temp->data.db[ti] = ab->data.db[ti];
+            cvReleaseMat( &ab );
+            ab = temp;
+            temp = 0;
+        }
+
+        ab->data.db[tree_count] = min_alpha;
+    }
+
+    ab->data.db[0] = 0.;
+
+    if( tree_count > 0 )
+    {
+        for( ti = 1; ti < tree_count-1; ti++ )
+            ab->data.db[ti] = sqrt(ab->data.db[ti]*ab->data.db[ti+1]);
+        ab->data.db[tree_count-1] = DBL_MAX*0.5;
+
+        CV_CALL( err_jk = cvCreateMat( cv_n, tree_count, CV_64F ));
+        err = err_jk->data.db;
+
+        for( j = 0; j < cv_n; j++ )
+        {
+            int tj = 0, tk = 0;
+            for( ; tk < tree_count; tj++ )
+            {
+                double min_alpha = update_tree_rnc(tj, j);
+                if( cut_tree(tj, j, min_alpha) )
+                    min_alpha = DBL_MAX;
+
+                for( ; tk < tree_count; tk++ )
+                {
+                    if( ab->data.db[tk] > min_alpha )
+                        break;
+                    err[j*tree_count + tk] = root->tree_error;
+                }
+            }
+        }
+
+        for( ti = 0; ti < tree_count; ti++ )
+        {
+            double sum_err = 0;
+            for( j = 0; j < cv_n; j++ )
+                sum_err += err[j*tree_count + ti];
+            if( ti == 0 || sum_err < min_err )
+            {
+                min_err = sum_err;
+                min_idx = ti;
+                if( use_1se )
+                    min_err_se = sqrt( sum_err*(n - sum_err) );
+            }
+            else if( sum_err < min_err + min_err_se )
+                min_idx = ti;
+        }
+    }
+
+    pruned_tree_idx = min_idx;
+    free_prune_data(data->params.truncate_pruned_tree != 0);
+
+    __END__;
+
+    cvReleaseMat( &err_jk );
+    cvReleaseMat( &ab );
+    cvReleaseMat( &temp );
+}
+
+
+double CvDTree::update_tree_rnc( int T, int fold )
+{
+    CvDTreeNode* node = root;
+    double min_alpha = DBL_MAX;
+
+    for(;;)
+    {
+        CvDTreeNode* parent;
+        for(;;)
+        {
+            int t = fold >= 0 ? node->cv_Tn[fold] : node->Tn;
+            if( t <= T || !node->left )
+            {
+                node->complexity = 1;
+                node->tree_risk = node->node_risk;
+                node->tree_error = 0.;
+                if( fold >= 0 )
+                {
+                    node->tree_risk = node->cv_node_risk[fold];
+                    node->tree_error = node->cv_node_error[fold];
+                }
+                break;
+            }
+            node = node->left;
+        }
+
+        for( parent = node->parent; parent && parent->right == node;
+            node = parent, parent = parent->parent )
+        {
+            parent->complexity += node->complexity;
+            parent->tree_risk += node->tree_risk;
+            parent->tree_error += node->tree_error;
+
+            parent->alpha = ((fold >= 0 ? parent->cv_node_risk[fold] : parent->node_risk)
+                - parent->tree_risk)/(parent->complexity - 1);
+            min_alpha = MIN( min_alpha, parent->alpha );
+        }
+
+        if( !parent )
+            break;
+
+        parent->complexity = node->complexity;
+        parent->tree_risk = node->tree_risk;
+        parent->tree_error = node->tree_error;
+        node = parent->right;
+    }
+
+    return min_alpha;
+}
+
+
+int CvDTree::cut_tree( int T, int fold, double min_alpha )
+{
+    CvDTreeNode* node = root;
+    if( !node->left )
+        return 1;
+
+    for(;;)
+    {
+        CvDTreeNode* parent;
+        for(;;)
+        {
+            int t = fold >= 0 ? node->cv_Tn[fold] : node->Tn;
+            if( t <= T || !node->left )
+                break;
+            if( node->alpha <= min_alpha + FLT_EPSILON )
+            {
+                if( fold >= 0 )
+                    node->cv_Tn[fold] = T;
+                else
+                    node->Tn = T;
+                if( node == root )
+                    return 1;
+                break;
+            }
+            node = node->left;
+        }
+
+        for( parent = node->parent; parent && parent->right == node;
+            node = parent, parent = parent->parent )
+            ;
+
+        if( !parent )
+            break;
+
+        node = parent->right;
+    }
+
+    return 0;
+}
+
+
+void CvDTree::free_prune_data(bool _cut_tree)
+{
+    CvDTreeNode* node = root;
+
+    for(;;)
+    {
+        CvDTreeNode* parent;
+        for(;;)
+        {
+            // do not call cvSetRemoveByPtr( cv_heap, node->cv_Tn )
+            // as we will clear the whole cross-validation heap at the end
+            node->cv_Tn = 0;
+            node->cv_node_error = node->cv_node_risk = 0;
+            if( !node->left )
+                break;
+            node = node->left;
+        }
+
+        for( parent = node->parent; parent && parent->right == node;
+            node = parent, parent = parent->parent )
+        {
+            if( _cut_tree && parent->Tn <= pruned_tree_idx )
+            {
+                data->free_node( parent->left );
+                data->free_node( parent->right );
+                parent->left = parent->right = 0;
+            }
+        }
+
+        if( !parent )
+            break;
+
+        node = parent->right;
+    }
+
+    if( data->cv_heap )
+        cvClearSet( data->cv_heap );
+}
+
+
+void CvDTree::free_tree()
+{
+    if( root && data && data->shared )
+    {
+        pruned_tree_idx = INT_MIN;
+        free_prune_data(true);
+        data->free_node(root);
+        root = 0;
+    }
+}
+
+CvDTreeNode* CvDTree::predict( const CvMat* _sample,
+    const CvMat* _missing, bool preprocessed_input ) const
+{
+    cv::AutoBuffer<int> catbuf;
+
+    int i, mstep = 0;
+    const uchar* m = 0;
+    CvDTreeNode* node = root;
+
+    if( !node )
+        CV_Error( CV_StsError, "The tree has not been trained yet" );
+
+    if( !CV_IS_MAT(_sample) || CV_MAT_TYPE(_sample->type) != CV_32FC1 ||
+        (_sample->cols != 1 && _sample->rows != 1) ||
+        (_sample->cols + _sample->rows - 1 != data->var_all && !preprocessed_input) ||
+        (_sample->cols + _sample->rows - 1 != data->var_count && preprocessed_input) )
+            CV_Error( CV_StsBadArg,
+        "the input sample must be 1d floating-point vector with the same "
+        "number of elements as the total number of variables used for training" );
+
+    const float* sample = _sample->data.fl;
+    int step = CV_IS_MAT_CONT(_sample->type) ? 1 : _sample->step/sizeof(sample[0]);
+
+    if( data->cat_count && !preprocessed_input ) // cache for categorical variables
+    {
+        int n = data->cat_count->cols;
+        catbuf.allocate(n);
+        for( i = 0; i < n; i++ )
+            catbuf[i] = -1;
+    }
+
+    if( _missing )
+    {
+        if( !CV_IS_MAT(_missing) || !CV_IS_MASK_ARR(_missing) ||
+            !CV_ARE_SIZES_EQ(_missing, _sample) )
+            CV_Error( CV_StsBadArg,
+        "the missing data mask must be 8-bit vector of the same size as input sample" );
+        m = _missing->data.ptr;
+        mstep = CV_IS_MAT_CONT(_missing->type) ? 1 : _missing->step/sizeof(m[0]);
+    }
+
+    const int* vtype = data->var_type->data.i;
+    const int* vidx = data->var_idx && !preprocessed_input ? data->var_idx->data.i : 0;
+    const int* cmap = data->cat_map ? data->cat_map->data.i : 0;
+    const int* cofs = data->cat_ofs ? data->cat_ofs->data.i : 0;
+
+    while( node->Tn > pruned_tree_idx && node->left )
+    {
+        CvDTreeSplit* split = node->split;
+        int dir = 0;
+        for( ; !dir && split != 0; split = split->next )
+        {
+            int vi = split->var_idx;
+            int ci = vtype[vi];
+            i = vidx ? vidx[vi] : vi;
+            float val = sample[(size_t)i*step];
+            if( m && m[(size_t)i*mstep] )
+                continue;
+            if( ci < 0 ) // ordered
+                dir = val <= split->ord.c ? -1 : 1;
+            else // categorical
+            {
+                int c;
+                if( preprocessed_input )
+                    c = cvRound(val);
+                else
+                {
+                    c = catbuf[ci];
+                    if( c < 0 )
+                    {
+                        int a = c = cofs[ci];
+                        int b = (ci+1 >= data->cat_ofs->cols) ? data->cat_map->cols : cofs[ci+1];
+
+                        int ival = cvRound(val);
+                        if( ival != val )
+                            CV_Error( CV_StsBadArg,
+                            "one of input categorical variable is not an integer" );
+
+                        int sh = 0;
+                        while( a < b )
+                        {
+                            sh++;
+                            c = (a + b) >> 1;
+                            if( ival < cmap[c] )
+                                b = c;
+                            else if( ival > cmap[c] )
+                                a = c+1;
+                            else
+                                break;
+                        }
+
+                        if( c < 0 || ival != cmap[c] )
+                            continue;
+
+                        catbuf[ci] = c -= cofs[ci];
+                    }
+                }
+                c = ( (c == 65535) && data->is_buf_16u ) ? -1 : c;
+                dir = CV_DTREE_CAT_DIR(c, split->subset);
+            }
+
+            if( split->inversed )
+                dir = -dir;
+        }
+
+        if( !dir )
+        {
+            double diff = node->right->sample_count - node->left->sample_count;
+            dir = diff < 0 ? -1 : 1;
+        }
+        node = dir < 0 ? node->left : node->right;
+    }
+
+    return node;
+}
+
+
+CvDTreeNode* CvDTree::predict( const Mat& _sample, const Mat& _missing, bool preprocessed_input ) const
+{
+    CvMat sample = _sample, mmask = _missing;
+    return predict(&sample, mmask.data.ptr ? &mmask : 0, preprocessed_input);
+}
+
+
+const CvMat* CvDTree::get_var_importance()
+{
+    if( !var_importance )
+    {
+        CvDTreeNode* node = root;
+        double* importance;
+        if( !node )
+            return 0;
+        var_importance = cvCreateMat( 1, data->var_count, CV_64F );
+        cvZero( var_importance );
+        importance = var_importance->data.db;
+
+        for(;;)
+        {
+            CvDTreeNode* parent;
+            for( ;; node = node->left )
+            {
+                CvDTreeSplit* split = node->split;
+
+                if( !node->left || node->Tn <= pruned_tree_idx )
+                    break;
+
+                for( ; split != 0; split = split->next )
+                    importance[split->var_idx] += split->quality;
+            }
+
+            for( parent = node->parent; parent && parent->right == node;
+                node = parent, parent = parent->parent )
+                ;
+
+            if( !parent )
+                break;
+
+            node = parent->right;
+        }
+
+        cvNormalize( var_importance, var_importance, 1., 0, CV_L1 );
+    }
+
+    return var_importance;
+}
+
+
+void CvDTree::write_split( CvFileStorage* fs, CvDTreeSplit* split ) const
+{
+    int ci;
+
+    cvStartWriteStruct( fs, 0, CV_NODE_MAP + CV_NODE_FLOW );
+    cvWriteInt( fs, "var", split->var_idx );
+    cvWriteReal( fs, "quality", split->quality );
+
+    ci = data->get_var_type(split->var_idx);
+    if( ci >= 0 ) // split on a categorical var
+    {
+        int i, n = data->cat_count->data.i[ci], to_right = 0, default_dir;
+        for( i = 0; i < n; i++ )
+            to_right += CV_DTREE_CAT_DIR(i,split->subset) > 0;
+
+        // ad-hoc rule when to use inverse categorical split notation
+        // to achieve more compact and clear representation
+        default_dir = to_right <= 1 || to_right <= MIN(3, n/2) || to_right <= n/3 ? -1 : 1;
+
+        cvStartWriteStruct( fs, default_dir*(split->inversed ? -1 : 1) > 0 ?
+                            "in" : "not_in", CV_NODE_SEQ+CV_NODE_FLOW );
+
+        for( i = 0; i < n; i++ )
+        {
+            int dir = CV_DTREE_CAT_DIR(i,split->subset);
+            if( dir*default_dir < 0 )
+                cvWriteInt( fs, 0, i );
+        }
+        cvEndWriteStruct( fs );
+    }
+    else
+        cvWriteReal( fs, !split->inversed ? "le" : "gt", split->ord.c );
+
+    cvEndWriteStruct( fs );
+}
+
+
+void CvDTree::write_node( CvFileStorage* fs, CvDTreeNode* node ) const
+{
+    CvDTreeSplit* split;
+
+    cvStartWriteStruct( fs, 0, CV_NODE_MAP );
+
+    cvWriteInt( fs, "depth", node->depth );
+    cvWriteInt( fs, "sample_count", node->sample_count );
+    cvWriteReal( fs, "value", node->value );
+
+    if( data->is_classifier )
+        cvWriteInt( fs, "norm_class_idx", node->class_idx );
+
+    cvWriteInt( fs, "Tn", node->Tn );
+    cvWriteInt( fs, "complexity", node->complexity );
+    cvWriteReal( fs, "alpha", node->alpha );
+    cvWriteReal( fs, "node_risk", node->node_risk );
+    cvWriteReal( fs, "tree_risk", node->tree_risk );
+    cvWriteReal( fs, "tree_error", node->tree_error );
+
+    if( node->left )
+    {
+        cvStartWriteStruct( fs, "splits", CV_NODE_SEQ );
+
+        for( split = node->split; split != 0; split = split->next )
+            write_split( fs, split );
+
+        cvEndWriteStruct( fs );
+    }
+
+    cvEndWriteStruct( fs );
+}
+
+
+void CvDTree::write_tree_nodes( CvFileStorage* fs ) const
+{
+    //CV_FUNCNAME( "CvDTree::write_tree_nodes" );
+
+    __BEGIN__;
+
+    CvDTreeNode* node = root;
+
+    // traverse the tree and save all the nodes in depth-first order
+    for(;;)
+    {
+        CvDTreeNode* parent;
+        for(;;)
+        {
+            write_node( fs, node );
+            if( !node->left )
+                break;
+            node = node->left;
+        }
+
+        for( parent = node->parent; parent && parent->right == node;
+            node = parent, parent = parent->parent )
+            ;
+
+        if( !parent )
+            break;
+
+        node = parent->right;
+    }
+
+    __END__;
+}
+
+
+void CvDTree::write( CvFileStorage* fs, const char* name ) const
+{
+    //CV_FUNCNAME( "CvDTree::write" );
+
+    __BEGIN__;
+
+    cvStartWriteStruct( fs, name, CV_NODE_MAP, CV_TYPE_NAME_ML_TREE );
+
+    //get_var_importance();
+    data->write_params( fs );
+    //if( var_importance )
+    //cvWrite( fs, "var_importance", var_importance );
+    write( fs );
+
+    cvEndWriteStruct( fs );
+
+    __END__;
+}
+
+
+void CvDTree::write( CvFileStorage* fs ) const
+{
+    //CV_FUNCNAME( "CvDTree::write" );
+
+    __BEGIN__;
+
+    cvWriteInt( fs, "best_tree_idx", pruned_tree_idx );
+
+    cvStartWriteStruct( fs, "nodes", CV_NODE_SEQ );
+    write_tree_nodes( fs );
+    cvEndWriteStruct( fs );
+
+    __END__;
+}
+
+
+CvDTreeSplit* CvDTree::read_split( CvFileStorage* fs, CvFileNode* fnode )
+{
+    CvDTreeSplit* split = 0;
+
+    CV_FUNCNAME( "CvDTree::read_split" );
+
+    __BEGIN__;
+
+    int vi, ci;
+
+    if( !fnode || CV_NODE_TYPE(fnode->tag) != CV_NODE_MAP )
+        CV_ERROR( CV_StsParseError, "some of the splits are not stored properly" );
+
+    vi = cvReadIntByName( fs, fnode, "var", -1 );
+    if( (unsigned)vi >= (unsigned)data->var_count )
+        CV_ERROR( CV_StsOutOfRange, "Split variable index is out of range" );
+
+    ci = data->get_var_type(vi);
+    if( ci >= 0 ) // split on categorical var
+    {
+        int i, n = data->cat_count->data.i[ci], inversed = 0, val;
+        CvSeqReader reader;
+        CvFileNode* inseq;
+        split = data->new_split_cat( vi, 0 );
+        inseq = cvGetFileNodeByName( fs, fnode, "in" );
+        if( !inseq )
+        {
+            inseq = cvGetFileNodeByName( fs, fnode, "not_in" );
+            inversed = 1;
+        }
+        if( !inseq ||
+            (CV_NODE_TYPE(inseq->tag) != CV_NODE_SEQ && CV_NODE_TYPE(inseq->tag) != CV_NODE_INT))
+            CV_ERROR( CV_StsParseError,
+            "Either 'in' or 'not_in' tags should be inside a categorical split data" );
+
+        if( CV_NODE_TYPE(inseq->tag) == CV_NODE_INT )
+        {
+            val = inseq->data.i;
+            if( (unsigned)val >= (unsigned)n )
+                CV_ERROR( CV_StsOutOfRange, "some of in/not_in elements are out of range" );
+
+            split->subset[val >> 5] |= 1 << (val & 31);
+        }
+        else
+        {
+            cvStartReadSeq( inseq->data.seq, &reader );
+
+            for( i = 0; i < reader.seq->total; i++ )
+            {
+                CvFileNode* inode = (CvFileNode*)reader.ptr;
+                val = inode->data.i;
+                if( CV_NODE_TYPE(inode->tag) != CV_NODE_INT || (unsigned)val >= (unsigned)n )
+                    CV_ERROR( CV_StsOutOfRange, "some of in/not_in elements are out of range" );
+
+                split->subset[val >> 5] |= 1 << (val & 31);
+                CV_NEXT_SEQ_ELEM( reader.seq->elem_size, reader );
+            }
+        }
+
+        // for categorical splits we do not use inversed splits,
+        // instead we inverse the variable set in the split
+        if( inversed )
+            for( i = 0; i < (n + 31) >> 5; i++ )
+                split->subset[i] ^= -1;
+    }
+    else
+    {
+        CvFileNode* cmp_node;
+        split = data->new_split_ord( vi, 0, 0, 0, 0 );
+
+        cmp_node = cvGetFileNodeByName( fs, fnode, "le" );
+        if( !cmp_node )
+        {
+            cmp_node = cvGetFileNodeByName( fs, fnode, "gt" );
+            split->inversed = 1;
+        }
+
+        split->ord.c = (float)cvReadReal( cmp_node );
+    }
+
+    split->quality = (float)cvReadRealByName( fs, fnode, "quality" );
+
+    __END__;
+
+    return split;
+}
+
+
+CvDTreeNode* CvDTree::read_node( CvFileStorage* fs, CvFileNode* fnode, CvDTreeNode* parent )
+{
+    CvDTreeNode* node = 0;
+
+    CV_FUNCNAME( "CvDTree::read_node" );
+
+    __BEGIN__;
+
+    CvFileNode* splits;
+    int i, depth;
+
+    if( !fnode || CV_NODE_TYPE(fnode->tag) != CV_NODE_MAP )
+        CV_ERROR( CV_StsParseError, "some of the tree elements are not stored properly" );
+
+    CV_CALL( node = data->new_node( parent, 0, 0, 0 ));
+    depth = cvReadIntByName( fs, fnode, "depth", -1 );
+    if( depth != node->depth )
+        CV_ERROR( CV_StsParseError, "incorrect node depth" );
+
+    node->sample_count = cvReadIntByName( fs, fnode, "sample_count" );
+    node->value = cvReadRealByName( fs, fnode, "value" );
+    if( data->is_classifier )
+        node->class_idx = cvReadIntByName( fs, fnode, "norm_class_idx" );
+
+    node->Tn = cvReadIntByName( fs, fnode, "Tn" );
+    node->complexity = cvReadIntByName( fs, fnode, "complexity" );
+    node->alpha = cvReadRealByName( fs, fnode, "alpha" );
+    node->node_risk = cvReadRealByName( fs, fnode, "node_risk" );
+    node->tree_risk = cvReadRealByName( fs, fnode, "tree_risk" );
+    node->tree_error = cvReadRealByName( fs, fnode, "tree_error" );
+
+    splits = cvGetFileNodeByName( fs, fnode, "splits" );
+    if( splits )
+    {
+        CvSeqReader reader;
+        CvDTreeSplit* last_split = 0;
+
+        if( CV_NODE_TYPE(splits->tag) != CV_NODE_SEQ )
+            CV_ERROR( CV_StsParseError, "splits tag must stored as a sequence" );
+
+        cvStartReadSeq( splits->data.seq, &reader );
+        for( i = 0; i < reader.seq->total; i++ )
+        {
+            CvDTreeSplit* split;
+            CV_CALL( split = read_split( fs, (CvFileNode*)reader.ptr ));
+            if( !last_split )
+                node->split = last_split = split;
+            else
+                last_split = last_split->next = split;
+
+            CV_NEXT_SEQ_ELEM( reader.seq->elem_size, reader );
+        }
+    }
+
+    __END__;
+
+    return node;
+}
+
+
+void CvDTree::read_tree_nodes( CvFileStorage* fs, CvFileNode* fnode )
+{
+    CV_FUNCNAME( "CvDTree::read_tree_nodes" );
+
+    __BEGIN__;
+
+    CvSeqReader reader;
+    CvDTreeNode _root;
+    CvDTreeNode* parent = &_root;
+    int i;
+    parent->left = parent->right = parent->parent = 0;
+
+    cvStartReadSeq( fnode->data.seq, &reader );
+
+    for( i = 0; i < reader.seq->total; i++ )
+    {
+        CvDTreeNode* node;
+
+        CV_CALL( node = read_node( fs, (CvFileNode*)reader.ptr, parent != &_root ? parent : 0 ));
+        if( !parent->left )
+            parent->left = node;
+        else
+            parent->right = node;
+        if( node->split )
+            parent = node;
+        else
+        {
+            while( parent && parent->right )
+                parent = parent->parent;
+        }
+
+        CV_NEXT_SEQ_ELEM( reader.seq->elem_size, reader );
+    }
+
+    root = _root.left;
+
+    __END__;
+}
+
+
+void CvDTree::read( CvFileStorage* fs, CvFileNode* fnode )
+{
+    CvDTreeTrainData* _data = new CvDTreeTrainData();
+    _data->read_params( fs, fnode );
+
+    read( fs, fnode, _data );
+    get_var_importance();
+}
+
+
+// a special entry point for reading weak decision trees from the tree ensembles
+void CvDTree::read( CvFileStorage* fs, CvFileNode* node, CvDTreeTrainData* _data )
+{
+    CV_FUNCNAME( "CvDTree::read" );
+
+    __BEGIN__;
+
+    CvFileNode* tree_nodes;
+
+    clear();
+    data = _data;
+
+    tree_nodes = cvGetFileNodeByName( fs, node, "nodes" );
+    if( !tree_nodes || CV_NODE_TYPE(tree_nodes->tag) != CV_NODE_SEQ )
+        CV_ERROR( CV_StsParseError, "nodes tag is missing" );
+
+    pruned_tree_idx = cvReadIntByName( fs, node, "best_tree_idx", -1 );
+    read_tree_nodes( fs, tree_nodes );
+
+    __END__;
+}
+
+Mat CvDTree::getVarImportance()
+{
+    return cvarrToMat(get_var_importance());
+}
+
+/* End of file. */
diff --git a/openbr/core/opencvutils.h b/openbr/core/opencvutils.h
index 1bd4d8f..f9d74b4 100644
--- a/openbr/core/opencvutils.h
+++ b/openbr/core/opencvutils.h
@@ -22,7 +22,8 @@
 #include <QString>
 #include <QStringList>
 #include <opencv2/core/core.hpp>
-#include <opencv2/ml/ml.hpp>
+//SAB #include <opencv2/ml/ml.hpp>
+#include "old_ml.hpp"
 #include <assert.h>
 #include <openbr/openbr_plugin.h>
 
@@ -93,7 +94,7 @@
     QList<cv::Point2f> toPoints(const QList<QPointF> &qPoints);
     QList<QPointF> fromPoints(const QList<cv::Point2f> &cvPoints);
     cv::Mat pointsToMatrix(const QList<QPointF> &qPoints);
-    cv::Rect toRect(const QRectF &qRect);
+    BR_EXPORT cv::Rect toRect(const QRectF &qRect);
     cv::RotatedRect toRotatedRect(const QRectF &qRect, float angle);
     QRectF fromRect(const cv::Rect &cvRect);
     QList<cv::Rect> toRects(const QList<QRectF> &qRects);
diff --git a/openbr/openbr_plugin.h b/openbr/openbr_plugin.h
index 12a95f4..0ab22fa 100644
--- a/openbr/openbr_plugin.h
+++ b/openbr/openbr_plugin.h
@@ -278,6 +278,7 @@
     inline operator cv::Mat&() { return m(); }
     inline operator cv::_InputArray() const { return m(); }
     inline operator cv::_OutputArray() { return m(); }
+    inline operator cv::_InputOutputArray() { return m(); } //SAB
     inline bool isNull() const { return isEmpty() || !m().data; }
     inline void merge(const Template &other) { append(other); file.append(other.file); }
 
diff --git a/openbr/plugins/classification/mlp.cpp b/openbr/plugins/classification/mlp.cpp
index 871283c..d6688cd 100644
--- a/openbr/plugins/classification/mlp.cpp
+++ b/openbr/plugins/classification/mlp.cpp
@@ -14,7 +14,7 @@
  * limitations under the License.                                            *
  * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
 
-#include <opencv2/ml/ml.hpp>
+//SAB #include <opencv2/ml/ml.hpp>
 
 #include <openbr/plugins/openbr_internal.h>
 #include <openbr/core/opencvutils.h>
diff --git a/openbr/plugins/classification/svm.cpp b/openbr/plugins/classification/svm.cpp
index 9cdf5e2..0e54c68 100644
--- a/openbr/plugins/classification/svm.cpp
+++ b/openbr/plugins/classification/svm.cpp
@@ -16,7 +16,7 @@
 
 #include <QTemporaryFile>
 #include <opencv2/core/core.hpp>
-#include <opencv2/ml/ml.hpp>
+//SAB #include <opencv2/ml/ml.hpp>
 
 #include <openbr/plugins/openbr_internal.h>
 #include <openbr/core/opencvutils.h>
diff --git a/openbr/plugins/cmake/opencv.cmake b/openbr/plugins/cmake/opencv.cmake
index 8e937db..7b9ab57 100644
--- a/openbr/plugins/cmake/opencv.cmake
+++ b/openbr/plugins/cmake/opencv.cmake
@@ -1,11 +1,11 @@
-option(BR_WITH_OPENCV_CONTRIB "Build with OpenCV contrib plugins." ON)
-if(${BR_WITH_OPENCV_CONTRIB})
-  set(BR_THIRDPARTY_LIBS ${BR_THIRDPARTY_LIBS} opencv_contrib)
-  set(OPENCV_DEPENDENCIES ${OPENCV_DEPENDENCIES} opencv_contrib)
-else()
-  set(BR_EXCLUDED_PLUGINS ${BR_EXCLUDED_PLUGINS} plugins/imgproc/heatmap.cpp
-                                                 plugins/imgproc/shapeaxisratio.cpp)
-endif()
+#SAB option(BR_WITH_OPENCV_CONTRIB "Build with OpenCV contrib plugins." ON)
+#SAB if(${BR_WITH_OPENCV_CONTRIB})
+#SAB  set(BR_THIRDPARTY_LIBS ${BR_THIRDPARTY_LIBS} opencv_contrib)
+#SAB  set(OPENCV_DEPENDENCIES ${OPENCV_DEPENDENCIES} opencv_contrib)
+#SAB else()
+#SAB  set(BR_EXCLUDED_PLUGINS ${BR_EXCLUDED_PLUGINS} plugins/imgproc/heatmap.cpp
+#SAB                                                 plugins/imgproc/shapeaxisratio.cpp)
+#SAB endif()
 
 option(BR_WITH_OPENCV_FEATURES2D "Build with OpenCV features2d plugins." ON)
 if(${BR_WITH_OPENCV_FEATURES2D})
@@ -28,11 +28,17 @@
 
 option(BR_WITH_OPENCV_NONFREE "Build with OpenCV nonfree plugins." ON)
 if(${BR_WITH_OPENCV_NONFREE})
-  set(BR_THIRDPARTY_LIBS ${BR_THIRDPARTY_LIBS} opencv_nonfree)
-  set(OPENCV_DEPENDENCIES ${OPENCV_DEPENDENCIES} opencv_nonfree)
+#SAB  set(BR_THIRDPARTY_LIBS ${BR_THIRDPARTY_LIBS} opencv_nonfree)
+  set(BR_THIRDPARTY_LIBS ${BR_THIRDPARTY_LIBS} opencv_xfeatures2d)
+#SAB  set(OPENCV_DEPENDENCIES ${OPENCV_DEPENDENCIES} opencv_nonfree)
+  set(OPENCV_DEPENDENCIES ${OPENCV_DEPENDENCIES} opencv_xfeatures2d)
 else()
   set(BR_EXCLUDED_PLUGINS ${BR_EXCLUDED_PLUGINS} plugins/imgproc/custom_sift.cpp
                                                  plugins/imgproc/sift.cpp)
+#SAB begin
+  set(BR_EXCLUDED_PLUGINS ${BR_EXCLUDED_PLUGINS} plugins/imgproc/keypointdescriptor.cpp
+                                                 plugins/metadata/keypointdetector.cpp)
+#SAB end
 endif()
 
 option(BR_WITH_OPENCV_OBJDETECT "Build with OpenCV objdetect plugins." ON)
diff --git a/openbr/plugins/core/algorithms.cpp b/openbr/plugins/core/algorithms.cpp
index 17e93a0..f11610a 100644
--- a/openbr/plugins/core/algorithms.cpp
+++ b/openbr/plugins/core/algorithms.cpp
@@ -37,7 +37,8 @@
         Globals->abbreviations.insert("FR_Eyes", "(CropFromLandmarks([30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47],paddingVertical=.8,paddingHorizontal=.2)+Resize(24,48))");
         Globals->abbreviations.insert("FR_Eyebrows", "(CropFromLandmarks([16,17,18,19,20,21,22,23,24,25,26,27],paddingVertical=.8,paddingHorizontal=.2)+Resize(24,48))");
         Globals->abbreviations.insert("FR_Mouth", "(CropFromLandmarks([59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76])+Resize(24,48))");
-        Globals->abbreviations.insert("FR_Nose", "(CropFromLandmarks([16,17,18,19,20,21,22,23,24,25,26,27],padding=3)+Resize(36,36))");
+// SAB  Globals->abbreviations.insert("FR_Nose", "(CropFromLandmarks([16,17,18,19,20,21,22,23,24,25,26,27],padding=3)+Resize(36,36))");
+        Globals->abbreviations.insert("FR_Nose", "(CropFromLandmarks([48,49,50,51,52,53,54,55,56,57,58],padding=3)+Resize(36,36))");
         Globals->abbreviations.insert("FR_Face", "(Crop(24,24,88,88)+Resize(44,44))");
         Globals->abbreviations.insert("FR_Detect", "(Open+Cvt(Gray)+Cascade+Stasm+Rename(StasmLeftEye,Affine_1,true)+Rename(StasmRightEye,Affine_0,true)+Affine(136,136,0.35,0.35,warpPoints=true))");
         Globals->abbreviations.insert("FR_Represent", "((DenseHOG/DenseLBP)+Cat+LDA(.98)+Normalize(L2))");
diff --git a/openbr/plugins/distance/svm.cpp b/openbr/plugins/distance/svm.cpp
index 37d04b9..361dd1e 100644
--- a/openbr/plugins/distance/svm.cpp
+++ b/openbr/plugins/distance/svm.cpp
@@ -1,5 +1,5 @@
 #include <opencv2/core/core.hpp>
-#include <opencv2/ml/ml.hpp>
+//SAB #include <opencv2/ml/ml.hpp>
 
 #include <openbr/plugins/openbr_internal.h>
 #include <openbr/core/opencvutils.h>
diff --git a/openbr/plugins/gallery/txt.cpp b/openbr/plugins/gallery/txt.cpp
index 3070471..4d31a98 100644
--- a/openbr/plugins/gallery/txt.cpp
+++ b/openbr/plugins/gallery/txt.cpp
@@ -61,6 +61,29 @@
 
             if (!line.isEmpty()){
                 int splitIndex = line.lastIndexOf(' ');
+
+                // SAB begin
+                // If the space is preceded by a backslash or between quotes,
+                // consider it part of the file path.
+                //
+                if (splitIndex > 0)
+                  {
+                    if (line[splitIndex - 1] == '\\')
+                      {
+                        splitIndex = -1;
+                      }
+                    else if ((line.startsWith ('"') && line.endsWith ('"'))
+                             || (line.startsWith ('\'') && line.endsWith ('\'')))
+                      {
+                        //
+                        // Trim the quotes.
+                        //
+                        splitIndex = -1;
+                        line = line.remove (line.size() - 1, 1).remove (0, 1);
+                      }
+                  }
+                // SAB end
+
                 if (splitIndex == -1) templates.append(File(line, QFileInfo(line).dir().dirName()));
                 else                  templates.append(File(line.mid(0, splitIndex), line.mid(splitIndex+1)));
                 templates.last().file.set("progress", this->position());
diff --git a/openbr/plugins/gui/draw.cpp b/openbr/plugins/gui/draw.cpp
index 59f5c81..d581536 100644
--- a/openbr/plugins/gui/draw.cpp
+++ b/openbr/plugins/gui/draw.cpp
@@ -17,6 +17,8 @@
 #include <openbr/plugins/openbr_internal.h>
 #include <openbr/core/opencvutils.h>
 
+#include "opencv2/imgproc.hpp" //SAB
+
 using namespace cv;
 
 namespace br
diff --git a/openbr/plugins/gui/drawdelaunay.cpp b/openbr/plugins/gui/drawdelaunay.cpp
index dc86c6c..769cedb 100644
--- a/openbr/plugins/gui/drawdelaunay.cpp
+++ b/openbr/plugins/gui/drawdelaunay.cpp
@@ -17,6 +17,8 @@
 #include <openbr/plugins/openbr_internal.h>
 #include <openbr/core/opencvutils.h>
 
+#include "opencv2/imgproc.hpp" //SAB
+
 using namespace cv;
 
 namespace br
diff --git a/openbr/plugins/gui/drawgridlines.cpp b/openbr/plugins/gui/drawgridlines.cpp
index 5f8129a..27b0cc4 100644
--- a/openbr/plugins/gui/drawgridlines.cpp
+++ b/openbr/plugins/gui/drawgridlines.cpp
@@ -18,6 +18,8 @@
 
 #include <openbr/plugins/openbr_internal.h>
 
+#include "opencv2/imgproc.hpp" //SAB
+
 using namespace cv;
 
 namespace br
diff --git a/openbr/plugins/gui/drawopticalflow.cpp b/openbr/plugins/gui/drawopticalflow.cpp
index fb271d8..77da4d5 100644
--- a/openbr/plugins/gui/drawopticalflow.cpp
+++ b/openbr/plugins/gui/drawopticalflow.cpp
@@ -17,6 +17,8 @@
 #include <openbr/plugins/openbr_internal.h>
 #include <openbr/core/opencvutils.h>
 
+#include "opencv2/imgproc.hpp" //SAB
+
 using namespace cv;
 
 namespace br
diff --git a/openbr/plugins/gui/drawpropertiespoint.cpp b/openbr/plugins/gui/drawpropertiespoint.cpp
index 5234807..5689b2f 100644
--- a/openbr/plugins/gui/drawpropertiespoint.cpp
+++ b/openbr/plugins/gui/drawpropertiespoint.cpp
@@ -17,6 +17,8 @@
 #include <openbr/plugins/openbr_internal.h>
 #include <openbr/core/opencvutils.h>
 
+#include "opencv2/imgproc.hpp" //SAB
+
 using namespace cv;
 
 namespace br
diff --git a/openbr/plugins/gui/drawpropertypoint.cpp b/openbr/plugins/gui/drawpropertypoint.cpp
index 6027cb3..42b1177 100644
--- a/openbr/plugins/gui/drawpropertypoint.cpp
+++ b/openbr/plugins/gui/drawpropertypoint.cpp
@@ -17,6 +17,8 @@
 #include <openbr/plugins/openbr_internal.h>
 #include <openbr/core/opencvutils.h>
 
+#include "opencv2/imgproc.hpp" //SAB
+
 using namespace cv;
 
 namespace br
diff --git a/openbr/plugins/imgproc/convexhull.cpp b/openbr/plugins/imgproc/convexhull.cpp
index 02d2348..147b24d 100644
--- a/openbr/plugins/imgproc/convexhull.cpp
+++ b/openbr/plugins/imgproc/convexhull.cpp
@@ -50,6 +50,7 @@
     {
         dst.m() = Mat::zeros(src.m().rows,src.m().cols,src.m().type());
 
+        using namespace std; //SAB
         vector<vector<Point> > contours;
         vector<Vec4i> hierarchy;
 
diff --git a/openbr/plugins/imgproc/custom_sift.cpp b/openbr/plugins/imgproc/custom_sift.cpp
index 429fa86..8c784f7 100644
--- a/openbr/plugins/imgproc/custom_sift.cpp
+++ b/openbr/plugins/imgproc/custom_sift.cpp
@@ -105,9 +105,14 @@
 #include <iostream>
 #include <stdarg.h>
 #include <opencv2/imgproc/imgproc.hpp>
-#include <opencv2/nonfree/nonfree.hpp>
+//SAB #include <opencv2/nonfree/nonfree.hpp>
+#include <opencv2/xfeatures2d.hpp> //SAB
+#include <opencv2/core/hal/hal.hpp> //SAB
 
 using namespace cv;
+using namespace cv::hal; //SAB
+template <typename T> //SAB
+using vector = std::vector<T>; //SAB
 
 /******************************* Defs and macros *****************************/
 
diff --git a/openbr/plugins/imgproc/equalizehist.cpp b/openbr/plugins/imgproc/equalizehist.cpp
index 9c9d614..78468bd 100644
--- a/openbr/plugins/imgproc/equalizehist.cpp
+++ b/openbr/plugins/imgproc/equalizehist.cpp
@@ -40,7 +40,8 @@
             // http://stackoverflow.com/questions/15007304/histogram-equalization-not-working-on-color-image-opencv
             Mat ycrcb;
             cvtColor(src, ycrcb, CV_BGR2YCrCb);
-            vector<Mat> channels;
+//SAB            vector<Mat> channels;
+            std::vector<Mat> channels;
             split(ycrcb, channels);
             equalizeHist(channels[0], channels[0]);
             merge(channels, ycrcb);
diff --git a/openbr/plugins/imgproc/fillcontours.cpp b/openbr/plugins/imgproc/fillcontours.cpp
index fae04f2..778dc49 100644
--- a/openbr/plugins/imgproc/fillcontours.cpp
+++ b/openbr/plugins/imgproc/fillcontours.cpp
@@ -52,6 +52,7 @@
     {
         dst.m() = Mat::zeros(src.m().rows,src.m().cols,src.m().type());
 
+        using namespace std; //SAB
         vector<vector<Point> > contours;
         vector<Vec4i> hierarchy;
 
diff --git a/openbr/plugins/imgproc/heatmap.cpp b/openbr/plugins/imgproc/heatmap.cpp
index 3b1bc46..804f5a1 100644
--- a/openbr/plugins/imgproc/heatmap.cpp
+++ b/openbr/plugins/imgproc/heatmap.cpp
@@ -1,7 +1,8 @@
 #include "openbr/plugins/openbr_internal.h"
 #include "openbr/core/opencvutils.h"
 
-#include <opencv2/contrib/contrib.hpp>
+//SAB #include <opencv2/contrib/contrib.hpp>
+#include <opencv2/imgproc.hpp>
 
 using namespace cv;
 
diff --git a/openbr/plugins/imgproc/keypointdescriptor.cpp b/openbr/plugins/imgproc/keypointdescriptor.cpp
index e0f7d5f..dea8729 100644
--- a/openbr/plugins/imgproc/keypointdescriptor.cpp
+++ b/openbr/plugins/imgproc/keypointdescriptor.cpp
@@ -14,7 +14,8 @@
  * limitations under the License.                                            *
  * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
 
-#include <opencv2/features2d/features2d.hpp>
+//SAB #include <opencv2/features2d/features2d.hpp>
+#include <opencv2/xfeatures2d.hpp>
 
 #include <openbr/plugins/openbr_internal.h>
 
@@ -41,7 +42,36 @@
 
     void init()
     {
-        descriptorExtractor = DescriptorExtractor::create(descriptor.toStdString());
+//SAB        descriptorExtractor = DescriptorExtractor::create(descriptor.toStdString());
+//SAB begin
+        // Support same set as former DescriptorExtractor::create.
+        std::string desc (descriptor.toStdString());
+
+        if (desc == "SIFT")
+        {
+            descriptorExtractor = xfeatures2d::SIFT::create();
+        }
+        else if (desc == "SURF")
+        {
+            descriptorExtractor = xfeatures2d::SURF::create();
+        }
+        else if (desc == "BRIEF")
+        {
+            descriptorExtractor = xfeatures2d::BriefDescriptorExtractor::create();
+        }
+        else if (desc == "BRISK")
+        {
+            descriptorExtractor = BRISK::create();
+        }
+        else if (desc == "ORB")
+        {
+            descriptorExtractor = ORB::create();
+        }
+        else if (desc == "FREAK")
+        {
+            descriptorExtractor = xfeatures2d::FREAK::create();
+        }
+//SAB end
         if (descriptorExtractor.empty())
             qFatal("Failed to create DescriptorExtractor: %s", qPrintable(descriptor));
     }
diff --git a/openbr/plugins/imgproc/mask.cpp b/openbr/plugins/imgproc/mask.cpp
index b4408b0..757721a 100644
--- a/openbr/plugins/imgproc/mask.cpp
+++ b/openbr/plugins/imgproc/mask.cpp
@@ -16,6 +16,8 @@
 
 #include <openbr/plugins/openbr_internal.h>
 
+#include <opencv2/imgproc.hpp> //SAB
+
 using namespace cv;
 
 namespace br
diff --git a/openbr/plugins/imgproc/rndsubspace.cpp b/openbr/plugins/imgproc/rndsubspace.cpp
index 423dbc9..5c76af7 100644
--- a/openbr/plugins/imgproc/rndsubspace.cpp
+++ b/openbr/plugins/imgproc/rndsubspace.cpp
@@ -50,7 +50,8 @@
             const int size = data.first()[index].rows * cols;
             QList<float> weights; weights.reserve(size);
             if (weighted) {
-                Mat flatData = OpenCVUtils::toMat(data.data());
+//SAB                Mat flatData = OpenCVUtils::toMat(data.data());
+                Mat flatData = OpenCVUtils::toMat(data.data(index));
                 for (int i=0; i<size; i++) {
                     Scalar mean, stddev;
                     cv::meanStdDev(flatData.col(i), mean, stddev);
diff --git a/openbr/plugins/imgproc/shapeaxisratio.cpp b/openbr/plugins/imgproc/shapeaxisratio.cpp
index 31d561d..4f7b23d 100644
--- a/openbr/plugins/imgproc/shapeaxisratio.cpp
+++ b/openbr/plugins/imgproc/shapeaxisratio.cpp
@@ -2,7 +2,7 @@
 #include "openbr/core/opencvutils.h"
 #include "openbr/core/eigenutils.h"
 
-#include <opencv2/contrib/contrib.hpp>
+//SAB #include <opencv2/contrib/contrib.hpp>
 
 #include <Eigen/Dense>
 
diff --git a/openbr/plugins/imgproc/sift.cpp b/openbr/plugins/imgproc/sift.cpp
index 17bb271..0af297b 100644
--- a/openbr/plugins/imgproc/sift.cpp
+++ b/openbr/plugins/imgproc/sift.cpp
@@ -14,7 +14,8 @@
  * limitations under the License.                                            *
  * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
 
-#include <opencv2/nonfree/nonfree.hpp>
+//SAB #include <opencv2/nonfree/nonfree.hpp>
+#include <opencv2/xfeatures2d.hpp>
 
 #include <openbr/plugins/openbr_internal.h>
 
@@ -47,13 +48,15 @@
     BR_PROPERTY(double, edgeThreshold, 10)
     BR_PROPERTY(double, sigma, 1.6)
 
-    SIFT sift;
+//SAB    SIFT sift;
+    Ptr<xfeatures2d::SIFT> sift;
 
     void init()
     {
         if (sizes.empty())
             sizes.append(size);
-        sift = SIFT(nFeatures, nOctaveLayers, contrastThreshold, edgeThreshold, sigma);
+//SAB        sift = SIFT(nFeatures, nOctaveLayers, contrastThreshold, edgeThreshold, sigma);
+        sift = xfeatures2d::SIFT::create(nFeatures, nOctaveLayers, contrastThreshold, edgeThreshold, sigma);
     }
 
     void project(const Template &src, Template &dst) const
@@ -64,7 +67,8 @@
                 keyPoints.push_back(KeyPoint(val.x(), val.y(), r));
 
         Mat m;
-        sift(src, Mat(), keyPoints, m, true);
+//SAB        sift(src, Mat(), keyPoints, m, true);
+        sift->detectAndCompute(src, Mat(), keyPoints, m, true);
         m.setTo(0, m<0); // SIFT returns large negative values when it goes off the edge of the image.
         dst += m;
     }
diff --git a/openbr/plugins/imgproc/watershedsegmentation.cpp b/openbr/plugins/imgproc/watershedsegmentation.cpp
index 104b6a6..861f0df 100644
--- a/openbr/plugins/imgproc/watershedsegmentation.cpp
+++ b/openbr/plugins/imgproc/watershedsegmentation.cpp
@@ -44,6 +44,7 @@
         // and modifies its source image
         if (mod.depth() != CV_8U) OpenCVUtils::cvtUChar(mod, mod);
         if (mod.channels() != 1) OpenCVUtils::cvtGray(mod, mod);
+        using namespace std; //SAB
         vector<vector<Point> > contours;
         vector<Vec4i> hierarchy;
         findContours(mod, contours, hierarchy, CV_RETR_CCOMP, CV_CHAIN_APPROX_SIMPLE);
diff --git a/openbr/plugins/metadata/delaunay.cpp b/openbr/plugins/metadata/delaunay.cpp
index fd05f6b..070b66d 100644
--- a/openbr/plugins/metadata/delaunay.cpp
+++ b/openbr/plugins/metadata/delaunay.cpp
@@ -71,7 +71,8 @@
             subdiv.insert(OpenCVUtils::toPoint(points[i]));
         }
 
-        vector<Vec6f> triangleList;
+//SAB        vector<Vec6f> triangleList;
+        std::vector<Vec6f> triangleList;
         subdiv.getTriangleList(triangleList);
 
         QList<QPointF> validTriangles;
diff --git a/openbr/plugins/metadata/keypointdetector.cpp b/openbr/plugins/metadata/keypointdetector.cpp
index 0d5acb5..3803b94 100644
--- a/openbr/plugins/metadata/keypointdetector.cpp
+++ b/openbr/plugins/metadata/keypointdetector.cpp
@@ -14,7 +14,8 @@
  * limitations under the License.                                            *
  * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
 
-#include <opencv2/features2d/features2d.hpp>
+//SAB #include <opencv2/features2d/features2d.hpp>
+#include <opencv2/xfeatures2d.hpp>
 
 #include <openbr/plugins/openbr_internal.h>
 #include <openbr/core/opencvutils.h>
@@ -40,7 +41,60 @@
 
     void init()
     {
-        featureDetector = FeatureDetector::create(detector.toStdString());
+//SAB        featureDetector = FeatureDetector::create(detector.toStdString());
+//SAB begin
+        // Support same set as former FeatureDetector::create, except for
+        // Dense, and grid and pyramid adapted detectors.
+        std::string det (detector.toStdString());
+
+        if (det == "SIFT")
+        {
+            featureDetector = xfeatures2d::SIFT::create();
+        }
+        else if (det == "FAST")
+        {
+            featureDetector = FastFeatureDetector::create();
+        }
+        else if (det == "STAR")
+        {
+            featureDetector = xfeatures2d::StarDetector::create();
+        }
+        else if (det == "SURF")
+        {
+            featureDetector = xfeatures2d::SURF::create();
+        }
+        else if (det == "ORB")
+        {
+            featureDetector = ORB::create();
+        }
+        else if (det == "BRISK")
+        {
+            featureDetector = BRISK::create();
+        }
+        else if (det == "MSER")
+        {
+            featureDetector = MSER::create();
+        }
+        else if (det == "GFTT")
+        {
+            featureDetector = GFTTDetector::create();
+        }
+        else if (det == "HARRIS")
+        {
+            GFTTDetector* tmp = GFTTDetector::create();
+
+            tmp->setHarrisDetector(true);
+            featureDetector = tmp;
+        }
+//        else if (det == "Dense")
+//        {
+//            featureDetector = DenseFeatureDetector::create();
+//        }
+        else if (det == "SimpleBlob")
+        {
+            featureDetector = SimpleBlobDetector::create();
+        }
+//SAB end
         if (featureDetector.empty())
             qFatal("Failed to create KeyPointDetector: %s", qPrintable(detector));
     }
diff --git a/openbr/plugins/metadata/stasm4.cpp b/openbr/plugins/metadata/stasm4.cpp
index 86dc024..4ea1411 100644
--- a/openbr/plugins/metadata/stasm4.cpp
+++ b/openbr/plugins/metadata/stasm4.cpp
@@ -2,7 +2,8 @@
 #include <stasmcascadeclassifier.h>
 #include <stasm_lib.h>
 #include <stasm.h>
-#include <opencv2/opencv.hpp>
+//SAB #include <opencv2/opencv.hpp>
+#include <opencv2/imgproc.hpp>
 
 #include <openbr/plugins/openbr_internal.h>
 #include <openbr/core/qtutils.h>
